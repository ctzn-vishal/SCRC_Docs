{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NYU Stern Center for Research Computing (SCRC) Documentation","text":"<p>Welcome to the official documentation for the NYU Stern Center for Research Computing (SCRC). SCRC provides advanced computing resources, large-scale storage solutions, specialized research datasets, and essential software tailored to support the diverse research needs of the NYU Stern School of Business community \u2013 faculty, researchers, and students.</p> <p>This site serves as your comprehensive guide to understanding, accessing, and effectively utilizing the SCRC ecosystem. Whether you're running complex simulations, analyzing large datasets, or exploring new computational methods, these resources are designed to accelerate your research.</p>"},{"location":"#how-this-documentation-is-organized","title":"How This Documentation is Organized","text":"<p>This documentation is structured into logical categories to help you find the information you need quickly. Each section covers a specific aspect of SCRC's services and resources:</p> <ul> <li>Getting Started: Essential first steps, including obtaining an account, connecting to our systems, and understanding the basics.</li> <li>Computing Resources: Information on our high-performance computing clusters, job scheduling with Slurm, interactive sessions, and hardware details.</li> <li>Storage Solutions: Details about your home directory, large-scale 'bigdata' storage options, and how to manage and transfer your files.</li> <li>Research Datasets: Guides on accessing available datasets, including specialized financial data via WRDS.</li> <li>Software and Applications: Instructions for using pre-installed software via the modules system, setting up virtual environments (Python, R), and running specific applications.</li> <li>Linux Tutorials: Foundational guides for users new to the Linux command-line environment used on our clusters.</li> <li>Cloud Computing: Information about leveraging cloud resources and requesting virtual machines for specific research projects.</li> </ul> <p>Browse the categories above or use the search function (if available) to find specific topics.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>New to SCRC? We recommend starting here! These guides will walk you through the initial setup process:</p> <ol> <li>Getting an SCRC Account: The first step to accessing any SCRC resource.</li> <li>Connecting to SCRC Clusters: How to log in to our computing systems from your own computer.</li> <li>Getting Started Overview: A broader look at the SCRC environment and common initial tasks.</li> <li>SCRC vs NYU HPC: Understand the differences between Stern-specific and university-wide computing resources.</li> </ol> <p>We encourage you to explore the documentation relevant to your research needs. If you have questions or require assistance, please consult the support resources outlined within these pages or contact the SCRC team directly.</p>"},{"location":"cloud-computing/","title":"Cloud Computing","text":"<p>Welcome to the Cloud Computing section of the SCRC documentation. This area provides resources and guides for utilizing the cloud computing services offered by SCRC, specifically focusing on our private cloud environment and virtual machines (VMs).</p> <p>Whether you need a dedicated environment for specialized software, long-running tasks, or specific operating system requirements, our cloud computing resources offer flexible and powerful solutions to support your research needs.</p> <p>Here, you'll find information on:</p> <ul> <li>Understanding our Cloud Environment: Learn about the benefits and architecture of SCRC's private cloud.</li> <li>Virtual Machines (VMs): Discover what VMs are and how they can be used within SCRC.</li> <li>Resource Requests: Find out how to request access to cloud resources, including dedicated VMs.</li> </ul>"},{"location":"cloud-computing/#topics-in-this-category","title":"Topics in this Category","text":"<p>Below is a list of available topics within the Cloud Computing category.</p> <ul> <li>Cloud Computing Overview<ul> <li>Introduction to Stern's private cloud computing environment, its benefits, and general use cases.</li> </ul> </li> <li>Requesting a Virtual Machine<ul> <li>How to request a dedicated virtual machine (VM) for specialized research needs.</li> </ul> </li> </ul>"},{"location":"cloud-computing/cloud-computing-overview/","title":"Overview","text":"<pre><code>---\ntitle: \"Cloud Computing Overview\"\ncategory: \"cloud-computing\"\ndescription: \"Introduction to Stern's private cloud computing environment, its benefits, and general use cases.\"\n---\n\n# Cloud Computing Overview\n\n## Introduction\n\nThe Stern Center for Research Computing (SCRC) offers a private cloud computing environment designed to support researchers with specialized computational needs. This environment allows the SCRC team to quickly provide you with a virtual machine (VM) tailored for your research.\n\nThis service is particularly useful for faculty who have research funds for equipment; typically, they purchase a cloud server, and their applications run in dedicated VMs on that server, alongside other VMs. This approach enhances flexibility, resource utilization, power savings, and administrative ease.\n\n## What is a Virtual Machine?\n\nA virtual machine (VM) functions essentially like a complete physical computer but runs within Stern's shared cloud infrastructure. Key aspects include:\n\n*   **Customization:** Each VM can be configured with specific amounts of memory, disk space, number of processors, operating system (Windows or Linux), and pre-installed software based on your research requirements.\n*   **Isolation:** VMs operate independently, ensuring your environment is separate from others.\n\n## Benefits of Using Stern's Cloud Environment\n\nUtilizing Stern's private cloud and virtual machines offers several advantages for researchers:\n\n*   **Customization:** Tailor VM resources (memory, disk, CPU, OS, software) precisely to your project needs.\n*   **Accessibility:** Access your dedicated computing environment securely from anywhere (office, home, or abroad). Off-campus access requires connecting to the NYU VPN.\n*   **Collaboration:** Facilitates collaboration with colleagues, regardless of their physical location.\n*   **Efficiency &amp; Cost-Effectiveness:** Get computing resources provisioned and configured quickly without the financial overhead and delay of purchasing and setting up physical hardware.\n*   **Resource Optimization:** Frees up your personal computer, allowing it to be used for other tasks while computations run on the VM.\n*   **Flexibility &amp; Reliability:** VMs can be migrated between physical cloud servers if maintenance or resource balancing is necessary, improving overall service reliability.\n\n## Getting Started\n\n### Requesting a Virtual Machine\n\nSCRC maintains a VMware cloud environment to support researchers.\n\n1.  **Send a Request:** Email `scrc-list@stern.nyu.edu` with a description of your research needs. Include details such as:\n    *   Required operating system (Windows or Linux are supported).\n    *   Specific software packages needed.\n    *   Estimates for computational requirements (CPU cores, memory/RAM, disk storage).\n2.  **Consultation:** SCRC staff will schedule a meeting to discuss your requirements in detail.\n3.  **Provisioning:** A customized VM can typically be provisioned within a day or so after the consultation. A common starting configuration is 6 CPU cores and 32GB RAM, but this is adjustable.\n\n### Accessing Your Virtual Machine\n\nYou will receive specific instructions on how to connect to your VM once it is ready. Remember to connect to the [NYU VPN](link-to-nyu-vpn) first if you are accessing the VM from off-campus.\n\n## Cloud Server Hardware\n\nThe SCRC's private cloud environment is supported by powerful server hardware. While your VM provides isolated resources, it runs on physical servers such as these:\n\n*   **tiffany:** 766GB RAM, 96 CPU x 2.1 GHz Intel Xeon Platinum 8160\n*   **einstein:** 1.5TB RAM, 96 CPU x 2.1 GHz Intel Xeon Platinum 8160\n*   **hawking:** 768GB RAM, 56 CPU x 2.59 GHz Intel Xeon Gold 6132\n*   **nobel:** 384GB RAM, 64 CPU x 2.59 GHz Intel Xeon GPU E5-2697A v4\n\n**Note:** The specific resources allocated to your VM are determined during the request and consultation process, based on your needs and overall system availability.\n</code></pre>"},{"location":"cloud-computing/requesting-virtual-machines/","title":"Requesting Virtual Machines","text":"<pre><code>---\ntitle: \"Requesting a Virtual Machine\"\ncategory: \"cloud-computing\"\ndescription: \"How to request a dedicated virtual machine (VM) for specialized research needs.\"\n---\n\n# Requesting a Virtual Machine\n\nThis page explains how researchers at NYU Stern can request a dedicated virtual machine (VM) from the Stern Center for Research Computing (SCRC) for specialized research requirements.\n\n## Overview\n\nThe SCRC maintains a small VMWare cloud environment to support researchers who have specialized computing needs that are not met by the standard HPC cluster or other shared resources. This private cloud allows SCRC to quickly provide customized virtual machines.\n\nVMs offer several benefits:\n*   **Customization:** Specify the amount of memory, disk space, number of processors, operating system (Windows or Linux), and required software.\n*   **Accessibility:** Access your VM from anywhere (office, home, abroad) via the network. An NYU VPN connection is required for off-campus access.\n*   **Collaboration:** Facilitates collaboration with geographically distant colleagues.\n*   **Efficiency:** Quickly provisioned and configured computing resources without the financial overhead of purchasing physical hardware for individual needs.\n*   **Isolation:** Frees up your personal computer and provides a dedicated environment for your research tasks.\n\n## How to Request a VM\n\nTo request a dedicated virtual machine, follow these steps:\n\n1.  **Determine Your Needs:** Identify the specific requirements for your VM, such as:\n    *   Operating System (Windows or Linux)\n    *   Number of CPU cores\n    *   Amount of RAM (Memory)\n    *   Required Disk Space\n    *   Any specialized software needed\n\n2.  **Send an Email Request:** Compose an email to the SCRC support list:\n    *   **To:** `scrc-list@stern.nyu.edu`\n    *   **Subject:** Virtual Machine Request\n    *   **Body:** Include a clear description of your research needs and the required VM specifications (as determined in step 1).\n\n3.  **Consultation (If Necessary):** SCRC staff will review your request. They may schedule a brief meeting to discuss your requirements in more detail to ensure the VM is configured appropriately.\n\n4.  **Provisioning:** Once the details are finalized, SCRC can typically provision and provide access to the VM within a day or so.\n\n## Typical VM Specifications\n\nWhile specifications are customizable, a typical VM configuration provided by SCRC includes:\n\n*   **CPU:** 6 cores\n*   **RAM:** 32GB\n\nIf your needs differ significantly (e.g., requiring GPUs or substantially more resources), please specify this clearly in your request email.\n\n## Contact\n\nFor any questions regarding virtual machine requests or the SCRC cloud environment, please contact `scrc-list@stern.nyu.edu`.\n</code></pre>"},{"location":"computing-resources/","title":"Computing Resources","text":"<p>Welcome to the Computing Resources documentation for the SCRC cluster. This section provides essential information about Slurm, batch jobs, interactive sessions, resource management, and hardware specifications.</p> <p>Here, you'll learn about Slurm, the workload manager that schedules and manages jobs on the cluster. We cover:</p> <ul> <li>Batch Jobs: Ideal for non-interactive tasks that can run independently. You'll submit these using scripts.</li> <li>Interactive Sessions: Necessary when you need to interact directly with software or visualize results in real time.</li> </ul> <p>Effectively managing resources means understanding how to request processors (CPUs), memory (RAM), time, and potential access to GPUs.</p>"},{"location":"computing-resources/#topics-in-this-category","title":"Topics in this Category","text":"<p>Explore the following pages to learn more about specific aspects of using SCRC's computing resources:</p> <ul> <li>Slurm Overview and Common Commands<ul> <li>Introduction to the Slurm workload manager and descriptions of essential commands like <code>sbatch</code>, <code>squeue</code>, and <code>scancel</code>.</li> </ul> </li> <li>Submitting Batch Jobs<ul> <li>Detailed guide on creating Slurm batch scripts (<code>.sbatch</code> files) and submitting non-interactive jobs to the cluster.</li> </ul> </li> <li>Running Interactive Jobs with FastX<ul> <li>How to use FastX to start interactive graphical sessions on the cluster, connect via browser or client, and manage resources.</li> </ul> </li> <li>Starting Interactive Sessions (srun)<ul> <li>Using <code>srun</code> for interactive command-line sessions on compute nodes.</li> </ul> </li> <li>Slurm Array Jobs<ul> <li>Submitting and managing job arrays for high-throughput computing.</li> </ul> </li> <li>Hardware Specifications<ul> <li>Overview of available hardware, including CPUs, memory, GPUs, and networking.</li> </ul> </li> </ul>"},{"location":"computing-resources/hardware-specifications/","title":"Hardware Specifications","text":"<pre><code>---\ntitle: \"Hardware Specifications\"\ncategory: \"computing-resources\"\ndescription: \"Details about the computing hardware available at SCRC, including cloud server specifications and network infrastructure.\"\n---\n\n# Hardware Specifications\n\nThis page provides details about the computing hardware resources available at the NYU Stern Center for Research Computing (SCRC). This includes specifications for cloud servers and information about the network and storage infrastructure.\n\n## Cloud Servers\n\nSCRC provides access to several high-performance cloud servers suitable for various computational tasks, including GPU processing. The specifications for these servers are as follows:\n\n*   **tiffany:**\n    *   **RAM:** 766 GB\n    *   **CPU:** 96 x 2.1 GHz Intel Xeon Platinum 8160\n*   **einstein:**\n    *   **RAM:** 1.5 TB\n    *   **CPU:** 96 x 2.1 GHz Intel Xeon Platinum 8160\n*   **hawking:**\n    *   **RAM:** 768 GB\n    *   **CPU:** 56 x 2.59 GHz Intel Xeon Gold 6132\n*   **nobel:**\n    *   **RAM:** 384 GB\n    *   **CPU:** 64 x 2.59 GHz Intel Xeon GPU E5-2697A v4\n\nThese servers form part of a small VMware cloud environment used to support researchers with specialized needs, allowing for the creation of customized virtual machines (VMs).\n\n## Other Hardware\n\nBeyond the primary cloud servers, the SCRC infrastructure includes:\n\n*   **Network Switches:** NetGear 10GB switches are utilized to ensure high-speed intra-cluster and inter-cluster communication, facilitating efficient data transfer and parallel processing.\n*   **Storage:** The center employs both Network-Attached Storage (NAS) and Storage Area Network (SAN) devices, providing a total storage capacity exceeding 120 TB for research data. This includes the `bighome` directory space for large dataset storage.\n</code></pre>"},{"location":"computing-resources/running-interactive-jobs-fastx/","title":"Interactive Jobs (FastX)","text":"<pre><code>---\ntitle: \"Running Interactive Jobs with FastX\"\ncategory: \"computing-resources\"\ndescription: \"How to use FastX to start interactive graphical sessions on the cluster, connect via browser or client, and launch applications.\"\n---\n\n# Running Interactive Jobs with FastX\n\nUsing FastX with the Stern Slurm cluster provides fast and efficient remote access to graphical applications such as R-Studio, MATLAB, xStata, SAS, and more. This page explains how to access FastX, connect using the client or browser, and start an interactive session on a compute node.\n\n## Accessing FastX\n\nTo access FastX, you need your Stern NetID and password.\n\n**Prerequisites:**\n\n*   You must have an activated Stern account. See [Getting An Account](/computing-resources/getting-an-account) if you need help setting up your account.\n*   If you are connecting from off-campus (e.g., home), you **must** first connect to the NYU VPN.\n\n**FastX Session Nodes:**\n\nYou can connect to any of the following FastX session nodes:\n\n*   [https://fx1.scrc.nyu.edu:3300](https://fx1.scrc.nyu.edu:3300)\n*   [https://fx2.scrc.nyu.edu:3300](https://fx2.scrc.nyu.edu:3300)\n*   [https://fx3.scrc.nyu.edu:3300](https://fx3.scrc.nyu.edu:3300)\n\n**Login Process:**\n\n1.  Navigate to one of the FastX session node URLs in your web browser.\n2.  Log in using your Stern NetID and password.\n3.  Once logged in, you will see the FastX home page. This is where you manage your sessions.\n4.  To start a new session, click on the **GNOME Terminal** application icon on the left.\n5.  Before the session starts, you will be prompted to choose how you want to connect:\n    *   **Connect using Browser client:** Use your session within your web browser tab. See [Using FastX in a Browser](#using-fastx-in-a-browser).\n    *   **Connect using the Desktop client:** Use the dedicated FastX desktop application. We recommend using the FastX Client for a smoother experience. See [Connecting with the FastX Client](#connecting-with-the-fastx-client).\n\n**Note:** For potentially better performance, you can log in remotely to a physical machine in your office at NYU first and then connect to one of the FastX session nodes from there.\n\n## Connecting with the FastX Client\n\nThere are two main ways to use the FastX Desktop Client. First, ensure you have downloaded and installed the client from [StarNet (FastX Client Download)](https://www.starnet.com/fastx/download).\n\n### Option 1: Launching from the Web Interface\n\n1.  Follow the steps in [Accessing FastX](#accessing-fastx) and click the **GNOME Terminal** application icon.\n2.  When prompted, select **Connect using the Desktop client**.\n3.  The FastX client application should launch automatically and open a GNOME terminal window connected to the FastX session node (`fx1`, `fx2`, or `fx3`).\n4.  Proceed to [Using Modules with FastX](#using-modules-with-fastx) to start an interactive job on a compute node.\n\n### Option 2: Launching the Client Directly\n\n1.  Find and start the installed FastX client program on your local machine. You should see a **Connections** window.\n2.  Click the **+** icon to create a new connection.\n3.  Enter the connection details:\n    *   **Host:** `fx1.scrc.nyu.edu` (or `fx2`, `fx3`)\n    *   **User:** `&lt;Your Stern NetID&gt;`\n    *   **Port:** `22`\n    *   **Name:** (Choose a descriptive name, e.g., \"SCRC FastX\")\n4.  Click **Save**.\n5.  Double-click the connection you just created in the **Connections** window.\n6.  Enter your Stern password when prompted. The session window will appear.\n7.  Click the **+** icon in the session window to show the available applications.\n8.  Double-click the **GNOME Terminal** application to start a terminal session on the FastX session node.\n9.  Proceed to [Using Modules with FastX](#using-modules-with-fastx) to start an interactive job on a compute node.\n\n## Using FastX in a Browser\n\n1.  Follow the steps in [Accessing FastX](#accessing-fastx) and click the **GNOME Terminal** application icon.\n2.  When prompted, select **Connect using Browser client**.\n3.  A new browser tab will open, displaying a GNOME terminal session connected to the FastX session node (`fx1`, `fx2`, or `fx3`).\n4.  Proceed to the next section [Using Modules with FastX](#using-modules-with-fastx) to start an interactive job on a compute node.\n\n## Using Modules with FastX\n\nWhether you connected via the Desktop Client or the Browser, you will now have a terminal window open. This terminal is connected to one of the FastX **login nodes** (`fx1`, `fx2`, or `fx3`).\n\n**Important:** The FastX login nodes (`fx1`, `fx2`, `fx3`) have limited software installed and are **not intended for computational tasks**. You must request an interactive session on a compute node to run research applications.\n\nYou can see the limited modules available on the login node by typing:\n\n```bash\nmodule avail\n</code></pre>"},{"location":"computing-resources/running-interactive-jobs-fastx/#logging-in-to-an-interactive-compute-node","title":"Logging in to an Interactive Compute Node","text":"<p>To run interactive jobs, use the <code>srun</code> command to request resources and start a session on a compute node.</p> <pre><code>srun --pty --mem=8gb --time=1:00:00 --cpus-per-task=1 --nodes=1 /bin/bash\n</code></pre> <p>Command Breakdown:</p> <ul> <li><code>srun</code>: The Slurm command to run a job interactively.</li> <li><code>--pty</code>: Allocate a pseudo-terminal for the interactive session.</li> <li><code>--mem=8gb</code>: Request 8 gigabytes of memory for the job. Adjust as needed.</li> <li><code>--time=1:00:00</code>: Set a maximum wall-clock time limit of 1 hour. Adjust as needed (e.g., <code>--time=4:00:00</code> for 4 hours).</li> <li><code>--cpus-per-task=1</code>: Request 1 CPU core. Adjust as needed.</li> <li><code>--nodes=1</code>: Request 1 compute node.</li> <li><code>/bin/bash</code>: Execute the Bash shell once the resources are allocated.</li> </ul> <p>Optional Parameters:</p> <ul> <li><code>--partition=gpu</code>: Request a node with GPUs.</li> <li><code>--partition=bigmem</code>: Request a node with a large amount of RAM.</li> </ul> <p>After running <code>srun</code>, your command prompt will change, indicating you are now logged into a compute node (e.g., <code>compute-0-1</code>).</p>"},{"location":"computing-resources/running-interactive-jobs-fastx/#loading-modules-and-running-applications-on-the-compute-node","title":"Loading Modules and Running Applications on the Compute Node","text":"<p>Once logged into an interactive compute node via <code>srun</code>:</p> <ol> <li> <p>Check available software: Use the <code>module avail</code> command again to see the full list of software modules available on the compute nodes.</p> <p><code>bash module avail</code></p> </li> <li> <p>Load a module: Use the <code>module load</code> command followed by the module name and version (if needed). For example, to load R:</p> <p><code>bash module load R/4.0.2</code> (Replace <code>R/4.0.2</code> with the desired software and version shown by <code>module avail</code>)</p> </li> <li> <p>Run the application: Launch the graphical application by typing its command name. For example, to start RStudio after loading the R module:</p> <p><code>bash rstudio &amp;</code> (Using <code>&amp;</code> runs the application in the background, freeing up your terminal.)</p> </li> </ol> <p>The graphical application (e.g., RStudio) should now open in a new window managed by your FastX session. You can continue using the terminal to load other modules or run commands.</p> <p>Terminating Sessions:</p> <ul> <li>To exit the interactive compute node session (<code>srun</code>), type <code>exit</code> in the terminal.</li> <li>To close the FastX session entirely, close the terminal window(s) and either close the FastX client application or log out from the FastX web interface. You can also manage (terminate) sessions from the FastX home page. ```</li> </ul>"},{"location":"computing-resources/slurm-array-jobs/","title":"Slurm Array Jobs","text":"<pre><code>---\ntitle: \"Slurm Array Jobs\"\ncategory: \"computing-resources\"\ndescription: \"Explanation and examples of using Slurm array jobs to run multiple similar tasks efficiently.\"\n---\n\n# Slurm Array Jobs\n\nThis page explains how to use Slurm array jobs on the SCRC cluster. Array jobs provide an efficient mechanism for submitting and managing collections of similar jobs quickly and easily. They are particularly useful when you need to run the same program multiple times with different input parameters or datasets.\n\n## Overview\n\nInstead of writing multiple individual job scripts or submitting the same script repeatedly, you can use a single job script with the `--array` directive. Slurm then creates multiple \"tasks\" within a single job allocation, each with its own unique task ID. This task ID can be used within your script to vary parameters, input files, or output filenames.\n\nBenefits of using array jobs include:\n\n*   **Efficiency:** Submit and manage many tasks with a single command.\n*   **Scalability:** Easily scale up the number of tasks.\n*   **Resource Management:** Each task can request specific resources, though often they share the same resource profile.\n\n## Submitting Array Jobs\n\nTo submit an array job, use the `#SBATCH --array` directive in your Slurm script or the `--array` option with the `sbatch` command.\n\n### The `--array` Directive\n\nThe `--array` directive specifies the range and stepping of the task IDs.\n\n**Syntax:**\n\n```bash\n#SBATCH --array=&lt;indices&gt;\n</code></pre> <ul> <li><code>&lt;indices&gt;</code> can be specified in several ways:<ul> <li>A simple range: <code>1-5</code> (Tasks 1, 2, 3, 4, 5)</li> <li>Specific indices: <code>0,1,5,10</code> (Tasks 0, 1, 5, 10)</li> <li>A range with a step: <code>1-10:2</code> (Tasks 1, 3, 5, 7, 9)</li> <li>A range with step and limit on concurrent tasks: <code>1-100%10</code> (Tasks 1 to 100, but only 10 run concurrently)</li> </ul> </li> </ul> <p>Example in <code>sbatch</code> script:</p> <pre><code>#SBATCH --array=1-5  # Creates 5 tasks with IDs 1, 2, 3, 4, 5\n</code></pre> <p>Example with <code>sbatch</code> command:</p> <p>You can also specify the array range directly when submitting the job, which overrides any <code>#SBATCH --array</code> directive in the script.</p> <pre><code>sbatch --array=5-15:5 my_script.sbatch # Creates 3 tasks with IDs 5, 10, 15\n</code></pre>"},{"location":"computing-resources/slurm-array-jobs/#using-the-task-id","title":"Using the Task ID","text":"<p>Slurm sets the environment variable <code>SLURM_ARRAY_TASK_ID</code> for each task within the array job. You can use this variable in your job script to control the behavior of each task. Common uses include:</p> <ul> <li>Selecting different input files based on the task ID.</li> <li>Passing different parameters to your program.</li> <li>Creating unique output directories or filenames for each task.</li> </ul> <p>Example Usage in Script:</p> <pre><code>#!/bin/bash\n#SBATCH --array=1-3\n#SBATCH --output=task_%a.out # Use %a for task ID in output name\n\nINPUT_FILE=\"data_part_${SLURM_ARRAY_TASK_ID}.txt\"\nOUTPUT_FILE=\"result_${SLURM_ARRAY_TASK_ID}.log\"\n\necho \"Running task ${SLURM_ARRAY_TASK_ID}\"\necho \"Input file: ${INPUT_FILE}\"\necho \"Output file: ${OUTPUT_FILE}\"\n\n# Your command using the task-specific variables\n# ./my_program --input ${INPUT_FILE} --output ${OUTPUT_FILE}\n</code></pre>"},{"location":"computing-resources/slurm-array-jobs/#managing-output-files","title":"Managing Output Files","text":"<p>It's crucial to manage output files so that tasks don't overwrite each other's results. Slurm provides placeholders for job and task IDs in the <code>#SBATCH --output</code> and <code>#SBATCH --error</code> directives:</p> <ul> <li><code>%A</code>: Replaced by the main Job ID.</li> <li><code>%a</code>: Replaced by the Array Task ID.</li> </ul> <p>Example:</p> <pre><code>#SBATCH --job-name=myArrayJob\n#SBATCH --array=1-10\n#SBATCH --output=myArrayJob.%A-%a.out  # Creates files like myArrayJob.12345-1.out, myArrayJob.12345-2.out, etc.\n#SBATCH --error=myArrayJob.%A-%a.err\n</code></pre>"},{"location":"computing-resources/slurm-array-jobs/#examples","title":"Examples","text":"<p>Run <code>getSlurmExamples</code> at the command line of <code>rnd</code> or <code>vleda</code> to have all the tutorials copied to your home directory.</p>"},{"location":"computing-resources/slurm-array-jobs/#python-example-real-volatility-calculation","title":"Python Example: Real Volatility Calculation","text":"<p>This example runs a Python script (<code>realVol.py</code>) five times, each time calculating the real volatility for a different input price series file (<code>series1.txt</code> to <code>series5.txt</code>).</p> <p>Python Script (<code>realVol.py</code>)</p> <p>This script reads price data from standard input and calculates realized volatility.</p> <pre><code>#------------------------------------\n# realVol.py\n# Calculate Real Volitility\n#------------------------------------\nimport sys\nimport math\n\ndef main():\n    sumsq = 0\n    n = 0\n    # read first price in file\n    price_previous = sys.stdin.readline()\n    # read each subsequent price in file\n    for price_current in sys.stdin:\n        # calculate the daily return\n        daily_return = math.log(float(price_current) / float(price_previous))\n        # compute the sum of squares\n        sumsq = sumsq + daily_return ** 2\n        price_previous = price_current\n        n = n + 1\n    # compute and output realized volatility\n    real_vol = 100 * math.sqrt((252.0 / n) * sumsq)\n    print(\"realized volatility = %.2f\" % (real_vol))\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Slurm Script (<code>realVol.sbatch</code>)</p> <p>This script uses <code>#SBATCH --array=1-5</code> to create 5 tasks. The <code>${SLURM_ARRAY_TASK_ID}</code> variable is used to select the correct input file (<code>series&lt;ID&gt;.txt</code>) for each task via input redirection. Output files are named using the Job ID (<code>%A</code>) and Task ID (<code>%a</code>).</p> <pre><code>#!/bin/bash\n##------------------------------------\n# realVol.sbatch\n# Slurm job script\n#------------------------------------\n#SBATCH --job-name=realVol                 # set the name of the job\n#SBATCH --array=1-5                        # create an array job with task IDs ranging from 1 to 5\n#SBATCH --export=ALL                       # export env variables to the compute nodes\n#SBATCH --mem=512m                         # set the amount of memory required for each task to 512 megabytes\n#SBATCH --mail-type=BEGIN,END,FAIL        # send email notifications when the job starts, ends or fails\n#SBATCH --output=realVol.%A-%a.out       # set the output file name for each task. The %A is replaced by the job ID and %a is replaced by the task ID.\n#SBATCH --partition=test                   # specify the partition to run the job in. In this case, it is \"test\".\n#SBATCH --time=00:10:00                   # set the maximum time limit for each task to 10 minutes. If a task exceeds this time limit, it will be terminated.\n\n# The --array directive in Slurm allows a single job script to be executed multiple times, each with a different input parameter.\n# When a job is submitted with --array, Slurm automatically creates an array of tasks, with each task corresponding to a specific value or range of values for the input parameter.\n# The tasks can be executed in parallel on different nodes, and each task can be identified using its task ID, which is automatically assigned by Slurm.\n# This feature is particularly useful when running a large number of similar tasks, such as parameter sweeps or simulations with different initial conditions.\n# For our use case, we are using it to pass in files dynamically.\n\necho python realVol.py &lt; `bash -c \"unset noglob; ls series${SLURM_ARRAY_TASK_ID}*.txt\"`\npython realVol.py &lt; `bash -c \"unset noglob; ls series${SLURM_ARRAY_TASK_ID}*.txt\"`\n</code></pre> <p>Submission Command:</p> <pre><code>sbatch realVol.sbatch\n</code></pre>"},{"location":"computing-resources/slurm-array-jobs/#r-example-fitspline-model","title":"R Example: Fitspline Model","text":"<p>This example runs an R script (<code>fitspline.R</code>) multiple times. The script fits a cubic smoothing spline to simulated data. The amount of noise added to the data depends on the Slurm Task ID, and the output plot filename also incorporates the task ID.</p> <p>R Script (<code>fitspline.R</code>)</p> <p>This script retrieves the task ID using <code>Sys.getenv(\"SLURM_ARRAY_TASK_ID\")</code>, uses it to calculate the noise variance (<code>v</code>), and generates a task-specific output filename (<code>result_&lt;ID&gt;.ps</code>).</p> <pre><code># fitspline.R\n\n# Set the initial seed for the random number generator.\nset.seed(sample(1:1000, 1))\n\n# Create n = 100 random data points.\n# x is n equally spaced values from 0 to 1.\nn &lt;- 100\nx &lt;- (1:n) / n\n\n# The model in this simulation (no random error)\nmval &lt;- ((exp(x / 3) - 2 * exp(-7 * x) + sin(9 * x)) + 1) / 3\n\n# Generate n independent normal random variates with mean 0\n# and variance derived from the task id\ntid &lt;- as.integer(Sys.getenv(\"SLURM_ARRAY_TASK_ID\")) # Get Task ID\nv &lt;- tid / 100 # Use Task ID to vary noise\nnoise &lt;- rnorm(n, 0, v)\n\n# Simulated observed values (model value + noise)\ny &lt;- mval + noise\n\n# Fit a cubic smoothing spline to the data\nfit &lt;- smooth.spline(x, y, cv = FALSE, all.knots = TRUE)\n\n# Create a graph that shows the data, the smoothing spline,\n# and the original model\nr &lt;- paste(\"result_\", tid, \".ps\", sep = \"\") # Use Task ID in filename\npostscript(r, height = 8, width = 10, horizo = FALSE)\n\n# Plot data points\nplot(x, y, xlab = \"x\", ylab = \"y\", cex = 0.5)\n\n# Plot original model values without noise\nlines(x, mval, lty = 2)\n\n# Plot smooth spline fit\nlines(fit$x, fit$y)\n\n# Save the graph to a PS file\ngraphics.off()\n</code></pre> <p>Slurm Script (<code>fitspline.sbatch</code>)</p> <p>This script loads the necessary R module and executes the R script using <code>R CMD BATCH</code>. The output R log file is named using the <code>${SLURM_ARRAY_TASK_ID}</code> variable. Note the comment indicating how to submit this script as an array job with specific task IDs.</p> <pre><code>#!/bin/bash\n#\n# [ fitspline.sbatch ]\n#\n# This script demonstrates how to run an R job, specifically,\n# how to fit a smooth spline model. It uses a range of SLURM_ARRAY_TASK_ID's\n# as a noise parameter for the model.\n# --------------------------------------------------------------------\n# Submit this job via 'sbatch' which accepts these command line options:\n#SBATCH --job-name=fitsplineJob        # Define the name of the current job.\n#SBATCH --output=fitsplineJob.Rout     # Define the stdout (the terminal output) file name. (Note: Overridden by R CMD BATCH output redirection below)\n#SBATCH --error=fitsplineJob.err      # Define the stderr (the terminal error output) file name.\n#SBATCH --export=ALL                  # Export ALL environment variables.\n#SBATCH --mail-type=END,FAIL           # Send email when the job ENDs or FAILs.\n#SBATCH --mail-user=your-stern-netid@stern.nyu.edu  # Specify your Stern email address.\n#SBATCH --mem=512m                    # Specifies the maximum memory per task.\n#SBATCH --time=00:10:00               # The wall-clock time limit per task.\n#SBATCH --partition=test               # Specify the partition.\n# --------------------------------------------------------------------\n# How to submit an array job to Stern Slurm cluster.\n# Pass array variable values 5, 10, 15 to the R script.\n#\n# sbatch --array=5-15:5 fitspline.sbatch\n# --------------------------------------------------------------------\n\n# Select stat package and version to use\nmodule purge\nmodule load R/4.3.2\n\n# Run R script, directing R output to a task-specific file\nR CMD BATCH --no-save --no-restore fitspline.R fitspline.$SLURM_ARRAY_TASK_ID.Rout\n</code></pre> <p>Submission Command:</p> <p>This command submits the script and creates tasks with IDs 5, 10, and 15.</p> <pre><code>sbatch --array=5-15:5 fitspline.sbatch\n</code></pre>"},{"location":"computing-resources/slurm-array-jobs/#notes-and-considerations","title":"Notes and Considerations","text":"<ul> <li>Resource Allocation: Resources specified with <code>#SBATCH</code> directives (like <code>--mem</code>, <code>--time</code>, <code>--cpus-per-task</code>) apply to each task in the array job individually.</li> <li>Job Monitoring: You can monitor array jobs using <code>squeue</code>. Tasks will often appear with an underscore and the task ID appended to the job ID (e.g., <code>12345_1</code>, <code>12345_2</code>).</li> <li>Canceling Tasks: You can cancel the entire array job (<code>scancel &lt;JobID&gt;</code>) or individual tasks (<code>scancel &lt;JobID&gt;_&lt;TaskID&gt;</code>). ```</li> </ul>"},{"location":"computing-resources/slurm-overview-commands/","title":"Slurm Overview and Commands","text":"<pre><code>---\ntitle: \"Slurm Overview and Common Commands\"\ncategory: \"computing-resources\"\ndescription: \"Introduction to the Slurm workload manager and descriptions of essential commands like sbatch, squeue, sinfo, srun, and scancel.\"\n---\n\n# Slurm Overview and Common Commands\n\nThis document provides an overview of the Slurm Workload Manager used on the NYU Stern Center for Research Computing (SCRC) clusters and details the usage of common Slurm commands essential for managing jobs.\n\n## Slurm Overview\n\nThe SCRC Slurm cluster is a collection of moderate-sized compute nodes designed to support a wide range of computational tasks, including data analysis, simulations, and modeling. Slurm (Simple Linux Utility for Resource Management) is the workload manager responsible for accepting, scheduling, and managing computational jobs submitted to the cluster.\n\nResearchers can interact with the Slurm cluster in two primary ways:\n\n1.  **Batch Jobs:** Users submit scripts (using `sbatch`) that define the resources needed and the commands to execute. Slurm runs these jobs non-interactively when the requested resources become available.\n2.  **Interactive Jobs:** Users request resources (using `srun`) to get direct, real-time access to compute nodes for tasks like debugging, interactive data analysis, or running graphical applications.\n\n## Common Slurm Commands\n\nThe following commands are fundamental for interacting with the Slurm scheduler:\n\n| Command  | Usage                                     | Description                                       | Example                                                            |\n| :------- | :---------------------------------------- | :------------------------------------------------ | :----------------------------------------------------------------- |\n| `sbatch` | `sbatch [options] &lt;script_file&gt;`          | Submits a batch job script to the Slurm scheduler | `sbatch myScript.sbatch`                                           |\n| `squeue` | `squeue [options]`                        | Displays the status of jobs in the queue          | `squeue -u $USER`                                                  |\n| `sinfo`  | `sinfo [options]`                         | Provides information about Slurm nodes and queues | `sinfo`                                                            |\n| `srun`   | `srun [options] &lt;executable&gt; [arguments]` | Runs a parallel job or starts an interactive session | `srun --pty --mem=8gb --time=1:00:00 /bin/bash`                    |\n| `scancel`| `scancel [options] &lt;job_id&gt;`              | Cancels a pending or running job                  | `scancel 1203`                                                     |\n\n### `sbatch`\n\nThe `sbatch` command submits a job script for later execution. The script typically contains Slurm directives (lines starting with `#SBATCH`) that specify job options, resource requirements, and the commands to be executed.\n\n**Syntax:**\n\n```bash\nsbatch [options] &lt;script_file&gt;\n</code></pre> <p>Common <code>#SBATCH</code> Directives (within the script file):</p> <ul> <li><code>--job-name=&lt;job_name&gt;</code>: Specifies a name for the job.</li> <li><code>--output=&lt;output_file&gt;</code>: Specifies the file to redirect stdout. <code>%j</code> can be used for job ID, <code>%A</code> for job array master ID, <code>%a</code> for array task ID.</li> <li><code>--error=&lt;error_file&gt;</code>: Specifies the file to redirect stderr.</li> <li><code>--export=ALL</code>: Exports all environment variables from the submission environment to the job environment.</li> <li><code>--time=&lt;time_limit&gt;</code>: Sets the maximum wall-clock time limit (e.g., <code>01:30:00</code> for 1 hour 30 minutes, <code>2-00:00:00</code> for 2 days).</li> <li><code>--mem=&lt;memory&gt;</code>: Specifies the real memory required per node (e.g., <code>4G</code> for 4 Gigabytes, <code>512m</code> for 512 Megabytes).</li> <li><code>--nodes=&lt;node_count&gt;</code>: Requests a specific number of nodes.</li> <li><code>--cpus-per-task=&lt;cpu_count&gt;</code>: Requests a specific number of CPU cores per task.</li> <li><code>--partition=&lt;partition_name&gt;</code>: Specifies the partition (queue) to submit the job to (e.g., <code>test</code>, <code>gpu</code>, <code>bigmem</code>).</li> <li><code>--mail-type=&lt;event_list&gt;</code>: Sends email notifications for specified events (e.g., <code>BEGIN</code>, <code>END</code>, <code>FAIL</code>, <code>ALL</code>).</li> <li><code>--mail-user=&lt;email_address&gt;</code>: Specifies the email address for notifications.</li> <li><code>--array=&lt;task_range&gt;</code>: Submits a job array (e.g., <code>1-5</code>, <code>5-15:5</code>).</li> </ul> <p>Example Usage:</p> <pre><code>sbatch my_analysis_script.sbatch\n</code></pre> <p>Example Slurm Script (<code>hello-world.sbatch</code>):</p> <pre><code>#!/bin/bash\n#\n# [hello-world.sbatch]\n#\n#SBATCH --job-name=hello            # Job name\n#SBATCH --output=hello.%j.out       # Output file name (%j expands to jobID)\n#SBATCH --export=ALL               # Export all environment variables\n#SBATCH --time=00:01:00             # Set max runtime of job = 1 minute\n#SBATCH --mem=4G                    # Request 4 gigabytes of memory\n#SBATCH --mail-type=END,FAIL        # Send email notifications for job end or failure\n#SBATCH --mail-user=your_netid@stern.nyu.edu # Email address for notifications\n#SBATCH --partition=test            # Specify the partition to submit the job to\n\nmodule purge                         # Start with a clean environment\nmodule load python/3.9.7           # Load python module\npython hello-world.py               # Run the python script\n</code></pre> <p>(Note: <code>hello-world.py</code> should contain the Python code to be executed.)</p>"},{"location":"computing-resources/slurm-overview-commands/#squeue","title":"<code>squeue</code>","text":"<p>The <code>squeue</code> command shows the state of jobs in the Slurm scheduling queue.</p> <p>Syntax:</p> <pre><code>squeue [options]\n</code></pre> <p>Common Options:</p> <ul> <li><code>-u &lt;username&gt;</code>: Displays jobs for a specific user (e.g., <code>-u $USER</code> for your jobs).</li> <li><code>-p &lt;partition_name&gt;</code>: Displays jobs in a specific partition.</li> <li><code>-t &lt;state&gt;</code>: Displays jobs in a specific state (e.g., <code>PENDING</code>, <code>RUNNING</code>). Common states are <code>PD</code> (Pending), <code>R</code> (Running), <code>CG</code> (Completing).</li> </ul> <p>Example Usage:</p> <pre><code># Show all jobs in the queue\nsqueue\n\n# Show only your jobs\nsqueue -u $USER\n\n# Show jobs for user xy12\nsqueue -u xy12\n</code></pre>"},{"location":"computing-resources/slurm-overview-commands/#sinfo","title":"<code>sinfo</code>","text":"<p>The <code>sinfo</code> command reports the state of partitions and nodes managed by Slurm.</p> <p>Syntax:</p> <pre><code>sinfo [options]\n</code></pre> <p>Example Usage:</p> <pre><code># Show summary information about partitions and nodes\nsinfo\n</code></pre> <p>This command helps you see available partitions (<code>gpu</code>, <code>bigmem</code>, <code>test</code>, etc.) and the status of nodes within them (idle, allocated, down).</p>"},{"location":"computing-resources/slurm-overview-commands/#srun","title":"<code>srun</code>","text":"<p>The <code>srun</code> command is used to submit jobs for execution in real-time or to allocate resources for an interactive session.</p> <p>Syntax:</p> <pre><code>srun [options] &lt;executable&gt; [arguments]\n</code></pre> <p>Common Options for Interactive Sessions:</p> <ul> <li><code>--pty</code>: Allocates a pseudo-terminal, necessary for interactive shell sessions.</li> <li><code>--mem=&lt;memory&gt;</code>: Requests a specific amount of memory (e.g., <code>8gb</code>, <code>16G</code>).</li> <li><code>--time=&lt;time_limit&gt;</code>: Sets a time limit for the interactive session (e.g., <code>1:00:00</code>).</li> <li><code>--cpus-per-task=&lt;cpu_count&gt;</code>: Requests a number of CPU cores (e.g., <code>1</code>, <code>4</code>).</li> <li><code>--nodes=&lt;node_count&gt;</code>: Requests a number of compute nodes (usually <code>1</code> for interactive sessions).</li> <li><code>--partition=&lt;partition_name&gt;</code>: Specifies the partition to run on (e.g., <code>gpu</code>, <code>bigmem</code>). If omitted, uses a default partition.</li> <li><code>&lt;executable&gt;</code>: The program to run, often <code>/bin/bash</code> to start an interactive shell.</li> </ul> <p>Example Usage (Starting an Interactive Shell):</p> <pre><code># Request an interactive session with 8GB RAM, 1 CPU, for 1 hour\nsrun --pty --mem=8gb --time=1:00:00 --cpus-per-task=1 --nodes=1 /bin/bash\n\n# Request an interactive session on a GPU node\nsrun --pty --mem=16gb --time=2:00:00 --cpus-per-task=2 --nodes=1 --partition=gpu /bin/bash\n</code></pre> <p>Once the resources are allocated, your command prompt will change, indicating you are now on a compute node instead of a login node. You can then load modules and run commands directly on the allocated node.</p>"},{"location":"computing-resources/slurm-overview-commands/#scancel","title":"<code>scancel</code>","text":"<p>The <code>scancel</code> command is used to cancel pending or running Slurm jobs.</p> <p>Syntax:</p> <pre><code>scancel [options] &lt;job_id&gt;[_&lt;task_id&gt;]\n</code></pre> <p>Common Options:</p> <ul> <li><code>-u &lt;username&gt;</code>: Cancels jobs belonging to the specified user.</li> <li><code>-n &lt;job_name&gt;</code>: Cancels jobs with the specified name.</li> <li><code>-t &lt;state&gt;</code>: Cancels jobs in the specified state (e.g., <code>PENDING</code>).</li> </ul> <p>Example Usage:</p> <pre><code># Cancel job with ID 1203\nscancel 1203\n\n# Cancel task 5 of job array 1204\nscancel 1204_5\n\n# Cancel all pending jobs for the current user\nscancel -t PENDING -u $USER\n\n# Cancel all jobs for the current user\nscancel -u $USER\n</code></pre> <p>```</p>"},{"location":"computing-resources/starting-interactive-sessions-srun/","title":"Interactive Sessions (srun)","text":"<pre><code>---\ntitle: \"Starting Interactive Compute Sessions (srun)\"\ncategory: \"computing-resources\"\ndescription: \"Using the 'srun' command to allocate resources and start interactive command-line sessions on compute nodes.\"\n---\n\n# Starting Interactive Compute Sessions (srun)\n\nThis document describes how to use the `srun` command to request computational resources and start an interactive command-line session directly on a compute node within the SCRC Slurm cluster. Interactive sessions are useful for tasks like debugging code, running short tests, or using applications with graphical interfaces (via FastX).\n\nUnlike batch jobs submitted with `sbatch`, an interactive session provides a real-time command prompt on the allocated compute node.\n\n## Starting an Interactive Session\n\nTo start an interactive session, you first need to be logged into one of the SCRC login nodes (`rnd.scrc.nyu.edu` or `vleda.scrc.nyu.edu`) or a FastX node (`fx1`, `fx2`, `fx3`).\n\nFrom the command line, use the `srun` command with the `--pty` option and specify the resources you need. The final argument is the command you want to run, typically `/bin/bash` to get a shell prompt.\n\n**Basic Command Structure:**\n\n```bash\nsrun [options] /bin/bash\n</code></pre> <p>Common Example:</p> <p>This command requests 1 CPU core, 8GB of memory, and 1 node for a duration of 1 hour, then starts a Bash shell session on the allocated node.</p> <pre><code>srun --pty --mem=8gb --time=1:00:00 --cpus-per-task=1 --nodes=1 /bin/bash\n</code></pre> <p>Once the resources are allocated, your command prompt will change, indicating you are now on a compute node (e.g., <code>[user@compute-node-name ~]$</code>) rather than a login or FastX node.</p>"},{"location":"computing-resources/starting-interactive-sessions-srun/#common-parameters","title":"Common Parameters","text":"<p>Here are explanations for the frequently used parameters in the example above:</p> <ul> <li><code>srun</code>: The Slurm command to run a job interactively (or as part of a batch job).</li> <li><code>--pty</code>: Allocates a pseudo-terminal, making the session interactive. This is crucial for getting a command prompt.</li> <li><code>--mem=&lt;size&gt;</code>: Specifies the amount of memory required for the job. Use units like <code>mb</code> (megabytes) or <code>gb</code> (gigabytes). Example: <code>--mem=8gb</code>.</li> <li><code>--time=&lt;hh:mm:ss&gt;</code>: Sets the maximum wall-clock time limit for the job. Your session will be terminated after this time. Example: <code>--time=1:00:00</code> for 1 hour. Be mindful of partition limits.</li> <li><code>--cpus-per-task=&lt;number&gt;</code>: Requests a specific number of CPU cores for your task. Example: <code>--cpus-per-task=1</code>.</li> <li><code>--nodes=&lt;number&gt;</code>: Requests a specific number of compute nodes. For most interactive sessions, this will be <code>--nodes=1</code>.</li> <li><code>/bin/bash</code>: The command to execute once the resources are allocated. <code>/bin/bash</code> starts an interactive Bash shell.</li> </ul>"},{"location":"computing-resources/starting-interactive-sessions-srun/#requesting-specific-resources-partitions","title":"Requesting Specific Resources (Partitions)","text":"<p>You can request nodes with specific characteristics (like GPUs or large amounts of memory) by specifying a partition using the <code>--partition</code> option.</p> <ul> <li> <p>Requesting a GPU Node:</p> <p><code>bash srun --pty --mem=8gb --time=1:00:00 --cpus-per-task=1 --nodes=1 --partition=gpu /bin/bash</code></p> </li> <li> <p>Requesting a High-Memory Node:</p> <p><code>bash srun --pty --mem=64gb --time=2:00:00 --cpus-per-task=4 --nodes=1 --partition=bigmem /bin/bash</code></p> </li> </ul> <p>You can view available nodes and partitions using the <code>sinfo</code> command.</p>"},{"location":"computing-resources/starting-interactive-sessions-srun/#example-workflow","title":"Example Workflow","text":"<p>A common workflow for using an interactive session involves:</p> <ol> <li>Logging into an SCRC login node or FastX node.</li> <li>Using <code>srun</code> to allocate resources and get a shell on a compute node.</li> <li>Loading necessary software modules using the <code>module</code> command.</li> <li>Running your application or commands.</li> <li>Exiting the shell (typing <code>exit</code> or <code>Ctrl+D</code>) when finished, which releases the allocated resources.</li> </ol> <p>Example: Running RStudio interactively via FastX</p> <pre><code># 1. Log in to a FastX node (e.g., fx1.scrc.nyu.edu) and open a GNOME Terminal\n\n# 2. Request an interactive session on a compute node\nsrun --pty --mem=8gb --time=1:00:00 --cpus-per-task=1 --nodes=1 /bin/bash\n\n# ---&gt; Wait for resource allocation. Prompt changes to compute node prompt. &lt;---\n# Example prompt: [yourNetID@compute-node-XX ~]$\n\n# 3. Load the RStudio module\nmodule load rstudio\n\n# 4. Launch RStudio\nrstudio\n\n# ---&gt; RStudio GUI opens &lt;---\n\n# 5. When finished with RStudio, close its window.\n#    Then, exit the interactive shell on the compute node:\nexit\n# ---&gt; Returns you to the FastX node prompt. &lt;---\n</code></pre>"},{"location":"computing-resources/starting-interactive-sessions-srun/#notes","title":"Notes","text":"<ul> <li>Do not run computationally intensive tasks directly on login nodes (<code>rnd</code>, <code>vleda</code>) or FastX nodes (<code>fx1</code>, <code>fx2</code>, <code>fx3</code>). These nodes are shared resources for accessing the cluster, editing files, and submitting jobs. Use <code>srun</code> or <code>sbatch</code> to run computations on the compute nodes.</li> <li>Be realistic with resource requests (<code>--time</code>, <code>--mem</code>, <code>--cpus-per-task</code>). Requesting excessive resources may increase your wait time in the queue.</li> <li>Your interactive session will be terminated automatically when the requested <code>--time</code> limit is reached. Save your work frequently. ```</li> </ul>"},{"location":"computing-resources/submitting-batch-jobs/","title":"Submitting Batch Jobs","text":"<pre><code>---\ntitle: \"Submitting Batch Jobs\"\ncategory: \"computing-resources\"\ndescription: \"Detailed guide on creating Slurm batch scripts (.sbatch files) and submitting non-interactive jobs to the cluster.\"\n---\n\n# Submitting Batch Jobs\n\nThis guide provides detailed instructions on how to create Slurm batch scripts (`.sbatch` files) and submit non-interactive jobs to the SCRC Slurm cluster. Batch jobs are ideal for computationally intensive tasks that do not require direct user interaction during execution.\n\n## What is a Batch Job?\n\nA batch job is a script containing commands and Slurm directives that is submitted to the Slurm workload manager. Slurm schedules the job to run on the cluster's compute nodes when the requested resources become available. This allows you to queue up tasks and let them run in the background without needing to stay logged in or actively monitor them. The output and errors are typically written to files for later review.\n\n## Creating a Slurm Batch Script (`.sbatch`)\n\nA Slurm batch script is a text file, typically ending with `.sbatch`, that contains two main components:\n1.  `#SBATCH` directives: These lines, starting with `#SBATCH`, provide instructions to the Slurm scheduler about the job's requirements (e.g., runtime, memory, partition) and behavior (e.g., email notifications, output files).\n2.  Shell commands: These are the commands needed to set up the environment and execute your program (e.g., loading modules, running Python/SAS/MATLAB/R scripts).\n\n### Basic Structure\n\n```bash\n#!/bin/bash\n#\n# [your-script-name.sbatch]\n# Description of what the script does\n\n# --- SLURM Directives ---\n#SBATCH --job-name=your_job_name      # Job name\n#SBATCH --output=output_file.%j.out   # Standard output file (%j expands to job ID)\n#SBATCH --error=error_file.%j.err     # Standard error file (%j expands to job ID)\n#SBATCH --time=HH:MM:SS               # Wall clock time limit\n#SBATCH --mem=MemoryG                 # Memory requirement (e.g., 4G, 8G)\n#SBATCH --partition=partition_name    # Partition (queue) to submit to (e.g., test, gpu)\n#SBATCH --mail-type=BEGIN,END,FAIL    # Email notifications\n#SBATCH --mail-user=your_netid@stern.nyu.edu # Your email address\n\n# --- Environment Setup ---\necho \"Job started on $(hostname) at $(date)\"\nmodule purge                          # Start with a clean environment\nmodule load software/version          # Load necessary modules (e.g., python/3.9.7, sas/9.4)\n\n# --- Job Execution ---\n# Your commands go here\n# Example: python your_script.py\n# Example: sas your_script.sas\n# Example: R CMD BATCH your_script.R\n\necho \"Job finished at $(date)\"\n</code></pre>"},{"location":"computing-resources/submitting-batch-jobs/#common-sbatch-directives","title":"Common <code>#SBATCH</code> Directives","text":"Directive Description Example <code>--job-name</code> Specifies a name for the job. <code>--job-name=my_analysis</code> <code>--output</code> Specifies the file path for standard output. <code>%j</code> is replaced by the job ID, <code>%A</code> by job array ID, <code>%a</code> by task ID. <code>--output=job_%j.out</code> <code>--error</code> Specifies the file path for standard error. If omitted, stderr is merged with stdout. <code>--error=job_%j.err</code> <code>--time</code> Sets the maximum wall clock time limit (Hours:Minutes:Seconds). <code>--time=02:30:00</code> <code>--mem</code> Specifies the memory required per node (e.g., M for Megabytes, G for Gigabytes). <code>--mem=8G</code> <code>--partition</code> Specifies the partition (queue) to submit the job to. Common partitions include <code>test</code>, <code>gpu</code>, <code>bigmem</code>. <code>--partition=test</code> <code>--mail-type</code> Specifies events for email notification (e.g., <code>BEGIN</code>, <code>END</code>, <code>FAIL</code>, <code>ALL</code>). <code>--mail-type=END,FAIL</code> <code>--mail-user</code> Specifies the email address for notifications. <code>--mail-user=xy12@stern.nyu.edu</code> <code>--export</code> Exports environment variables from the submission environment to the job. <code>ALL</code> exports all variables. <code>--export=ALL</code> <code>--nodes</code> Specifies the number of nodes to allocate. <code>--nodes=1</code> <code>--cpus-per-task</code> Specifies the number of CPU cores required per task. <code>--cpus-per-task=4</code> <code>--array</code> Submits a job array. Specifies the task ID range (e.g., <code>1-5</code>, <code>1-10:2</code>). <code>--array=1-10</code> <code>--gres</code> or <code>--gpus</code> Requests generic resources, commonly used for GPUs. <code>--gres=gpu:1</code> or <code>--gpus=1</code> <code>--gpu-bind</code> Defines how tasks should be bound to GPUs (relevant on multi-GPU nodes). <code>--gpu-bind=closest</code>"},{"location":"computing-resources/submitting-batch-jobs/#loading-modules","title":"Loading Modules","text":"<p>It's crucial to load the correct software modules within your batch script to ensure your program runs with the intended environment. Always start with <code>module purge</code> to clear any potentially conflicting modules inherited from your login session, then load the specific modules you need.</p> <pre><code>module purge                # Clear existing modules\nmodule load python/3.9.7    # Load a specific Python version\nmodule load sas/9.4         # Load SAS\nmodule load R/4.3.2         # Load R\nmodule load matlab/2019a    # Load MATLAB\n</code></pre>"},{"location":"computing-resources/submitting-batch-jobs/#executing-your-code","title":"Executing Your Code","text":"<p>After setting up the environment, include the command(s) needed to run your program.</p> <ul> <li>Python: <code>bash     python your_script.py</code><ul> <li>If using a virtual environment:     <code>bash     source /path/to/your/venv/bin/activate # Activate the environment     python your_script.py     deactivate # Optional: Deactivate afterwards</code></li> </ul> </li> <li>SAS: <code>bash     sas -nodms your_script.sas</code></li> <li>MATLAB: (using <code>-nojvm</code> can save resources if no Java-based features are needed)     <code>bash     matlab -nojvm &lt; your_script.m</code></li> <li>R: <code>bash     R CMD BATCH --no-save --no-restore your_script.R output_file.Rout</code><ul> <li>The <code>--no-save</code> and <code>--no-restore</code> flags prevent saving/restoring the R workspace.</li> <li>Output is typically directed to a <code>.Rout</code> file.</li> </ul> </li> </ul>"},{"location":"computing-resources/submitting-batch-jobs/#submitting-the-batch-job-sbatch","title":"Submitting the Batch Job (<code>sbatch</code>)","text":"<p>Once your <code>.sbatch</code> script is created, you submit it to the Slurm scheduler using the <code>sbatch</code> command from one of the login nodes (<code>rnd.scrc.nyu.edu</code> or <code>vleda.scrc.nyu.edu</code>).</p>"},{"location":"computing-resources/submitting-batch-jobs/#command-syntax","title":"Command Syntax","text":"<pre><code>sbatch [options] your_script.sbatch\n</code></pre> <ul> <li><code>[options]</code>: Optional flags that can override directives within the script (e.g., <code>sbatch --time=1:00:00 your_script.sbatch</code>).</li> <li><code>your_script.sbatch</code>: The path to your Slurm batch script.</li> </ul>"},{"location":"computing-resources/submitting-batch-jobs/#example-submissions","title":"Example Submissions","text":"<pre><code># Submit a simple job\nsbatch hello-world.sbatch\n\n# Submit a SAS job\nsbatch crosstab.sbatch\n\n# Submit a MATLAB GPU job\nsbatch gpu-bench.sbatch\n\n# Submit a Python array job (tasks 1 through 5)\nsbatch realVol.sbatch\n\n# Submit an R array job (tasks 5, 10, 15)\nsbatch --array=5-15:5 fitspline.sbatch\n</code></pre> <p>Upon successful submission, Slurm will respond with the assigned job ID: <code>Submitted batch job 12345</code></p>"},{"location":"computing-resources/submitting-batch-jobs/#example-batch-scripts","title":"Example Batch Scripts","text":"<p>Here are examples adapted from SCRC tutorials for various software.</p>"},{"location":"computing-resources/submitting-batch-jobs/#simple-python-job-hello-worldsbatch","title":"Simple Python Job (<code>hello-world.sbatch</code>)","text":"<pre><code>#!/bin/bash\n#\n# [hello-world.sbatch]\n# Runs a simple Python script printing \"Hello World!\"\n#\n#SBATCH --job-name=hello            # Job name\n#SBATCH --output=hello_%j.out       # Output file name (%j = job ID)\n#SBATCH --export=ALL               # Export all environment variables\n#SBATCH --time=00:01:00             # Set max runtime of job = 1 minute\n#SBATCH --mem=4G                    # Request 4 gigabytes of memory\n#SBATCH --mail-type=BEGIN,END,FAIL    # Send email notifications\n#SBATCH --mail-user=you@stern.nyu.edu # email TO\n#SBATCH --partition=test            # Specify the partition to submit the job to\n\nmodule purge                         # Start with a clean environment\nmodule load python/3.9.7           # Load python module\npython hello-world.py               # Run the script using the loaded Python module\n</code></pre> <ul> <li>Python Script (<code>hello-world.py</code>): <code>python     print(\"Hello World!\")</code></li> <li>Submit: <code>sbatch hello-world.sbatch</code></li> </ul>"},{"location":"computing-resources/submitting-batch-jobs/#python-with-virtual-environment","title":"Python with Virtual Environment","text":"<pre><code>#!/bin/bash\n#\n# [venv-hello-world.sbatch]\n# Runs a Python script inside a specific virtual environment\n#\n#SBATCH --job-name=venv-hello     # Job name\n#SBATCH --output=venv-hello_%j.out # Output file name\n#SBATCH --export=ALL             # Export all environment variables\n#SBATCH --time=00:01:00          # Set max runtime of job = 1 minute\n#SBATCH --mem=4G                 # Request 4 gigabytes of memory\n#SBATCH --mail-type=BEGIN,END,FAIL # Send email notifications\n#SBATCH --mail-user=you@stern.nyu.edu # email TO\n#SBATCH --partition=test       # Specify the partition to submit the job to\n\nmodule purge                   # Start with a clean environment\nmodule load python/3.9.7       # Load python module (matching venv base)\n\n# Activate the virtual environment (adjust path as needed)\nsource ~/bigdata/05-virtenvs/py3.9/bin/activate\n\npython hello-world.py          # Run python script within the venv\n</code></pre> <ul> <li>Submit: <code>sbatch venv-hello-world.sbatch</code></li> </ul>"},{"location":"computing-resources/submitting-batch-jobs/#sas-job-crosstabsbatch","title":"SAS Job (<code>crosstab.sbatch</code>)","text":"<pre><code>#!/bin/bash\n#\n# [ crosstab.sbatch ]\n# Runs a simple SAS crosstab procedure\n#\n#SBATCH --job-name=crosstab\n#SBATCH --output=crosstab_%j.out # SAS produces .log and .lst files separately\n#SBATCH --export=ALL\n#SBATCH --time=00:10:00\n#SBATCH --mem=512m\n#SBATCH --partition=test\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=you@stern.nyu.edu\n\nmodule purge\nmodule load sas/9.4\nsas -nodms crosstab.sas # Runs the SAS script, creates crosstab.log and crosstab.lst\n</code></pre> <ul> <li>SAS Script (<code>crosstab.sas</code>): <code>sas     /* crosstab.sas */     DATA Hand;     INPUT gender $ handed $ ;     DATALINES;     Female Right     Male Left     Male Right     Female Right     Female Right     Male Right     Male Left     Male Right     Female Right     Female Left     Male Right     Female Right     ;     PROC FREQ DATA=Hand;     TABLES gender*handed;     RUN;</code></li> <li>Submit: <code>sbatch crosstab.sbatch</code></li> </ul>"},{"location":"computing-resources/submitting-batch-jobs/#matlab-gpu-job-gpu-benchsbatch","title":"MATLAB GPU Job (<code>gpu-bench.sbatch</code>)","text":"<pre><code>#!/bin/bash\n#\n# [ gpu-bench.sbatch ]\n# Runs a MATLAB script utilizing GPU resources\n#\n#SBATCH --job-name=gpu-bench\n#SBATCH --output=gpu-bench.%j.out\n#SBATCH --export=ALL\n#SBATCH --gres=gpu:1             # Request 1 GPU\n#SBATCH --gpu-bind=closest       # Bind task to the nearest GPU\n#SBATCH --time=00:09:00\n#SBATCH --mem=32G\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=you@stern.nyu.edu\n#SBATCH --partition=gpu          # Specify the GPU partition\n\necho \"Running on node: `hostname`\"\necho \"\"\nmodule purge\nmodule load matlab/2019a\nmatlab -nojvm &lt; gpu-bench.m # Run the MATLAB script\n</code></pre> <ul> <li>MATLAB Script (<code>gpu-bench.m</code>): (Assumed to exist, performs GPU computation)</li> <li>Submit: <code>sbatch gpu-bench.sbatch</code></li> </ul>"},{"location":"computing-resources/submitting-batch-jobs/#python-array-job-realvolsbatch","title":"Python Array Job (<code>realVol.sbatch</code>)","text":"<pre><code>#!/bin/bash\n#\n# [ realVol.sbatch ]\n# Runs a Python script multiple times with different inputs using job arrays\n#\n#SBATCH --job-name=realVol                 # Job name\n#SBATCH --array=1-5                        # Create 5 tasks with IDs 1, 2, 3, 4, 5\n#SBATCH --export=ALL                       # Export env variables\n#SBATCH --mem=512m                         # Memory per task\n#SBATCH --mail-type=BEGIN,END,FAIL         # Email notifications\n#SBATCH --output=realVol.%A-%a.out       # Output file per task (%A=array ID, %a=task ID)\n#SBATCH --partition=test                   # Specify the partition\n#SBATCH --time=00:10:00                    # Time limit per task\n\nmodule purge\nmodule load python/3.9.7 # Or the appropriate Python version\n\n# Execute python script, using the task ID to select the input file\n# Assumes input files are named series1.txt, series2.txt, ..., series5.txt\nINPUT_FILE=$(ls series${SLURM_ARRAY_TASK_ID}*.txt)\necho \"Task ID: $SLURM_ARRAY_TASK_ID, Input File: $INPUT_FILE\"\npython realVol.py &lt; \"$INPUT_FILE\"\n</code></pre> <ul> <li>Python Script (<code>realVol.py</code>): (Assumed to exist, reads from stdin)</li> <li>Input Files: Requires <code>series1.txt</code>, <code>series2.txt</code>, ..., <code>series5.txt</code> in the submission directory.</li> <li>Submit: <code>sbatch realVol.sbatch</code></li> </ul>"},{"location":"computing-resources/submitting-batch-jobs/#r-array-job-fitsplinesbatch","title":"R Array Job (<code>fitspline.sbatch</code>)","text":"<pre><code>#!/bin/bash\n#\n# [ fitspline.sbatch ]\n# Runs an R script multiple times with different parameters via job arrays\n#\n#SBATCH --job-name=fitsplineJob\n#SBATCH --output=fitsplineJob_%A-%a.Rout # Output file per task\n#SBATCH --error=fitsplineJob_%A-%a.err   # Error file per task\n#SBATCH --export=ALL\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=you@stern.nyu.edu\n#SBATCH --mem=512m\n#SBATCH --time=00:10:00\n#SBATCH --partition=test\n#SBATCH --array=5-15:5 # Tasks with IDs 5, 10, 15\n\nmodule purge\nmodule load R/4.3.2 # Or appropriate R version\n\n# Run R script in batch mode. The R script uses Sys.getenv(\"SLURM_ARRAY_TASK_ID\")\n# to access the task ID. Output goes to fitspline.$SLURM_ARRAY_TASK_ID.Rout\nR CMD BATCH --no-save --no-restore fitspline.R fitspline.$SLURM_ARRAY_TASK_ID.Rout\n</code></pre> <ul> <li>R Script (<code>fitspline.R</code>): (Assumed to exist, uses <code>Sys.getenv(\"SLURM_ARRAY_TASK_ID\")</code>)</li> <li>Submit: <code>sbatch fitspline.sbatch</code> (The <code>--array=5-15:5</code> is defined in the script, but could also be passed on the command line: <code>sbatch --array=5-15:5 fitspline.sbatch</code>)</li> </ul>"},{"location":"computing-resources/submitting-batch-jobs/#r-job-stock-pricesbatch","title":"R Job (<code>stock-price.sbatch</code>)","text":"<pre><code>#!/bin/bash\n#\n# [ stock-price.sbatch ]\n# Runs an R script for Monte Carlo simulation\n#\n#SBATCH --job-name=spJob                # Job name\n#SBATCH --time=00:10:00               # Wall-clock time limit\n#SBATCH --mem=4G                      # Request 4G RAM\n#SBATCH --mail-type=END,FAIL           # email user when job ENDs or FAILs\n#SBATCH --mail-user=you@stern.nyu.edu  # email TO\n#SBATCH --output=stock-price_%j.out    # Standard output file\n#SBATCH --partition=test                # Specify partition\n\nmodule purge\nmodule load R/4.0.2 # Or appropriate R version\n\n# Run the R script, output goes to stock-price.Rout by default\nR CMD BATCH stock-price.R\n</code></pre> <ul> <li>R Script (<code>stock-price.R</code>): (Assumed to exist)</li> <li>Submit: <code>sbatch stock-price.sbatch</code></li> </ul>"},{"location":"computing-resources/submitting-batch-jobs/#monitoring-jobs","title":"Monitoring Jobs","text":"<p>You can check the status of your submitted jobs using the <code>squeue</code> command.</p> <ul> <li>Check your jobs: <code>bash     squeue -u $USER</code></li> <li>Check all jobs on the cluster: <code>bash     squeue</code></li> </ul> <p>For more details on monitoring and managing jobs, see the Common SLURM Commands documentation. To cancel a job, use <code>scancel &lt;job_id&gt;</code>. ```</p>"},{"location":"getting-started/","title":"Getting Started with SCRC","text":"<p>Welcome to the Stern Center for Research Computing (SCRC)! This section provides the essential information you need to begin using our computational resources. Whether you're a Stern affiliate, part of the wider NYU community, or an external collaborator, these guides will walk you through the initial steps.</p> <p>Our goal here is to help you quickly:</p> <ol> <li>Obtain access: Understand the process for getting your SCRC account based on your affiliation.</li> <li>Connect securely: Learn how to log in to our clusters from your computer, including necessary steps like using the NYU VPN when off-campus.</li> <li>Understand the environment: Get acquainted with the first steps after logging in and know when SCRC is the right resource compared to NYU's central HPC facilities.</li> </ol> <p>Follow these guides to get up and running smoothly on the SCRC systems.</p>"},{"location":"getting-started/#topics-in-this-category","title":"Topics in this Category","text":"<p>Below you'll find detailed instructions covering the core aspects of getting started:</p> <ul> <li>Getting an SCRC Account: Instructions for obtaining an SCRC account, with specific guidance for Stern users, non-Stern NYU users, and non-NYU collaborators.</li> <li>Connecting to SCRC Clusters: Detailed steps on how to connect to SCRC login nodes using SSH from different operating systems (Mac, Linux, Windows), including important information about VPN requirements when accessing from off-campus.</li> <li>Getting Started Overview: Outlines the initial steps for new users after obtaining an account and logging in for the first time, such as automatic home directory creation.</li> <li>SCRC vs NYU HPC: Provides guidance to help you decide whether Stern's SCRC resources or the central NYU High Performance Computing (HPC) facilities are better suited for your specific research needs.</li> </ul>"},{"location":"getting-started/connecting-to-clusters/","title":"Connecting to SCRC Clusters","text":"<p>This guide explains how to connect to the NYU Stern Center for Research Computing (SCRC) cluster login nodes using a Secure Shell (SSH) connection from various operating systems (Mac, Linux, Windows). It also covers the requirement for using the NYU VPN when connecting from off-campus.</p>"},{"location":"getting-started/connecting-to-clusters/#overview-command-line-interface-cli","title":"Overview: Command Line Interface (CLI)","text":"<p>The primary method for accessing SCRC servers is through a Command Line Interface (CLI) using a terminal application. The CLI is a text-based interface where you interact with the remote Linux host by typing commands.</p> <p>You will use your Stern NetID and password to authenticate.</p>"},{"location":"getting-started/connecting-to-clusters/#scrc-login-hosts","title":"SCRC Login Hosts","text":"<p>You can connect to either of the following SCRC remote login hosts:</p> <ul> <li><code>rnd.scrc.nyu.edu</code></li> <li><code>vleda.scrc.nyu.edu</code></li> </ul>"},{"location":"getting-started/connecting-to-clusters/#vpn-requirement-for-off-campus-access","title":"VPN Requirement for Off-Campus Access","text":"<p>If you are connecting from a location outside the NYU network (e.g., your home), you must first connect to the NYU VPN. Once the VPN connection is established, you can proceed with the SSH connection steps as if you were on the NYU network.</p> <p>If you are already connected to the NYU network (e.g., on campus), you do not need the VPN and can connect directly.</p>"},{"location":"getting-started/connecting-to-clusters/#connecting-from-maclinux","title":"Connecting from Mac/Linux","text":"<ol> <li>Open your Terminal application (e.g., Terminal on Mac, or any terminal emulator on Linux).</li> <li> <p>Use the <code>ssh</code> command followed by your Stern NetID, the <code>@</code> symbol, and the hostname of one of the login nodes.</p> <p>Replace <code>your_netid</code> with your actual Stern NetID (e.g., <code>xy12</code>).</p> <p><code>bash ssh your_netid@rnd.scrc.nyu.edu</code></p> <p>Alternatively, you can connect to the <code>vleda</code> node:</p> <p><code>bash ssh your_netid@vleda.scrc.nyu.edu</code></p> </li> <li> <p>When prompted, enter your Stern password. Note that the password will not be displayed on the screen as you type.</p> </li> </ol>"},{"location":"getting-started/connecting-to-clusters/#connecting-from-windows","title":"Connecting from Windows","text":"<p>Windows users have several options for connecting via SSH:</p> <ul> <li> <p>Windows Command Prompt (<code>cmd</code>) / PowerShell: Modern versions of Windows 10 and 11 include a built-in SSH client.</p> <ol> <li>Open Command Prompt or PowerShell.</li> <li>Use the <code>ssh</code> command exactly as shown in the Connecting from Mac/Linux section.     <code>bash     ssh your_netid@rnd.scrc.nyu.edu</code></li> <li>Enter your Stern password when prompted.</li> </ol> </li> <li> <p>Windows Subsystem for Linux (WSL2): If you have WSL2 installed (available on Windows 10 and later), you can install a Linux distribution (like Ubuntu) and use its native terminal and <code>ssh</code> client.</p> <ol> <li>Open your WSL terminal (e.g., Ubuntu).</li> <li>Use the <code>ssh</code> command as shown in the Connecting from Mac/Linux section.     <code>bash     ssh your_netid@rnd.scrc.nyu.edu</code></li> <li>Enter your Stern password when prompted.</li> <li>Note: If using WSL2, you might encounter issues accessing the internet when the Cisco AnyConnect VPN (installed via <code>.exe</code>) is active. A possible solution is to uninstall the standard Cisco AnyConnect client and install the AnyConnect app from the Microsoft Store, then configure the VPN connection using NYU IT's provided settings.</li> </ol> </li> <li> <p>PuTTY: PuTTY is a popular free and open-source SSH and telnet client for Windows.</p> </li> </ul>"},{"location":"getting-started/connecting-to-clusters/#using-putty","title":"Using PuTTY","text":"<ol> <li>Download PuTTY: Download the latest installer from the official PuTTY Download Page.</li> <li>Install PuTTY: Run the downloaded installer and follow the on-screen instructions.</li> <li>Launch PuTTY: Open the installed PuTTY application.</li> <li>Configure Session:<ul> <li>In the Host Name (or IP address) field, enter one of the SCRC login hostnames:<ul> <li><code>rnd.scrc.nyu.edu</code></li> <li>or <code>vleda.scrc.nyu.edu</code></li> </ul> </li> <li>Ensure the Port is set to <code>22</code>.</li> <li>Ensure the Connection type is set to <code>SSH</code>.</li> <li>(Optional) You can save these settings for future use by typing a name in the Saved Sessions field (e.g., \"SCRC RND\") and clicking Save.</li> </ul> </li> <li>Connect: Click the Open button.</li> <li>Security Alert (First Connection): The first time you connect to a host, PuTTY will display a security alert dialog box starting with \"The server's host key is not cached...\". This is normal. Click Yes (or Accept) to trust the host and cache its key.</li> <li>Login: A terminal window will open. You will be prompted to log in:<ul> <li>At the <code>login as:</code> prompt, type your Stern NetID and press Enter.</li> <li>At the <code>password:</code> prompt, type your Stern password and press Enter. (The password will not be shown as you type).</li> </ul> </li> </ol> <p>You should now be logged into the SCRC cluster and see a command line prompt (e.g., <code>[your_netid@rnd ~]$</code>). You can now enter Linux commands.</p>"},{"location":"getting-started/getting-an-account/","title":"Getting an SCRC Account","text":"<p>This guide provides instructions for obtaining access to the NYU Stern Center for Research Computing (SCRC) resources. SCRC uses Stern accounts (NetID and password) for authentication. The process differs depending on your affiliation with NYU and the Stern School of Business.</p>"},{"location":"getting-started/getting-an-account/#stern-users","title":"Stern Users","text":"<p>If you are a member of the Stern School of Business (faculty, PhD student, staff, or enrolled in a Stern course), you can use your existing activated Stern account (Stern NetID and password) to access SCRC services.</p> <ul> <li>Requirement: An active Stern NetID and password.</li> <li>Action: If you have not yet activated your Stern account, please do so here: Activate Stern Account</li> <li>Once activated, you can use your Stern credentials to log in to SCRC systems.</li> </ul>"},{"location":"getting-started/getting-an-account/#non-stern-nyu-users","title":"Non-Stern NYU Users","text":"<p>If you are affiliated with NYU but are not part of the Stern School of Business (and not enrolled in a Stern course), you need sponsorship from a full-time Stern employee to obtain a Stern account for SCRC access.</p> <ul> <li>Requirement: An existing NYU NetID and sponsorship from a full-time Stern employee.</li> <li>Process:<ol> <li>Your Stern sponsor must contact the Stern IT Help Desk on your behalf.</li> <li>The sponsor needs to provide the following information to the Help Desk:<ul> <li>Your Full Name</li> <li>Your NYU NetID</li> <li>Your NYU UniversityID (N#)</li> <li>Your Date of birth</li> <li>Sponsor's name (Stern faculty or employee)</li> <li>Sponsoring department</li> <li>Expiration date for access (maximum 1 year; extensions can be requested yearly)</li> </ul> </li> </ol> </li> <li>Contact:<ul> <li>Stern IT Help Desk: <code>helpdesk@stern.nyu.edu</code></li> <li>Phone: 212-998-0180</li> </ul> </li> <li>Action: After the account is created, you must activate your Stern account before you can use SCRC services.</li> </ul>"},{"location":"getting-started/getting-an-account/#non-nyu-collaborators","title":"Non-NYU Collaborators","text":"<p>Users who are not affiliated with NYU (e.g., external collaborators) must first obtain an NYU NetID through sponsorship, and then obtain Stern account sponsorship for SCRC access. This is a two-step process.</p>"},{"location":"getting-started/getting-an-account/#step-1-obtain-an-nyu-netid","title":"Step 1: Obtain an NYU NetID","text":"<ul> <li>Requirement: Sponsorship from an authorized NYU sponsor (typically a Stern faculty member or departmental coordinator).</li> <li>Process:<ol> <li>Your authorized sponsor requests an NYU NetID for you by filling out the IT Onboarding Form.</li> <li>Accessing the Form: The sponsor must:<ul> <li>Log in to the NYU VPN.</li> <li>Log in to identity.it.nyu.edu.</li> <li>Click the hamburger menu (\u2630).</li> <li>Select \"Request Affiliate\".</li> <li>Select \"Request Affiliate User\".</li> </ul> </li> </ol> </li> <li>Support: For assistance with the NYU NetID request process, your sponsor can contact NYU IT support:<ul> <li>Email: <code>AskIT@nyu.edu</code></li> <li>Phone: 212-998-3333</li> </ul> </li> </ul>"},{"location":"getting-started/getting-an-account/#step-2-obtain-stern-account-sponsorship","title":"Step 2: Obtain Stern Account Sponsorship","text":"<ul> <li>Requirement: An active NYU NetID (obtained in Step 1) and sponsorship from a full-time Stern employee (this may be the same sponsor as in Step 1).</li> <li>Process: Once you have your NYU NetID, your Stern sponsor must follow the process outlined above for Non-Stern NYU Users to request Stern account access for you. They will need to contact the Stern IT Help Desk with the required information (Full Name, NYU NetID, N#, DOB, Sponsor Name, Department, Expiration Date).</li> <li>Action: After the Stern account is created, you must activate your Stern account before you can use SCRC services.</li> </ul>"},{"location":"getting-started/getting-started-overview/","title":"Getting Started Overview","text":"<p>This guide outlines the essential first steps for new users after obtaining an SCRC account and logging into the SCRC environment for the first time via SSH. It covers the initial environment setup and basic commands to verify your access.</p>"},{"location":"getting-started/getting-started-overview/#your-first-login","title":"Your First Login","text":"<p>Once your Stern account is activated and you have successfully logged into one of the SCRC login nodes (<code>rnd.scrc.nyu.edu</code> or <code>vleda.scrc.nyu.edu</code>) using your Stern NetID and password via SSH:</p> <ol> <li> <p>Automatic Home Directory Creation: Upon your first login, your personal home directory will be automatically created on the SCRC file system. This directory serves as your primary storage space for files and projects.</p> </li> <li> <p>The Command Prompt: After logging in, you will be presented with a command prompt in the Linux BASH shell. This is the interface where you will enter commands. The prompt typically looks similar to this (where <code>xy12</code> is the NetID and <code>rnd</code> is the login node):</p> <p><code>text [xy12@rnd ~ ]$</code></p> </li> </ol>"},{"location":"getting-started/getting-started-overview/#initial-steps-and-verification","title":"Initial Steps and Verification","text":"<p>After logging in for the first time, you can perform these basic steps to familiarize yourself with the environment:</p> <ol> <li> <p>Verify Your Location (Present Working Directory):     To confirm you are in your home directory, use the <code>pwd</code> (print working directory) command:</p> <p><code>bash pwd</code></p> <p>The output will show the full path to your home directory, for example:</p> <p><code>text /homedir/employees/x/xy12</code> (Note: The exact path structure might vary)</p> </li> <li> <p>List Directory Contents:     Your newly created home directory will initially be empty or contain only default configuration files. You can view its contents using the <code>ls -l</code> command (list directory contents in long format):</p> <p><code>bash ls -l</code></p> <p>If the directory is empty, the command will return immediately without listing any files. If there are hidden configuration files, it might look something like this:</p> <p>```text total 0</p> </li> </ol>"},{"location":"getting-started/getting-started-overview/#or-potentially-some-hidden-files-starting-with","title":"Or potentially some hidden files starting with '.'","text":"<p>```</p>"},{"location":"getting-started/getting-started-overview/#environment-notes","title":"Environment Notes","text":"<ul> <li>Operating System: The SCRC systems run Linux.</li> <li>Shell: The default command-line interface (shell) is BASH (Bourne-Again SHell).</li> <li>Linux Familiarity: Basic familiarity with Linux/Unix commands is expected for using SCRC resources effectively.</li> </ul>"},{"location":"getting-started/getting-started-overview/#next-steps","title":"Next Steps","text":"<p>Now that you have successfully logged in and verified your home directory, you can proceed to:</p> <ul> <li>Learn more essential commands in the Linux Tutorial.</li> <li>Understand how to Transfer Files between your local machine and SCRC.</li> <li>Learn about submitting jobs using the Slurm Workload Manager.</li> </ul>"},{"location":"getting-started/scrc-vs-nyu-hpc/","title":"SCRC vs NYU HPC","text":"<p>This page provides guidance to help you decide whether to use the computing resources provided by the NYU Stern Center for Research Computing (SCRC) or the central NYU High Performance Computing (HPC) facilities.</p>"},{"location":"getting-started/scrc-vs-nyu-hpc/#overview","title":"Overview","text":"<p>Both SCRC and NYU HPC offer powerful computing resources, but they differ in scope, scale, software availability, and support models. Choosing the right resource depends on your specific research needs, computational requirements, and familiarity with HPC environments.</p>"},{"location":"getting-started/scrc-vs-nyu-hpc/#key-considerations","title":"Key Considerations","text":"<p>Here's a breakdown of factors to consider when making your choice:</p> <ul> <li> <p>Scope and Scale:</p> <ul> <li>SCRC: Provides a moderately sized HPC cluster primarily for the Stern community. It's well-suited for many research tasks within Stern.</li> <li>NYU HPC: Serves the entire university community with a significantly larger and more powerful cluster, recognized as one of the top academic HPC facilities. It offers extensive capabilities for very large-scale computations.</li> </ul> </li> <li> <p>Getting Started &amp; Support:</p> <ul> <li>SCRC: Recommended as a starting point, especially for those new to HPC. The SCRC team offers personalized assistance to help Stern users get started and troubleshoot issues.</li> <li>NYU HPC: Offers support to the entire NYU community, but the support model might be less personalized compared to the dedicated Stern SCRC team.</li> </ul> </li> <li> <p>Computational Requirements:</p> <ul> <li>SCRC: Suitable for relatively modest computational requirements.</li> <li>NYU HPC: Better equipped for very large-scale simulations, data analysis, and tasks demanding massive parallelism or specialized hardware.</li> </ul> </li> <li> <p>Response Time / Job Turnaround:</p> <ul> <li>SCRC: Typically offers a faster response time for job scheduling and execution compared to NYU HPC, although this can vary depending on current system load.</li> <li>NYU HPC: Due to the larger user base and potentially larger jobs, turnaround times might be longer, though this fluctuates.</li> </ul> </li> <li> <p>Software Availability:</p> <ul> <li>SCRC: Offers a curated selection of software commonly used within the Stern research community.</li> <li>NYU HPC: Provides a wider range of software options catering to diverse disciplines across the university.</li> </ul> </li> <li> <p>Specialized Services (e.g., Big Data):</p> <ul> <li>SCRC: Does not have dedicated clusters for services like Hadoop.</li> <li>NYU HPC: Recommended if you need specific Big Data services like Hadoop, as they operate dedicated clusters (e.g., Peel cluster) for these purposes.</li> </ul> </li> </ul>"},{"location":"getting-started/scrc-vs-nyu-hpc/#recommendations","title":"Recommendations","text":"<ul> <li> <p>Choose SCRC if:</p> <ul> <li>You are new to High-Performance Computing.</li> <li>Your computational requirements are relatively modest.</li> <li>You are part of the Stern community and prefer personalized support.</li> <li>Faster job turnaround time is a primary concern (though this can vary).</li> <li>The software you need is available on the SCRC cluster.</li> </ul> </li> <li> <p>Choose NYU HPC if:</p> <ul> <li>You require very large-scale computational resources.</li> <li>You need access to a wider range of software packages not available at SCRC.</li> <li>Your research involves specific Big Data tools like Hadoop that utilize specialized clusters (e.g., Peel).</li> <li>You are comfortable working within a larger, university-wide HPC environment.</li> </ul> </li> </ul>"},{"location":"getting-started/scrc-vs-nyu-hpc/#getting-help","title":"Getting Help","text":"<p>The SCRC staff can help you evaluate your computational needs and guide you in selecting the most suitable facility, whether it's SCRC or NYU HPC. Contact the SCRC team at <code>scrc-list@stern.nyu.edu</code> for consultation.</p>"},{"location":"linux-tutorials/","title":"Linux Tutorials","text":"<p>Welcome to the Linux Tutorials section! If you're new to using the command line interface (CLI) on Linux systems, or just need a refresher, you've come to the right place. This collection of tutorials provides a foundation for navigating and interacting with the Linux environment commonly used within the SCRC.</p> <p>Understanding the Linux command line is essential for effectively using our computing resources. These guides will walk you through the basics, starting with navigating the file system and progressing to managing files, understanding permissions, and utilizing powerful command-line tools.</p> <p>Here you'll find step-by-step instructions covering core concepts such as:</p> <ul> <li>Basic Navigation: Moving around the directory structure.</li> <li>File Operations: Creating, viewing, modifying, and deleting files and directories.</li> <li>Permissions: Understanding who can read, write, or execute files.</li> <li>Pipes &amp; Redirection: Controlling the input and output of commands.</li> <li>Getting Help: Finding documentation and using command-line shortcuts.</li> </ul> <p>Browse the topics below to get started:</p> <ul> <li>Linux Introduction and Basic Navigation<ul> <li>Introduction to the Linux command line, shell prompt, basic commands like <code>pwd</code>, <code>ls</code>, <code>cd</code>, and understanding the directory structure.</li> </ul> </li> <li>Linux File Operations<ul> <li>Commands for creating, editing, copying, moving, renaming, and deleting files and directories.</li> </ul> </li> <li>Viewing File Contents<ul> <li>Using commands like <code>more</code>, <code>cat</code>, <code>head</code>, and <code>tail</code> to display the contents of text files.</li> </ul> </li> <li>Linux File Permissions<ul> <li>Understanding and modifying file and directory permissions using <code>ls -l</code> and <code>chmod</code>.</li> </ul> </li> <li>Linux Pipes and Redirection<ul> <li>Using redirection symbols (<code>&gt;</code>, <code>&gt;&gt;</code>) to control command output and pipes (<code>|</code>) to chain commands together.</li> </ul> </li> <li>Linux Command Line Tips and Help<ul> <li>Useful command line features like command history, auto-completion, online help (<code>man</code>), and basic cursor control.</li> </ul> </li> </ul> <p>We recommend working through these tutorials sequentially if you are new to Linux. Feel free to jump to specific topics if you need help with a particular command or concept.</p>"},{"location":"linux-tutorials/linux-command-line-tips/","title":"Linux command line tips","text":"<pre><code>---\ntitle: \"Linux Command Line Tips and Help\"\ncategory: \"linux-tutorials\"\ndescription: \"Useful command line features like command history, auto-completion, online help (man), and basic cursor control.\"\n---\n\n# Linux Command Line Tips and Help\n\nThe Linux command line interface (Shell) offers several helpful features to make your work more efficient. This guide covers some essential tips, including auto-completion, command history navigation, accessing online help manuals, and basic cursor control for editing commands.\n\n## Auto Completion\n\nThe shell can automatically complete commands or file/directory names for you.\n\n1.  Start typing the beginning of a command, filename, or directory name.\n2.  Press the `Tab` key.\n3.  If the typed portion is unique, the shell will complete the rest of the name.\n4.  If there are multiple possibilities that start with the typed portion, pressing `Tab` a second time will list all possible completions. Continue typing a few more characters to make the name unique and press `Tab` again.\n\n## Command History\n\nThe shell keeps a record of the commands you have recently entered. You can easily access and re-run previous commands.\n\n*   **Navigate History:** Press the `Up Arrow` key to cycle backward through previous commands one by one. Press the `Down Arrow` key to cycle forward.\n*   **Search History:**\n    1.  Press `Ctrl+r`.\n    2.  Start typing any part of the command you are looking for. The most recent matching command will appear.\n    3.  Keep typing to refine the search or press `Ctrl+r` again to cycle through older matches.\n    4.  Once you find the desired command, press `Enter` to execute it immediately, or use the arrow keys to edit it first.\n    5.  Press `Ctrl+c` to cancel the history search and return to the prompt.\n\n## Online Manual (Help)\n\nLinux provides built-in help resources for most commands.\n\n### Using `man`\n\nThe `man` command displays the manual page for a specific command, providing detailed information about its usage, options, and behavior.\n\n**Syntax:**\n\n```bash\nman &lt;command_name&gt;\n</code></pre> <p>Example: To view the manual page for the <code>ls</code> command:</p> <pre><code>man ls\n</code></pre> <p>Navigate the <code>man</code> page using arrow keys, Page Up/Down, or Spacebar. Press <code>q</code> to quit.</p>"},{"location":"linux-tutorials/linux-command-line-tips/#searching-manual-pages-with-man-k","title":"Searching Manual Pages with <code>man -k</code>","text":"<p>If you don't know the exact command name but know a related keyword, you can search the manual page index.</p> <p>Syntax:</p> <pre><code>man -k &lt;keyword&gt;\n</code></pre> <p>Example: To find manual pages related to directories:</p> <pre><code>man -k directory\n</code></pre> <p>This command displays a list of manual pages with a one-line synopsis containing the keyword \"directory\".</p>"},{"location":"linux-tutorials/linux-command-line-tips/#using-the-help-option","title":"Using the <code>--help</code> Option","text":"<p>Many commands offer a quick usage summary via the <code>--help</code> option. This is often faster than reading the full <code>man</code> page if you just need a reminder of the basic syntax or available options.</p> <p>Syntax:</p> <pre><code>&lt;command_name&gt; --help\n</code></pre> <p>Example: To get help for the <code>cat</code> command:</p> <pre><code>cat --help\n</code></pre> <p>The command will print usage information directly to the terminal.</p>"},{"location":"linux-tutorials/linux-command-line-tips/#basic-cursor-movement-and-editing","title":"Basic Cursor Movement and Editing","text":"<p>You can easily edit the command line before pressing <code>Enter</code> using these keyboard shortcuts:</p> <ul> <li><code>Ctrl+a</code>: Move the cursor to the beginning of the line.</li> <li><code>Ctrl+e</code>: Move the cursor to the end of the line.</li> <li><code>Ctrl+k</code>: Cut (kill) everything from the cursor position to the end of the line.</li> <li><code>Ctrl+y</code>: Paste (yank) the last text that was cut using <code>Ctrl+k</code>.</li> <li><code>Ctrl+_</code> (Control + Underscore): Undo the last editing action. You might need to press <code>Ctrl+Shift+-</code> on some keyboards to get the underscore. This can be pressed multiple times to undo multiple actions.</li> </ul>"},{"location":"linux-tutorials/linux-command-line-tips/#breaking-out-of-commands","title":"Breaking Out of Commands","text":"<p>If a command is taking too long, you entered the wrong command, or a program seems stuck, you can usually interrupt or cancel it.</p> <ul> <li>Press <code>Ctrl+c</code>: This sends an interrupt signal to the currently running process. For most command-line programs, this will terminate them and return you to the shell prompt. ```</li> </ul>"},{"location":"linux-tutorials/linux-file-operations/","title":"Linux file operations","text":"<pre><code>---\ntitle: \"Linux File Operations\"\ncategory: \"linux-tutorials\"\ndescription: \"Commands for creating, editing, copying, moving, renaming, and deleting files and directories.\"\n---\n\n# Linux File Operations\n\nThis tutorial covers essential Linux commands for managing files and directories via the command line interface (Shell). You will learn how to create, edit, copy, move, rename, and delete files and directories.\n\nMost Linux commands follow a common syntax:\n\n```bash\ncommand -options arguments\n</code></pre> <p>Note: Linux is case-sensitive. Command names are typically lowercase, and filenames like <code>MyFile</code> and <code>myfile</code> are treated as distinct.</p>"},{"location":"linux-tutorials/linux-file-operations/#creating-and-editing-files","title":"Creating and Editing Files","text":""},{"location":"linux-tutorials/linux-file-operations/#using-nano","title":"Using <code>nano</code>","text":"<p><code>nano</code> is a simple, beginner-friendly text editor commonly used in Linux environments to create or edit files.</p> <p>To create a new file or open an existing one for editing:</p> <pre><code>nano filename.txt\n</code></pre> <p>For example, to create or edit a file named <code>costdata.dat</code>:</p> <pre><code>nano costdata.dat\n</code></pre> <p>Inside the <code>nano</code> editor, you'll see the file content area and a list of commands at the bottom. The <code>^</code> symbol represents the <code>Ctrl</code> key.</p> <ul> <li>Save: Press <code>Ctrl+O</code> (Write Out) and confirm the filename by pressing Enter.</li> <li>Exit: Press <code>Ctrl+X</code>. If you have unsaved changes, <code>nano</code> will ask if you want to save them.</li> </ul>"},{"location":"linux-tutorials/linux-file-operations/#copying-files","title":"Copying Files","text":"<p>The <code>cp</code> command is used to copy files or directories.</p> <p>Syntax:</p> <pre><code>cp source_file destination_file\n</code></pre> <p>Example: To make a backup copy of <code>costdata.dat</code> named <code>costdata.dat.bak</code>:</p> <pre><code>cp costdata.dat costdata.dat.bak\n</code></pre> <p>This creates a new file <code>costdata.dat.bak</code> with the same content as <code>costdata.dat</code>. The original file <code>costdata.dat</code> remains unchanged.</p>"},{"location":"linux-tutorials/linux-file-operations/#moving-and-renaming-files","title":"Moving and Renaming Files","text":"<p>The <code>mv</code> command is used to move files or directories from one location to another, or to rename them.</p> <p>Syntax (Renaming):</p> <pre><code>mv old_filename new_filename\n</code></pre> <p>Example: To rename the file <code>costdata.dat.bak</code> to <code>newcostdata.dat</code>:</p> <pre><code>mv costdata.dat.bak newcostdata.dat\n</code></pre> <p>Syntax (Moving):</p> <pre><code>mv source_file destination_directory/\n</code></pre> <p>Example: To move <code>newcostdata.dat</code> into a directory named <code>archives</code>:</p> <pre><code>mv newcostdata.dat archives/\n</code></pre>"},{"location":"linux-tutorials/linux-file-operations/#deleting-files","title":"Deleting Files","text":"<p>The <code>rm</code> command is used to remove (delete) files.</p> <p>Syntax:</p> <pre><code>rm filename\n</code></pre> <p>Example: To delete the file <code>costdata.dat</code>:</p> <pre><code>rm costdata.dat\n</code></pre> <p>Warning: Files deleted with <code>rm</code> are permanently removed. There is no \"Trash\" or \"Recycle Bin\" in the standard Linux command line. Exercise caution when using <code>rm</code>.</p>"},{"location":"linux-tutorials/linux-file-operations/#using-wildcards-with-rm","title":"Using Wildcards with <code>rm</code>","text":"<p>The asterisk (<code>*</code>) is a wildcard character that matches any sequence of characters (including no characters). It can be powerful but dangerous when used with <code>rm</code>.</p> <p>Example: To remove all files in the current directory whose names start with <code>costdata</code>:</p> <pre><code>rm costdata*\n</code></pre> <p>Important: Be VERY careful when using wildcards with <code>rm</code>. You might permanently delete more files than intended. It is highly recommended to first list the files that match the pattern using <code>ls</code> before deleting them:</p> <pre><code>ls costdata*  # Check which files will be matched\nrm costdata*  # Only run this after verifying the ls output\n</code></pre>"},{"location":"linux-tutorials/linux-file-operations/#creating-directories","title":"Creating Directories","text":"<p>The <code>mkdir</code> command is used to create new directories (folders).</p> <p>Syntax:</p> <pre><code>mkdir directory_name\n</code></pre> <p>Example: To create a new directory named <code>study1</code>:</p> <pre><code>mkdir study1\n</code></pre> <p>You can also create directories within other directories, forming a hierarchy.</p> <p>Example: To create a directory <code>costProject</code> in your current location:</p> <pre><code>mkdir costProject\n</code></pre>"},{"location":"linux-tutorials/linux-file-operations/#deleting-directories","title":"Deleting Directories","text":""},{"location":"linux-tutorials/linux-file-operations/#rmdir-for-empty-directories","title":"<code>rmdir</code> (for empty directories)","text":"<p>The <code>rmdir</code> command is used to remove empty directories.</p> <p>Syntax:</p> <pre><code>rmdir directory_name\n</code></pre> <p>Example: To remove the empty directory <code>study1</code>:</p> <pre><code>rmdir study1\n</code></pre> <p>If the directory contains any files or subdirectories, <code>rmdir</code> will fail.</p>"},{"location":"linux-tutorials/linux-file-operations/#rm-r-for-non-empty-directories","title":"<code>rm -r</code> (for non-empty directories)","text":"<p>To remove a directory and all its contents (files and subdirectories), use the <code>rm</code> command with the <code>-r</code> (recursive) option.</p> <p>Syntax:</p> <pre><code>rm -r directory_name\n</code></pre> <p>Example: To remove the directory <code>costProject</code> and everything inside it:</p> <pre><code>rm -r costProject\n</code></pre> <p>Warning: Using <code>rm -r</code> is powerful and permanently deletes the directory and its entire contents. Double-check the directory name before executing this command. ```</p>"},{"location":"linux-tutorials/linux-file-permissions/","title":"Linux file permissions","text":"<pre><code>---\ntitle: \"Linux File Permissions\"\ncategory: \"linux-tutorials\"\ndescription: \"Understanding and modifying file and directory permissions using 'ls -l' and 'chmod'.\"\n---\n\n# Linux File Permissions\n\nThis document explains how to view and modify file and directory permissions in the Linux environment using the `ls -l` and `chmod` commands. Understanding permissions is crucial for controlling access to your files and ensuring proper security and collaboration on shared systems like the SCRC clusters.\n\n## Viewing File Permissions\n\nSince Linux is designed for multiple users to share files, you have options to allow or deny access to your files and directories for others on the system. Permissions determine who can access a file or directory and what actions they can perform (read, write, execute).\n\nTo view the permissions associated with files and directories, use the `ls` command with the `-l` (long format) option:\n\n```bash\nls -l\n</code></pre> <p>This command lists the contents of the current directory in a detailed format. The permissions are shown at the beginning of each line.</p> <p>Example Output:</p> <pre><code>total 180\ndrwxr-xr-x 3 ct27 nobody 4096 Apr 5 2023 mywork\ndrwxr-xr-x 3 ct27 nobody 4096 Apr 3 2023 archived-work\n-rw-r--r-- 1 ct27 nobody    0 Mar 18 2023 prog.sas7bdat\n-rw-r--r-- 1 ct27 nobody 1421 Mar 18 2023 prog.sas7bdat.log\n-rw------- 1 ct27 devel Aug 20 10:15 logon\n-rwx------ 1 ct27 devel Aug 19 15:23 a.out\n-r-xr-xr-x 1 ct27 devel Aug 28 09:48 ls-list\ndrwx------ 1 ct27 resch Aug 27 15:45 sas-one/\ndrw------- 1 ct27 resch Aug 27 15:45 tex/\n</code></pre>"},{"location":"linux-tutorials/linux-file-permissions/#understanding-the-permission-string","title":"Understanding the Permission String","text":"<p>The first 10 characters on each line represent the file type and its permissions (e.g., <code>drwxr-xr-x</code>, <code>-rw-r--r--</code>).</p> <ol> <li> <p>Position 1: File Type</p> <ul> <li><code>d</code>: Indicates a directory.</li> <li><code>-</code>: Indicates a regular file.</li> <li>Other characters exist (e.g., <code>l</code> for symbolic link) but <code>d</code> and <code>-</code> are the most common.</li> </ul> </li> <li> <p>Positions 2-10: Access Permissions     These nine characters are divided into three sets of three, representing permissions for different user classes:</p> <ul> <li>Positions 2, 3, 4: Owner permissions (the user who created the file/directory).</li> <li>Positions 5, 6, 7: Group permissions (the group associated with the file/directory).</li> <li>Positions 8, 9, 10: Other permissions (all other users on the system).</li> </ul> </li> </ol>"},{"location":"linux-tutorials/linux-file-permissions/#permission-characters","title":"Permission Characters","text":"<p>Within each set of three, the characters represent specific permissions:</p> <ul> <li><code>r</code> (Read):<ul> <li>Files: Allows viewing or copying the file's contents.</li> <li>Directories: Allows listing the contents of the directory (e.g., using <code>ls</code>).</li> </ul> </li> <li><code>w</code> (Write):<ul> <li>Files: Allows modifying, renaming, or deleting the file.</li> <li>Directories: Allows creating, deleting, or renaming files within the directory. (Requires <code>x</code> permission as well).</li> </ul> </li> <li><code>x</code> (Execute):<ul> <li>Files: Allows running the file as a program or script.</li> <li>Directories: Allows entering (changing into) the directory (e.g., using <code>cd</code>) and accessing files/subdirectories within it.</li> </ul> </li> <li><code>-</code> (Hyphen): Indicates that the specific permission is not granted.</li> </ul>"},{"location":"linux-tutorials/linux-file-permissions/#permission-examples","title":"Permission Examples","text":"<ul> <li><code>-rw-r--r--</code>: A regular file (<code>-</code>). The owner has read and write (<code>rw-</code>) permissions. The group (<code>r--</code>) and others (<code>r--</code>) have only read permission.</li> <li><code>drwxr-x--x</code>: A directory (<code>d</code>). The owner has read, write, and execute (<code>rwx</code>) permissions. The group has read and execute (<code>r-x</code>) permissions. Others have only execute (<code>--x</code>) permission (meaning they can enter the directory if they know the path, but cannot list its contents).</li> <li><code>-rwx------</code>: A regular file (<code>-</code>). The owner has read, write, and execute (<code>rwx</code>) permissions. The group (<code>---</code>) and others (<code>---</code>) have no permissions.</li> <li><code>drwx------</code>: A directory (<code>d</code>). The owner has read, write, and execute (<code>rwx</code>) permissions. The group (<code>---</code>) and others (<code>---</code>) have no permissions.</li> </ul>"},{"location":"linux-tutorials/linux-file-permissions/#changing-file-and-directory-permissions","title":"Changing File and Directory Permissions","text":"<p>The <code>chmod</code> (change mode) command is used to modify the permission levels of files and directories. You must be the owner of the file/directory or the system administrator to change its permissions.</p> <p><code>chmod</code> can use symbolic notation to specify permission changes.</p>"},{"location":"linux-tutorials/linux-file-permissions/#symbolic-notation","title":"Symbolic Notation","text":"<p>This method uses characters to represent user classes, operators, and permissions.</p> <ul> <li>User Classes:<ul> <li><code>u</code>: User (owner)</li> <li><code>g</code>: Group</li> <li><code>o</code>: Others</li> <li><code>a</code>: All (equivalent to <code>ugo</code>)</li> </ul> </li> <li>Operators:<ul> <li><code>+</code>: Add permission</li> <li><code>-</code>: Remove permission</li> <li><code>=</code>: Set exact permissions (overwriting existing ones)</li> </ul> </li> <li>Permissions:<ul> <li><code>r</code>: Read</li> <li><code>w</code>: Write</li> <li><code>x</code>: Execute</li> </ul> </li> </ul> <p>Syntax:</p> <pre><code>chmod &lt;user_class(es)&gt;&lt;operator&gt;&lt;permission(s)&gt; &lt;filename/directory&gt;\n</code></pre> <p>Examples:</p> <ol> <li> <p>Add read, write, and execute permissions for everyone (all users) to <code>my.dat</code>:     Assume <code>my.dat</code> starts with <code>-rw-r--r--</code>.</p> <p><code>bash chmod a+rwx my.dat</code></p> <p>Resulting permissions: <code>-rwxrwxrwx</code></p> </li> <li> <p>Make <code>my.dat</code> readable and writable only by the owner:     Assume <code>my.dat</code> currently has <code>-rwxrwxrwx</code>. This requires multiple steps or a combined command.</p> <ul> <li>Remove execute permission for the owner (<code>u</code>):     <code>bash     chmod u-x my.dat</code>     Result: <code>-rw-rwxrwx</code></li> <li>Remove all permissions for the group (<code>g</code>):     <code>bash     chmod g-rwx my.dat</code>     Result: <code>-rw----rwx</code></li> <li>Remove all permissions for others (<code>o</code>):     <code>bash     chmod o-rwx my.dat</code>     Result: <code>-rw-------</code></li> </ul> <p>Alternatively, set the exact permissions: <code>bash chmod u=rw,g=,o= my.dat</code> Result: <code>-rw-------</code></p> </li> <li> <p>Add execute permission for the owner: <code>bash     chmod u+x script.sh</code></p> </li> <li> <p>Remove write permission for the group and others: <code>bash     chmod go-w shared_file.txt</code> ```</p> </li> </ol>"},{"location":"linux-tutorials/linux-introduction-navigation/","title":"Linux introduction navigation","text":"<pre><code>---\ntitle: \"Linux Introduction and Basic Navigation\"\ncategory: \"linux-tutorials\"\ndescription: \"Introduction to the Linux command line, shell prompt, basic commands like pwd, ls, cd, and understanding the directory structure.\"\n---\n\n# Linux Introduction and Basic Navigation\n\nThis tutorial introduces the basic Linux commands needed to use and run programs on the SCRC Linux computers, focusing on navigating the file system.\n\n## The Command Line Interface (Shell)\n\nThe Linux machines use a command line interface called a Shell. The Shell is the interface where you enter Linux commands. There are many types of shells; by default, SCRC Linux machines use the BASH shell.\n\nWhen you log in, the first thing you see is the shell's **command prompt**. This indicates the shell is ready to accept your commands. The prompt might look something like this:\n\n```text\n[your_netid@hostname ~ ]$\n</code></pre>"},{"location":"linux-tutorials/linux-introduction-navigation/#your-home-directory","title":"Your Home Directory","text":"<p>When you log in, you start in your home directory. Every user has a unique home directory, which is the primary location for storing your personal files and data.</p>"},{"location":"linux-tutorials/linux-introduction-navigation/#basic-navigation-commands","title":"Basic Navigation Commands","text":"<p>Here are some fundamental commands for navigating the Linux file system:</p>"},{"location":"linux-tutorials/linux-introduction-navigation/#show-the-present-working-directory-pwd","title":"Show the present working directory (<code>pwd</code>)","text":"<p>To see the complete name and path of the directory you are currently in (the \"present working directory\"), use the <code>pwd</code> command:</p> <pre><code>pwd\n</code></pre> <p>Example Output:</p> <pre><code>/homedir/employees/c/ct27\n</code></pre> <p>This output shows the full path to the current directory. In this example, <code>ct27</code> is the user's home directory, located inside <code>c</code>, inside <code>employees</code>, inside <code>homedir</code>.</p>"},{"location":"linux-tutorials/linux-introduction-navigation/#show-contents-of-a-directory-ls","title":"Show contents of a directory (<code>ls</code>)","text":"<p>To see a list of the files and sub-directories within the current directory, use the <code>ls</code> command. Using the <code>-l</code> option provides a detailed \"long format\" listing:</p> <pre><code>ls -l\n</code></pre> <p>Example Output:</p> <pre><code>total 180\ndrwxr-xr-x 3 ct27 nobody 4096 Apr  5 2023 mywork\ndrwxr-xr-x 3 ct27 nobody 4096 Apr  3 2023 archived-work\n-rw-r--r-- 1 ct27 nobody    0 Mar 18 2023 prog.sas7bdat\n-rw-r--r-- 1 ct27 nobody 1421 Mar 18 2023 prog.sas7bdat.log\n</code></pre> <ul> <li><code>ls</code> is the command, and <code>-l</code> is an option.</li> <li>Lines starting with <code>d</code> indicate directories (<code>mywork</code>, <code>archived-work</code>).</li> <li>Lines starting with <code>-</code> indicate regular files (<code>prog.sas7bdat</code>, <code>prog.sas7bdat.log</code>).</li> <li>The long format shows permissions, owner (<code>ct27</code>), size (e.g., <code>1421</code> bytes), modification date, and name.</li> <li>Using <code>ls</code> without options typically shows only the names of files and directories.</li> </ul>"},{"location":"linux-tutorials/linux-introduction-navigation/#create-a-sub-directory-mkdir","title":"Create a sub-directory (<code>mkdir</code>)","text":"<p>To create a new directory (folder) inside your current directory, use the <code>mkdir</code> (make directory) command, followed by the desired directory name:</p> <pre><code>mkdir costProject\n</code></pre> <p>This creates a new sub-directory named <code>costProject</code>. You can verify its creation using <code>ls -l</code>.</p> <p>Note: Linux file and directory names are case-sensitive. <code>costProject</code> is different from <code>costproject</code>.</p>"},{"location":"linux-tutorials/linux-introduction-navigation/#change-directory-cd","title":"Change Directory (<code>cd</code>)","text":"<p>To move into a different directory, use the <code>cd</code> (change directory) command, followed by the name of the directory you want to enter:</p> <pre><code>cd costProject\n</code></pre> <p>After running this command, your present working directory becomes <code>/homedir/employees/c/ct27/costProject</code> (in this example). You can verify this using <code>pwd</code>.</p> <p>To move up one level in the directory structure (to the parent directory), use <code>cd ..</code>:</p> <pre><code>cd ..\n</code></pre>"},{"location":"linux-tutorials/linux-introduction-navigation/#the-linux-file-system-structure","title":"The Linux File System Structure","text":"<p>Linux uses a hierarchical file structure, often visualized as an upside-down tree.</p> <ul> <li>The top-level directory is called the root directory, represented by a forward slash (<code>/</code>).</li> <li>All other files and directories are located within the root directory or its sub-directories.</li> <li>A file can hold text, data, or a program.</li> <li>A directory (or folder) contains files and other directories (sub-directories).</li> <li>A sub-directory is simply a directory located inside another directory.</li> </ul> <p>Understanding this structure is key to navigating using commands like <code>pwd</code>, <code>ls</code>, and <code>cd</code>.</p>"},{"location":"linux-tutorials/linux-introduction-navigation/#useful-tips-and-tools","title":"Useful Tips and Tools","text":""},{"location":"linux-tutorials/linux-introduction-navigation/#auto-completion-in-the-shell","title":"Auto Completion in the Shell","text":"<p>The BASH shell offers tab completion. Start typing a command or filename, and press the <code>Tab</code> key.</p> <ul> <li>If the typed portion is unique, the shell will complete the rest of the name.</li> <li>If there are multiple possibilities, pressing <code>Tab</code> a second time will often list them.</li> </ul>"},{"location":"linux-tutorials/linux-introduction-navigation/#create-or-edit-a-file-nano","title":"Create or Edit a File (<code>nano</code>)","text":"<p><code>nano</code> is a simple, beginner-friendly text editor available on the command line. To create a new file or open an existing one for editing:</p> <pre><code>nano my_program.py\n</code></pre> <pre><code>nano data_notes.txt\n</code></pre> <p>Inside <code>nano</code>, commands are listed at the bottom. The <code>^</code> symbol represents the <code>Ctrl</code> key.</p> <ul> <li><code>Ctrl+O</code>: Write Out (Save the file)</li> <li><code>Ctrl+X</code>: Exit <code>nano</code></li> </ul>"},{"location":"linux-tutorials/linux-introduction-navigation/#list-the-contents-of-a-file-more","title":"List the Contents of a File (<code>more</code>)","text":"<p>To view the contents of a text file one screen at a time, use the <code>more</code> command:</p> <pre><code>more my_data.txt\n</code></pre> <ul> <li>Press the <code>Spacebar</code> to advance to the next page.</li> <li>Press <code>q</code> to quit viewing the file and return to the command prompt.</li> </ul>"},{"location":"linux-tutorials/linux-introduction-navigation/#breaking-out-ctrlc","title":"Breaking Out (<code>Ctrl+C</code>)","text":"<p>If a command seems stuck, unresponsive, or you want to cancel the current operation, press <code>Ctrl+C</code>. This usually interrupts the running process and returns you to the command prompt. ```</p>"},{"location":"linux-tutorials/linux-pipes-redirection/","title":"Linux Pipes and Redirection","text":"<pre><code>---\ntitle: \"Linux Pipes and Redirection\"\ncategory: \"linux-tutorials\"\ndescription: \"Using redirection symbols (&gt;, &gt;&gt;) to control command output and pipes (|) to chain commands together.\"\n---\n\n# Linux Pipes and Redirection\n\nThis tutorial explains how to use redirection symbols (`&gt;` and `&gt;&gt;`) to control where the output of Linux commands goes, and how to use pipes (`|`) to connect the output of one command to the input of another.\n\n## Redirecting Input and Output\n\nBy default, Linux commands display their results (standard output) to the screen and take input from the keyboard (standard input). Redirection allows you to change these defaults, often sending output to a file instead of the screen.\n\n### Overwriting Output with `&gt;`\n\nA right angle-bracket `&gt;` (called an \"into\") on the command line indicates that the output of the command should be placed into the file specified after the symbol, instead of being displayed on the screen.\n\n**Syntax:**\n\n```bash\ncommand &gt; filename\n</code></pre> <p>Example:</p> <pre><code>ls &gt; list\n</code></pre> <p>This command will execute <code>ls</code> (list directory contents) and place its output into a file named <code>list</code> in the current directory.</p> <p>Warning: If a file named <code>list</code> already exists, its previous contents will be overwritten without warning.</p>"},{"location":"linux-tutorials/linux-pipes-redirection/#appending-output-with","title":"Appending Output with <code>&gt;&gt;</code>","text":"<p>You can append output to the end of an existing file using a double right angle-bracket <code>&gt;&gt;</code> (called an \"onto\"). If the file does not exist, it will be created.</p> <p>Syntax:</p> <pre><code>command &gt;&gt; filename\n</code></pre> <p>Example:</p> <p>Following the previous example, if the next command entered was:</p> <pre><code>date &gt;&gt; list\n</code></pre> <p>The output of the <code>date</code> command (the current date and time) would be added to the end of the file called <code>list</code>, preserving the original content from the <code>ls</code> command.</p>"},{"location":"linux-tutorials/linux-pipes-redirection/#pipes","title":"Pipes (<code>|</code>)","text":"<p>Pipes allow you to connect multiple commands together. The standard output of the command to the left of the pipe symbol is \"piped\" into the standard input of the command to the right of the pipe. The symbol for this connection is a vertical bar <code>|</code> called a pipe.</p> <p>Syntax:</p> <pre><code>command1 | command2\n</code></pre> <p>This takes the output of <code>command1</code> and uses it as the input for <code>command2</code>.</p> <p>Example 1: Paging through long output</p> <p>If a directory contains many files, the output of <code>ls</code> might scroll off the screen too quickly to read. You can pipe the output of <code>ls</code> to the <code>more</code> command, which displays output one screen-full at a time.</p> <pre><code>ls | more\n</code></pre> <p>In this case, <code>ls</code> generates the directory listing, which is then passed as input to <code>more</code>. <code>more</code> displays the first page and waits for user input (space bar for next page, <code>q</code> to quit).</p> <p>Example 2: Searching within a file</p> <p>You can use pipes to filter the contents of a file. For instance, use <code>cat</code> to display a file's content and pipe it to <code>grep</code> to search for specific text within that content.</p> <pre><code>cat filename | grep searchstring\n</code></pre> <p>This command displays the content of <code>filename</code> using <code>cat</code>, and then <code>grep</code> filters that output, showing only the lines containing <code>searchstring</code>. ```</p>"},{"location":"linux-tutorials/linux-viewing-files/","title":"Linux viewing files","text":"<pre><code>---\ntitle: \"Viewing File Contents\"\ncategory: \"linux-tutorials\"\ndescription: \"Using commands like 'more', 'cat', 'head', and 'tail' to display the contents of text files.\"\n---\n\n# Viewing File Contents\n\nThis document explains how to view the contents of text files in the Linux command line environment using common utilities like `cat`, `more`, `head`, and `tail`. These commands allow you to display entire files or specific portions of them directly in your terminal.\n\n## Displaying Entire File Content\n\nThere are several ways to display the full content of a file.\n\n### `cat`: Concatenate and Display Files\n\nThe `cat` command reads files sequentially, writing them to standard output. It's commonly used to display the contents of short files or to concatenate multiple files.\n\n**Syntax:**\n\n```bash\ncat [options] [filename...]\n</code></pre> <p>Example: Display a single file</p> <pre><code>cat my_file.txt\n</code></pre> <p>This command will print the entire content of <code>my_file.txt</code> to the terminal.</p> <p>Example: Using <code>cat</code> with pipes</p> <p>The <code>cat</code> command can be piped (<code>|</code>) to other commands for further processing. For instance, searching within a file:</p> <pre><code>cat costdata.dat | grep searchstring\n</code></pre> <p>This command displays the contents of <code>costdata.dat</code> and pipes the output to <code>grep</code>, which then prints only the lines containing \"searchstring\".</p> <p>Getting Help:</p> <p>To see options for the <code>cat</code> command, use the <code>--help</code> flag:</p> <pre><code>cat --help\n</code></pre>"},{"location":"linux-tutorials/linux-viewing-files/#more-display-file-content-page-by-page","title":"<code>more</code>: Display File Content Page by Page","text":"<p>The <code>more</code> command displays the contents of a file one screen-full at a time. This is useful for viewing larger files that don't fit entirely on the screen.</p> <p>Syntax:</p> <pre><code>more [filename]\n</code></pre> <p>Example:</p> <pre><code>more costdata.dat\n</code></pre> <p>Usage:</p> <ul> <li>When the file content exceeds the screen size, <code>more</code> pauses after displaying the first page.</li> <li>Press the Spacebar to advance to the next page.</li> <li>Press the <code>q</code> key to quit <code>more</code> and return to the command prompt.</li> </ul> <p>Example: Using <code>more</code> with pipes</p> <p><code>more</code> is often used with pipes to paginate the output of other commands:</p> <pre><code>ls -l | more\n</code></pre> <p>This command lists the directory contents in long format and pipes the output to <code>more</code>, allowing you to view the list page by page if it's long.</p>"},{"location":"linux-tutorials/linux-viewing-files/#displaying-parts-of-a-file","title":"Displaying Parts of a File","text":"<p>Sometimes you only need to see the beginning or end of a file.</p>"},{"location":"linux-tutorials/linux-viewing-files/#head-display-the-beginning-of-a-file","title":"<code>head</code>: Display the Beginning of a File","text":"<p>The <code>head</code> command displays the first few lines of a file (10 lines by default).</p> <p>Syntax:</p> <pre><code>head [options] [filename...]\n</code></pre> <p>Example:</p> <pre><code>head my_large_file.log\n</code></pre> <p>This displays the first 10 lines of <code>my_large_file.log</code>.</p> <p>Example: Specify number of lines</p> <p>Use the <code>-n</code> option to specify a different number of lines:</p> <pre><code>head -n 20 my_large_file.log\n</code></pre> <p>This displays the first 20 lines.</p>"},{"location":"linux-tutorials/linux-viewing-files/#tail-display-the-end-of-a-file","title":"<code>tail</code>: Display the End of a File","text":"<p>The <code>tail</code> command displays the last few lines of a file (10 lines by default). This is particularly useful for checking log files or the end of large datasets.</p> <p>Syntax:</p> <pre><code>tail [options] [filename...]\n</code></pre> <p>Example:</p> <pre><code>tail my_large_file.log\n</code></pre> <p>This displays the last 10 lines of <code>my_large_file.log</code>.</p> <p>Example: Specify number of lines</p> <p>Use the <code>-n</code> option to specify a different number of lines:</p> <pre><code>tail -n 50 my_large_file.log\n</code></pre> <p>This displays the last 50 lines.</p> <p>Example: Follow file changes</p> <p>The <code>-f</code> option allows you to \"follow\" a file, displaying new lines as they are added (useful for monitoring logs in real-time).</p> <pre><code>tail -f application.log\n</code></pre> <p>Press <code>Ctrl+C</code> to stop following the file.</p>"},{"location":"linux-tutorials/linux-viewing-files/#breaking-out-of-commands","title":"Breaking Out of Commands","text":"<p>If a command like <code>more</code> or a long <code>cat</code> seems stuck, or if you want to stop a command like <code>tail -f</code>, you can usually interrupt it by pressing <code>Ctrl+C</code>. This will cancel the current command and return you to the command prompt. ```</p>"},{"location":"research-datasets/","title":"Research Datasets","text":"<p>Welcome to the Research Datasets section of the SCRC documentation. This area is dedicated to helping you discover, access, and utilize the extensive collection of research datasets available to support your work. Whether you're conducting financial analysis, exploring market trends, or investigating economic indicators, this guide provides the essential information you need.</p>"},{"location":"research-datasets/#overview","title":"Overview","text":"<p>Understanding and accessing research data is crucial for impactful analysis. This section covers two primary areas:</p> <ol> <li>Available Datasets: We provide an overview of the major datasets accessible through SCRC, encompassing a wide range of financial, marketing, and economic information.</li> <li>Access Methods (WRDS): Many premier datasets are available via Wharton Research Data Services (WRDS). We detail the process for obtaining and managing a WRDS account, which is your gateway to these powerful resources.</li> </ol> <p>Our goal is to streamline your access to the data you need, enabling you to focus on your research.</p>"},{"location":"research-datasets/#topics-in-this-category","title":"Topics in this Category","text":"<p>Below are the specific guides available within this section. Each page provides detailed instructions or information related to accessing and understanding our research data resources.</p>"},{"location":"research-datasets/#available-datasets-overview","title":"Available Datasets Overview","text":"<p>A summary of major research datasets available through SCRC and WRDS, including financial, marketing, and economic data.</p>"},{"location":"research-datasets/#accessing-wrds-wharton-research-data-services","title":"Accessing WRDS (Wharton Research Data Services)","text":"<p>How to request and manage a WRDS account to access various financial and business datasets.</p>"},{"location":"research-datasets/available-datasets-overview/","title":"Available Datasets Overview","text":"<p>This document provides a summary of major research datasets available to the NYU Stern community through the Stern Center for Research Computing (SCRC) and Wharton Research Data Services (WRDS). These datasets cover a wide range of financial, marketing, and economic data.</p>"},{"location":"research-datasets/available-datasets-overview/#accessing-wrds","title":"Accessing WRDS","text":"<p>Wharton Research Data Services (WRDS) is an internet-based research business data service from the Wharton School, providing access to historical financial information. Many of the datasets listed below are accessible via WRDS.</p>"},{"location":"research-datasets/available-datasets-overview/#access-options","title":"Access Options","text":"<ul> <li>Without an Account: The WRDS website can be accessed without an account at WRDS Connect on public workstations at Bobst Library.</li> <li>With an Account: Users with a WRDS account can log in at Wharton Research Database Services from on or off campus. Accounts provide personal/shared workspace, query history, and secure server access. Faculty may request Class accounts for student use.</li> </ul>"},{"location":"research-datasets/available-datasets-overview/#requesting-an-account","title":"Requesting an Account","text":"<p>NYU faculty, administrators, staff, and students may apply for a WRDS account. Non-PhD students can request a temporary Research Assistant account when working with faculty.</p> <ol> <li>Visit Wharton Research Database Services.</li> <li>Click the Register button.</li> <li>Complete the form, read the Terms of Use, and submit.</li> <li>The NYU Representative for WRDS will review your application and affiliation.</li> <li>Upon approval, WRDS will email instructions for setting your password.</li> </ol>"},{"location":"research-datasets/available-datasets-overview/#support","title":"Support","text":"<ul> <li>For general questions about WRDS access at Stern: research@stern.nyu.edu.</li> <li>For specific WRDS database, query, or programming issues: Use the Contact button on the WRDS website.</li> </ul>"},{"location":"research-datasets/available-datasets-overview/#datasets","title":"Datasets","text":""},{"location":"research-datasets/available-datasets-overview/#bank-regulatory-formerly-fdic","title":"Bank Regulatory (formerly FDIC)","text":"<ul> <li>Description: Contains financial data and history for entities filing the Report Of Condition and Income (Call Report) and some savings institutions filing the OTS Thrift Financial Report (TFR). Includes structure data, financial time series, derived integers, ratios, and merger history for commercial banks, savings banks, and savings &amp; loans.</li> <li>Access:<ul> <li>WRDS: Web query site with data manuals and user guides.</li> <li>WRDS Cloud (<code>wrds.wharton.upenn.edu</code>): SAS datasets and sample SAS programs.</li> </ul> </li> <li>Links:<ul> <li>FDIC Government Website</li> <li>WRDS Introduction</li> <li>WRDS Website</li> </ul> </li> </ul>"},{"location":"research-datasets/available-datasets-overview/#bloomberg","title":"Bloomberg","text":"<ul> <li>Description: Online financial information service providing real-time, historical, and descriptive financial data, news, and analytics, including pricing and risk assessment models.</li> <li>Access:<ul> <li>Stern Faculty/PhD Students: Exclusive access to a Bloomberg station. Contact research@stern.nyu.edu or call 212-998-0163.</li> <li>All NYU Faculty/Students: Terminals available at Bobst Library (3 on 5th floor, 2 on Lower Level).</li> </ul> </li> <li>Links:<ul> <li>Bloomberg commercial website</li> </ul> </li> </ul>"},{"location":"research-datasets/available-datasets-overview/#boardex","title":"BoardEx","text":"<ul> <li>Description: Offers relationship capital management data, including director profiles, networks, compensation, board summaries, organization analysis, and announcements.</li> <li>Access: Available to Stern researchers as Excel files via FTP. Contact research@stern.nyu.edu for information.</li> <li>Links:<ul> <li>BoardEx Data (commercial website)</li> </ul> </li> </ul>"},{"location":"research-datasets/available-datasets-overview/#capital-iq-datafeeds","title":"Capital IQ Datafeeds","text":"<ul> <li>Description: Comprises three categories:<ul> <li>Capital Structure: Debt and equity capital structure information for global companies.</li> <li>Key Developments: Structured summaries of material news and events affecting security market values.</li> <li>People Intelligence: Data on over 4.5 million professionals, including executives, board members, and investment professionals.</li> </ul> </li> <li>Access:<ul> <li>WRDS: Web query pages with documentation.</li> <li>WRDS Cloud (<code>wrds.wharton.upenn.edu</code>): SAS datasets.</li> </ul> </li> <li>Links:<ul> <li>Capital IQ Datafeeds (commercial website)</li> <li>WRDS Introduction</li> <li>WRDS Website</li> </ul> </li> </ul>"},{"location":"research-datasets/available-datasets-overview/#compustat","title":"Compustat","text":"<ul> <li>Description: Financial, statistical, and market data for publicly traded US and Canadian companies. Includes annual/quarterly income statements, balance sheets, cash flows, aggregates, industry segments, bank data, market prices, dividends, earnings, executive compensation (Execucomp), and Global Vantage.</li> <li>Access:<ul> <li>WRDS: Web query pages.</li> <li>WRDS Cloud (<code>wrds.wharton.upenn.edu</code>): Sequential datasets and SAS datasets.</li> <li>CRSP/Compustat Merged Database: Combines Compustat financial data with CRSP stock data, accessible via WRDS under CRSP.</li> </ul> </li> <li>Links:<ul> <li>WRDS Introduction</li> <li>WRDS Website</li> </ul> </li> </ul>"},{"location":"research-datasets/available-datasets-overview/#crsp","title":"CRSP","text":"<ul> <li>Description: Comprehensive security price data for NYSE, AMEX, and Nasdaq stocks, spanning from December 1925. Includes unique identifiers for tracking securities history. Stern's subscription covers Stocks, Indices, Treasuries, and Mutual Funds.</li> <li>Access:<ul> <li>WRDS: Web query pages.</li> <li>WRDS Cloud (<code>wrds.wharton.upenn.edu</code>): Sequential datasets and SAS datasets.</li> <li>CRSP/Compustat Merged Database: Accessible via WRDS under CRSP.</li> <li>CRSP Cloud: Data files can be delivered; contact research@stern.nyu.edu for arrangements.</li> </ul> </li> <li>Links:<ul> <li>CRSP Website at University of Chicago</li> <li>WRDS Introduction</li> <li>WRDS Website</li> </ul> </li> </ul>"},{"location":"research-datasets/available-datasets-overview/#datastream","title":"Datastream","text":"<ul> <li>Description: Online historical financial database with data on financial instruments, equity and fixed-income securities, and indicators for over 175 countries and 60 markets.</li> <li>Access:<ul> <li>WRDS: Datastream Equities, Economics, Commodities, and Futures modules are available.</li> <li>Refinitiv Workspace: Access can be requested via the Bobst Library Datastream Guide. (Note: Link missing in source)</li> </ul> </li> <li>Links:<ul> <li>Datastream Overview (commercial website)</li> </ul> </li> </ul>"},{"location":"research-datasets/available-datasets-overview/#dealscan","title":"Dealscan","text":"<ul> <li>Description: Global commercial loan market data, featuring detailed terms and conditions on loan transactions. Also known as WRDS-Reuters DealScan.</li> <li>Access:<ul> <li>WRDS: Accessible under Thomson Reuters, includes Company, Facility, and Package data.</li> <li>WRDS Cloud (<code>wrds.wharton.upenn.edu</code>): SAS datasets located in <code>/wrds/tfn/sasdata/dealscan</code>.</li> </ul> </li> <li>Links:<ul> <li>WRDS Introduction</li> <li>WRDS Website</li> </ul> </li> </ul>"},{"location":"research-datasets/available-datasets-overview/#dow-jones-averages","title":"Dow Jones Averages","text":"<ul> <li>Description: Comprises The Dow Jones Industrial Average (DJIA), The Dow Jones Transportation Average (DJTA), and The Dow Jones Utility Average (DJUA).</li> <li>Access:<ul> <li>WRDS: Web query site with variable documentation.</li> <li>WRDS Cloud (<code>wrds.wharton.upenn.edu</code>): Sequential data, SAS dataset, and sample SAS program. Data provided by Dow Jones Indexes.</li> </ul> </li> <li>Links:<ul> <li>Dow Jones Commercial Website</li> <li>WRDS Website</li> </ul> </li> </ul>"},{"location":"research-datasets/available-datasets-overview/#federal-reserve-bank-reports","title":"Federal Reserve Bank Reports","text":"<ul> <li>Description: Includes Foreign Exchange Rates (H.10 Report) and Interest Rates (H.15 Report) from the Federal Reserve Board, and FRB-Philadelphia State Indexes from the Federal Reserve Bank of Philadelphia.</li> <li>Access:<ul> <li>WRDS: Web query site.</li> <li>WRDS Cloud (<code>wrds.wharton.upenn.edu</code>): Data can be manipulated using SAS, FORTRAN, C.</li> </ul> </li> <li>Links:<ul> <li>Federal Reserve Board Statistics &amp; Historical Data</li> <li>Federal Reserve Bank of Philadelphia</li> <li>WRDS Introduction</li> <li>WRDS Website</li> </ul> </li> </ul>"},{"location":"research-datasets/available-datasets-overview/#ibes","title":"I/B/E/S","text":"<ul> <li>Description: Institutional Brokers Estimate System. Provides global summary and individual forecast information, buy-sell-hold recommendations, cash flow, dividend, and pre-tax profit forecasts.</li> <li>Access:<ul> <li>WRDS: Web query site with documentation and data manuals.</li> <li>WRDS Cloud (<code>wrds.wharton.upenn.edu</code>): SAS datasets and sample SAS programs.</li> </ul> </li> <li>Links:<ul> <li>I/B/E/S Estimates (commercial website)</li> <li>WRDS Introduction</li> <li>WRDS Website</li> </ul> </li> </ul>"},{"location":"research-datasets/available-datasets-overview/#ibes-guidance","title":"IBES Guidance","text":"<ul> <li>Description: Datafeed with numeric company expectations across 14 measures, derived from press releases, corporate event transcripts, and IBES earnings forecasts. Includes current and historical US data.</li> <li>Access: Available to NYU Stern academic researchers via FTP. Contact research@stern.nyu.edu for information.</li> </ul>"},{"location":"research-datasets/available-datasets-overview/#iri","title":"IRI","text":"<ul> <li>Description: Information Resources, Inc. point-of-sale marketing data from grocery store purchases, tracking UPC-coded brand items across all categories.</li> <li>Access:<ul> <li>WRDS: Web query pages.</li> <li>WRDS Cloud (<code>wrds.wharton.upenn.edu</code>): SAS datasets.</li> </ul> </li> <li>Links:<ul> <li>IRI home page (commercial website)</li> <li>WRDS Introduction</li> <li>WRDS Website</li> </ul> </li> </ul>"},{"location":"research-datasets/available-datasets-overview/#iss-institutional-shareholder-services","title":"ISS: Institutional Shareholder Services","text":"<ul> <li>Description: Includes Governance data, Directors data, Shareholder Proposals, and Voting Analytics data.</li> <li>Access:<ul> <li>WRDS: Web query pages.</li> <li>WRDS Cloud (<code>wrds.wharton.upenn.edu</code>): SAS datasets.</li> </ul> </li> <li>Links:<ul> <li>ISS Website</li> <li>WRDS Introduction</li> <li>WRDS Website</li> </ul> </li> </ul>"},{"location":"research-datasets/available-datasets-overview/#mergent-fisd","title":"Mergent-FISD","text":"<ul> <li>Description: Mergent Fixed Income Securities Database. A U.S. Bond database covering over 140,000 securities with 550+ data items, including issuer-specific, issue-specific, and transaction information.</li> <li>Access:<ul> <li>WRDS: Web query pages.</li> <li>WRDS Cloud (<code>wrds.wharton.upenn.edu</code>): SAS datasets.</li> </ul> </li> <li>Links:<ul> <li>WRDS Introduction</li> <li>WRDS Website</li> </ul> </li> </ul>"},{"location":"research-datasets/available-datasets-overview/#nastraq","title":"NASTRAQ","text":"<ul> <li>Description: North American Securities Tracking and Quantifying System. Displays trades and quotes for NASDAQ-listed securities (service ended 2006). Provides trade data, inside quote data, and dealer quote data.</li> <li>Access:<ul> <li>WRDS: Legacy data via web query pages.</li> <li>WRDS Cloud (<code>wrds.wharton.upenn.edu</code>): SAS datasets.</li> </ul> </li> <li>Links:<ul> <li>WRDS Introduction</li> <li>WRDS Website</li> </ul> </li> </ul>"},{"location":"research-datasets/available-datasets-overview/#nyse-taq","title":"NYSE TAQ","text":"<ul> <li>Description: NYSE Trades and Quotes database. Contains intraday transaction data from NYSE, AMEX, Nasdaq, and Smallcap. Includes TAQ through 2014 and Millisecond TAQ from 2012 onwards.</li> <li>Access:<ul> <li>WRDS: Web query pages (utilizes specialized hardware for large datasets).</li> <li>WRDS Cloud (<code>wrds.wharton.upenn.edu</code>): SAS datasets.</li> </ul> </li> <li>Links:<ul> <li>WRDS Introduction</li> <li>WRDS Website</li> </ul> </li> </ul>"},{"location":"research-datasets/available-datasets-overview/#sdc","title":"SDC","text":"<ul> <li>Description: SDC Platinum is a collection of financial transaction databases from Thomson Reuters/Refinitiv.</li> <li>Access:<ul> <li>SDC Platinum Windows Interface: Available for Stern installation until Dec 2023.</li> <li>Refinitiv Workspace: SDC data is now available via this platform.</li> <li>Contact research@stern.nyu.edu for more information and assistance.</li> </ul> </li> <li>Links:<ul> <li>SDC Platinum Overview</li> </ul> </li> <li>Datasets included in SDC:<ul> <li>Mergers &amp; Acquisitions: US Targets, Non-US Targets, Joint Ventures / Alliances, Repurchases</li> <li>Global New Issues Databases: All Equity (Common Stock, Convertible Equity, Pipeline/Registrations, Private Equity), All Bonds (Non-Convertible, Preferred Stock, Mortgage/Asset Backed, Pipeline/Registrations, MTN Programs, Private Debt)</li> <li>Corporate Governance: Poison Pills, Proxy Fights</li> <li>Corporate Restructurings: Full Detail Bankruptcies (1988-present), Limited Detail Bankruptcies (1980-1990), Exchange Offers</li> <li>VentureXpert: Industry Resources (Funds, Firms, Portfolio Companies, Limited Partners, Executives), Industry Statistics (Fund Commitments, Performance), Industrial Statistics (Company Investments/Disbursements, IPOs), Mergers &amp; Acquisitions</li> <li>Global Public Finance: U.S. New Issues, Non-U.S. New Issues, MuniProfiles, Refunding Candidates, Project Finance</li> </ul> </li> </ul>"},{"location":"research-datasets/available-datasets-overview/#sec-order-execution","title":"SEC Order Execution","text":"<ul> <li>Description: SEC-mandated disclosure of order execution statistics from market centers trading national market system securities (Rule 11Ac1-5). Data available through 2005 only via WRDS in the source text.</li> <li>Access:<ul> <li>WRDS: Web query page with documentation.</li> <li>WRDS Cloud (<code>wrds.wharton.upenn.edu</code>): SAS datasets in <code>/wrds/doe/sasdata</code>.</li> </ul> </li> <li>Links:<ul> <li>SEC Rule 11Ac1-5</li> <li>WRDS Introduction</li> <li>WRDS Website</li> </ul> </li> </ul>"},{"location":"research-datasets/available-datasets-overview/#thompsonrefinitiv","title":"Thompson/Refinitiv","text":"<ul> <li>Description: Represents various datasets from Thomson Reuters (now Refinitiv, part of LSEG) available via WRDS.</li> <li>Access:<ul> <li>WRDS: Web query pages with documentation.</li> <li>WRDS Cloud (<code>wrds.wharton.upenn.edu</code>): SAS datasets and sequential data.</li> </ul> </li> <li>Databases on WRDS under Thomson/Refinitiv:<ul> <li>Datastream</li> <li>Insiders Data</li> <li>13f Institutional Holdings</li> <li>Lipper Hedge Fund Database (TASS)</li> <li>Mutual Funds Holdings</li> <li>Refinitiv ESG</li> <li>Worldscope</li> <li>WRDS-Reuters DealScan (See Dealscan)</li> <li>(Note: I/B/E/S datasets are also from Refinitiv but often listed separately on WRDS. See I/B/E/S.)</li> <li>(Note: SDC and IBES Guidance are other Thomson Reuters/Refinitiv products, accessed differently. See SDC and IBES Guidance.)</li> </ul> </li> <li>Links:<ul> <li>WRDS Introduction</li> <li>WRDS Website</li> </ul> </li> </ul>"},{"location":"research-datasets/available-datasets-overview/#worldscope-fundamentals","title":"WorldScope Fundamentals","text":"<ul> <li>Description: Detailed financial information for public companies in over 50 developed and emerging markets. Includes up to 20 years of historical data for ~31,000 active and ~9,000 inactive companies, using over 1,500 data elements per company. Often considered the international equivalent of Compustat.</li> <li>Access:<ul> <li>WRDS: Available as part of the Thomson/Refinitiv offerings.</li> <li>Refinitiv Workspace: Incorporated with Datastream.</li> </ul> </li> <li>Links:<ul> <li>Worldscope Fundamentals Overview (commercial website)</li> </ul> </li> </ul>"},{"location":"research-datasets/wrds-access/","title":"Accessing WRDS (Wharton Research Data Services)","text":"<p>Wharton Research Data Services (WRDS) is an internet-based business data research service from The Wharton School of the University of Pennsylvania. It provides access to a wide array of historical financial, accounting, banking, economics, management, marketing, and public policy databases through a uniform web interface. This page outlines how NYU Stern users can access WRDS resources.</p>"},{"location":"research-datasets/wrds-access/#overview-of-wrds","title":"Overview of WRDS","text":"<p>WRDS hosts data from various providers, offering a centralized platform for researchers. Datasets available through WRDS include, but are not limited to:</p> <ul> <li>Bank Regulatory (formerly FDIC)</li> <li>Capital IQ Datafeeds (Capital Structure, Key Developments, People Intelligence)</li> <li>Compustat (North America, Global)</li> <li>CRSP (Stock Market and Security Data, Treasuries, Mutual Funds)</li> <li>Datastream (Equities, Economics, Commodities, Futures)</li> <li>DealScan (Loan Market Data)</li> <li>Dow Jones Averages</li> <li>Federal Reserve Bank Reports (Foreign Exchange, Interest Rates)</li> <li>I/B/E/S (Earnings Estimates)</li> <li>IRI (Marketing Data)</li> <li>ISS (Governance Data)</li> <li>Mergent-FISD (Fixed Income Securities)</li> <li>NASTRAQ (Legacy NASDAQ Trades and Quotes)</li> <li>NYSE TAQ (Trades and Quotes)</li> <li>SEC Order Execution Data</li> <li>Thompson/Refinitiv datasets (Insiders Data, 13f Holdings, Lipper Hedge Fund, Mutual Funds Holdings, Refinitiv ESG, Worldscope)</li> <li>WorldScope Fundamentals</li> </ul> <p>WRDS access is primarily intended for academic research and instructional purposes.</p>"},{"location":"research-datasets/wrds-access/#how-to-access-wrds","title":"How to Access WRDS","text":"<p>There are two main ways to access WRDS:</p>"},{"location":"research-datasets/wrds-access/#access-without-an-account","title":"Access Without an Account","text":"<p>You can access the WRDS website without a personal account using WRDS Connect public workstations available at Bobst Library. This provides access to the web query interface but lacks features like personal workspace or server access.</p> <p>Some datasets might also be accessible via Stern wireless networks without an account, but this is often limited compared to account access.</p>"},{"location":"research-datasets/wrds-access/#access-with-an-account","title":"Access With an Account","text":"<p>Users with an approved WRDS account can log in directly via the Wharton Research Database Services website.</p> <p>Account holders (Faculty, PhD, Research Assistant) benefit from:</p> <ul> <li>Personal and shared workspace on the WRDS server.</li> <li>Saved query history.</li> <li>Secure SSH access to the WRDS Linux server (<code>wrds.wharton.upenn.edu</code>) for accessing data directly (e.g., SAS datasets, sequential data) and running programs (SAS, FORTRAN, C supported).</li> </ul>"},{"location":"research-datasets/wrds-access/#requesting-a-wrds-account","title":"Requesting a WRDS Account","text":"<p>NYU faculty, administrators, staff, and students are eligible to apply for a WRDS account.</p> <ul> <li>Faculty, PhD Students, Staff: Can apply for standard accounts.</li> <li>Non-PhD Students: May request a Research Assistant (RA) account for a limited time if working with a faculty member on research.</li> <li>Class Accounts: Faculty can request accounts for their classes, allowing students off-campus access for coursework.</li> </ul> <p>Steps to Request an Account:</p> <ol> <li>Navigate to the Wharton Research Database Services website.</li> <li>Click the Register button located at the top of the login page.</li> <li>Carefully complete the registration form.</li> <li>Read and agree to the WRDS Terms of Use Statement.</li> <li>Submit the completed form.</li> <li>Your application will be sent to the designated NYU WRDS Representative for review.</li> <li>The NYU Representative will verify your affiliation with NYU.</li> <li>Once approved, WRDS will send you an email with instructions on how to set your account password.</li> </ol>"},{"location":"research-datasets/wrds-access/#getting-help","title":"Getting Help","text":"<ul> <li>NYU-Specific Questions: For general questions about WRDS access at NYU, suggestions, or comments, please contact the SCRC team at research@stern.nyu.edu.</li> <li>WRDS Platform/Data Issues: For specific problems, questions, or suggestions regarding WRDS database access, data querying, or programming on the WRDS server:<ul> <li>Log in to the WRDS website.</li> <li>Click the Contact or Support button (usually found at the top or bottom of the page).</li> <li>Complete and submit the support request form directly to the WRDS team.</li> </ul> </li> </ul>"},{"location":"software-and-applications/","title":"Software And Applications","text":"<p>Welcome to the Software and Applications section of the SCRC documentation! This is your central hub for understanding how to access, manage, and effectively utilize the diverse range of software available on our computing resources.</p> <p>Whether you need to load a specific version of a statistical package, set up an isolated environment for your Python project, install custom R packages, or simply see what tools are available, you'll find the necessary guidance here.</p>"},{"location":"software-and-applications/#key-concepts","title":"Key Concepts","text":"<ul> <li>Available Software: We provide access to a wide variety of research software, from compilers and programming languages (Python, R, Java) to specialized applications (MATLAB, Stata, SAS) and data analysis tools.</li> <li>Modules System (<code>module</code> command): This is the primary way software environments are managed on the SCRC clusters. It allows multiple versions of software to coexist without conflicts and lets you easily load the specific version you need for your work.</li> <li>Virtual Environments &amp; Local Libraries: For languages like Python and R, managing project-specific dependencies is crucial. Learn how to create isolated environments (like Python virtual environments) and install packages locally without interfering with system-wide installations.</li> <li>Running Applications: Find instructions for running specific applications, including those with graphical user interfaces (GUIs) like RStudio or XStata, often facilitated through services like FastX or accessed via other platforms like Apps@Stern.</li> </ul>"},{"location":"software-and-applications/#topics-in-this-category","title":"Topics in this Category","text":"<p>Explore the following pages to learn more about specific software and management techniques:</p>"},{"location":"software-and-applications/#available-software-overview","title":"Available Software Overview","text":"<p>A list of commonly used research software available at SCRC, including statistical packages, programming languages, and data analysis tools.</p>"},{"location":"software-and-applications/#using-the-software-modules-system","title":"Using the Software Modules System","text":"<p>How to use the <code>module</code> command to find, load, and unload available software packages on the SCRC clusters. Essential for managing your software environment.</p>"},{"location":"software-and-applications/#python-virtual-environments","title":"Python Virtual Environments","text":"<p>Creating, activating, and managing isolated Python environments to install and use specific package versions, ensuring project reproducibility.</p>"},{"location":"software-and-applications/#installing-local-r-packages","title":"Installing Local R Packages","text":"<p>Instructions for installing R packages into your personal user library from CRAN, Bioconductor, and GitHub, giving you access to the latest R tools.</p>"},{"location":"software-and-applications/#running-gui-applications-rstudio-xstata-jupyter","title":"Running GUI Applications (RStudio, XStata, Jupyter)","text":"<p>Guides for running graphical applications like RStudio, XStata, and Anaconda/Jupyter notebooks within interactive sessions on SLURM, typically using FastX.</p>"},{"location":"software-and-applications/#using-appsstern","title":"Using Apps@Stern","text":"<p>Information on accessing certain academic software titles virtually through the NYU Stern Apps@Stern service.</p>"},{"location":"software-and-applications/apps-at-stern/","title":"Apps at stern","text":"<pre><code>---\ntitle: \"Using Apps@Stern\"\ncategory: \"software-and-applications\"\ndescription: \"Information on accessing academic software virtually through the Apps@Stern service.\"\n---\n\n# Using Apps@Stern\n\nThis page provides information on how NYU Stern students and faculty can access academic software applications virtually using the Apps@Stern service.\n\n## What is Apps@Stern?\n\nApps@Stern provides NYU Stern's students and faculty with virtual access to academically relevant software applications through a web browser or dedicated client. This allows users to run specialized software without needing to install it directly on their personal computers.\n\n**Important Note:** Use of Apps@Stern is intended for academic purposes only.\n\n## Accessing Apps@Stern\n\nYou can access the Apps@Stern service by navigating to their website:\n\n*   [Apps@Stern Website](link-to-apps-at-stern)\n\nYou will typically log in using your NYU Stern NetID and password.\n\n## Available Software\n\nApps@Stern provides access to various software packages relevant to academic work at Stern. Some of the available applications include:\n\n*   **Anaconda:** A distribution of Python and R, including many data science packages.\n*   **Eviews:** Statistical analysis software (Available to Stern Faculty and PhD Students).\n*   **Minitab:** Statistical analysis software.\n*   **Oracle Crystal Ball:** Predictive modeling application (runs with Microsoft Excel).\n*   **R:** Language and environment for statistical computing and graphics.\n*   **Stata:** Integrated statistical package for data analysis, management, and graphics.\n\n*Note: Software availability may change. Please check the Apps@Stern portal for the most current list.*\n\n## Comparison with NYU Virtual Computer Lab (VCL)\n\nWhile Apps@Stern provides applications specifically curated for the Stern community, NYU also offers the [NYU Virtual Computer Lab (VCL)](link-to-nyu-vcl). The VCL provides access to a broader range of software applications available to the wider NYU community.\n\n## Getting Help\n\nIf you encounter issues or have questions regarding Apps@Stern, please contact the Stern IT Helpdesk:\n\n*   **Email:** &lt;helpdesk@stern.nyu.edu&gt;\n*   **Phone:** (212) 998-0180\n*   **Location:** Tisch Hall, Room L-100\n\n## Additional Information\n\nFor more detailed information, refer to the official service documentation:\n\n*   [Apps@Stern Service Documentation](link-to-apps-at-stern-documentation)\n</code></pre>"},{"location":"software-and-applications/available-software-overview/","title":"Available software overview","text":"<pre><code>---\ntitle: \"Available Software Overview\"\ncategory: \"software-and-applications\"\ndescription: \"A list of commonly used research software available at SCRC, including statistical packages, programming languages, and data analysis tools.\"\n---\n\n# Available Software Overview\n\nThis page provides an overview of the commonly used research software available to the NYU Stern community through the Stern Center for Research Computing (SCRC) and associated NYU resources. This includes statistical packages, programming languages, data analysis tools, and utilities accessible on SCRC Linux servers, Apps@Stern, the NYU Virtual Computer Lab (VCL), or for local installation.\n\n## Accessing Software on SCRC Linux Servers\n\nMany software packages are available on the SCRC Linux servers (`rnd.scrc.nyu.edu` or `vleda.scrc.nyu.edu`) via environment modules.\n\nTo see available modules, use the command:\n\n```bash\nmodule avail\n</code></pre> <p>To load a specific module (e.g., Python 3.9.7), use:</p> <pre><code>module load python/3.9.7\n</code></pre> <p>To run graphical applications like RStudio or XStata, you typically need to: 1.  Connect via FastX (See Getting Started with Slurm Interactive Jobs). 2.  Start an interactive job on a compute node using <code>srun</code>. 3.  Load the appropriate module. 4.  Launch the application.</p> <p>Example <code>srun</code> command for an interactive session:</p> <pre><code>srun --pty --mem=8gb --time=1:00:00 --cpus-per-task=1 --nodes=1 /bin/bash\n</code></pre>"},{"location":"software-and-applications/available-software-overview/#software-list","title":"Software List","text":""},{"location":"software-and-applications/available-software-overview/#anaconda","title":"Anaconda","text":"<ul> <li>Description: Anaconda is a distribution of the Python and R programming languages tailored for scientific computing. It simplifies package management and deployment, including numerous data-science packages.</li> <li>Availability:<ul> <li>SCRC Linux Servers (Load via <code>module load anaconda3/...</code>)</li> <li>Apps@Stern</li> <li>Windows/Mac/Linux (Install Anaconda Community Edition locally)</li> </ul> </li> <li> <p>Usage on SCRC: Can be used interactively (e.g., with Jupyter Notebooks within an <code>srun</code> session) or in batch jobs.     ```bash     # Example: Load Anaconda module     module load anaconda3/py3.9</p> </li> </ul>"},{"location":"software-and-applications/available-software-overview/#example-start-jupyter-notebook-in-an-interactive-session","title":"Example: Start Jupyter Notebook in an interactive session","text":"<p>jupyter-notebook ``` *   Links: *   Anaconda Website *   Run Anaconda/Jupyter Environment on SLURM</p>"},{"location":"software-and-applications/available-software-overview/#eviews","title":"Eviews","text":"<ul> <li>Description: EViews provides tools for general statistical analysis, time series estimation and forecasting, large-scale model simulation, graphics, and data management.</li> <li>Availability: Apps@Stern (for Stern Faculty and PhD Students)</li> <li>Links:<ul> <li>EViews Commercial Website</li> </ul> </li> </ul>"},{"location":"software-and-applications/available-software-overview/#filezilla","title":"FileZilla","text":"<ul> <li>Description: FileZilla is a free, open-source graphical FTP, FTPS, and SFTP client for transferring files between your local machine and remote servers.</li> <li>Availability: Windows, Mac, Linux (Download locally)</li> <li>Usage for SCRC: Used to transfer files to/from SCRC Linux servers (<code>rnd.scrc.nyu.edu</code> or <code>vleda.scrc.nyu.edu</code>).<ol> <li>Ensure you are connected to the NYU VPN if off-campus.</li> <li>Start FileZilla.</li> <li>Enter connection details:<ul> <li>Host: <code>rnd.scrc.nyu.edu</code> (or <code>vleda.scrc.nyu.edu</code>)</li> <li>Username: <code>&lt;Your Stern NetID&gt;</code></li> <li>Password: <code>&lt;Your Stern Password&gt;</code></li> <li>Port: <code>22</code></li> </ul> </li> <li>Click \"Quickconnect\".</li> <li>Drag and drop files between the local (left) and remote (right) panels.</li> </ol> </li> <li>Links:<ul> <li>FileZilla Website</li> </ul> </li> </ul>"},{"location":"software-and-applications/available-software-overview/#java","title":"Java","text":"<ul> <li>Description: Java is a high-level, class-based, object-oriented programming language designed for platform independence.</li> <li>Availability:<ul> <li>SCRC Linux Servers</li> <li>Windows/Mac/Linux (Install locally)</li> </ul> </li> <li>Links:<ul> <li>Java Website</li> <li>Java Tutorial</li> </ul> </li> </ul>"},{"location":"software-and-applications/available-software-overview/#mathematica","title":"Mathematica","text":"<ul> <li>Description: Mathematica is an integrated technical computing system for symbolic and numerical calculation, visualization, and programming.</li> <li>Availability:<ul> <li>NYU Virtual Computer Lab (VCL) (Students)</li> <li>NYU Data Services workstations</li> <li>Local Installation:<ul> <li>Faculty can purchase via ITS. NYU has a site license.</li> <li>Stern PhD students: Contact the Stern Doctoral Office.</li> </ul> </li> </ul> </li> <li>Links:<ul> <li>Wolfram Commercial Website</li> <li>Accessing Wolfram Software at NYU</li> </ul> </li> </ul>"},{"location":"software-and-applications/available-software-overview/#matlab","title":"MATLAB","text":"<ul> <li>Description: MATLAB (MATrix LABoratory) is a high-level language and interactive environment for numerical computation, visualization, and programming. Widely used for math, computation, algorithm development, modeling, simulation, and data analysis. Supports GPU computation.</li> <li>Availability:<ul> <li>SCRC Linux Servers (Load via <code>module load matlab/...</code>)</li> <li>Windows (via SCRC Virtual Machines or local install)</li> <li>NYU Virtual Computer Lab (VCL)</li> <li>Local Installation: Available to everyone at NYU through the MATLAB Portal.</li> </ul> </li> <li> <p>Usage on SCRC: Can run interactively (requires FastX/<code>srun</code>) or in batch jobs. Supports GPU nodes (<code>--partition=gpu</code>).     ```bash     # Example: Load MATLAB module     module load matlab/2019a</p> </li> </ul>"},{"location":"software-and-applications/available-software-overview/#example-run-matlab-script-in-batch-job-from-gpu-benchsbatch","title":"Example: Run MATLAB script in batch job (from gpu-bench.sbatch)","text":"<p>matlab -nojvm &lt; gpu-bench.m</p>"},{"location":"software-and-applications/available-software-overview/#example-run-matlab-gui-interactively-requires-fastxsrun","title":"Example: Run MATLAB GUI interactively (requires FastX/srun)","text":"<p>matlab ``` *   Links: *   MathWorks Commercial Website *   MathWorks Documentation *   NYU Research Software *   MATLAB GPU Tutorial</p>"},{"location":"software-and-applications/available-software-overview/#minitab","title":"Minitab","text":"<ul> <li>Description: Minitab is a statistical software package for data analysis, hypothesis testing, regression, ANOVA, and more.</li> <li>Availability:<ul> <li>Apps@Stern</li> <li>NYU Virtual Computer Lab (VCL)</li> <li>NYU Research Software (Installation options may vary)</li> <li>Windows, MacOS, Linux</li> </ul> </li> </ul>"},{"location":"software-and-applications/available-software-overview/#oracle-crystal-ball","title":"Oracle Crystal Ball","text":"<ul> <li>Description: Oracle Crystal Ball is a predictive modeling application that integrates with Microsoft Excel, primarily used for teaching.</li> <li>Availability: Apps@Stern</li> </ul>"},{"location":"software-and-applications/available-software-overview/#putty","title":"PuTTY","text":"<ul> <li>Description: PuTTY is a free, lightweight SSH and Telnet client for Windows, used to connect to remote Linux/Unix servers via a command-line interface.</li> <li>Availability: Windows (Download locally)</li> <li>Usage for SCRC:<ol> <li>Download and install PuTTY.</li> <li>Start PuTTY.</li> <li>Enter Host Name: <code>rnd.stern.nyu.edu</code> or <code>vleda.stern.nyu.edu</code>.</li> <li>Ensure Port is <code>22</code> and Connection type is <code>SSH</code>.</li> <li>Click Open.</li> <li>Accept the server's host key if prompted (first time only).</li> <li>Log in with your Stern NetID and password.</li> </ol> </li> <li>Links:<ul> <li>PuTTY Download Page</li> </ul> </li> </ul>"},{"location":"software-and-applications/available-software-overview/#python","title":"Python","text":"<ul> <li>Description: Python is a versatile, high-level programming language widely used in data science, scripting, web development, and more. SCRC supports using Python within virtual environments for managing project dependencies.</li> <li>Availability:<ul> <li>SCRC Linux Servers (Multiple versions available via <code>module load python/...</code>)</li> <li>Windows/Mac/Linux (Install locally, often via Anaconda)</li> <li>Apps@Stern (Typically via Anaconda)</li> </ul> </li> <li> <p>Usage on SCRC:</p> <ul> <li>Use <code>module load python/&lt;version&gt;</code> to access system Python.</li> <li>Recommended practice: Create project-specific virtual environments in your <code>/bigdata</code> directory using <code>virtualenv</code> or <code>conda</code>.</li> <li>Run scripts directly (<code>python your_script.py</code>) or via Slurm batch jobs. ```bash</li> </ul> </li> </ul>"},{"location":"software-and-applications/available-software-overview/#example-load-python-module","title":"Example: Load Python module","text":"<p>module load python/3.9.7</p>"},{"location":"software-and-applications/available-software-overview/#example-create-a-virtual-environment-run-once","title":"Example: Create a virtual environment (run once)","text":""},{"location":"software-and-applications/available-software-overview/#module-load-python397-load-desired-python-version-first","title":"module load python/3.9.7 # Load desired python version first","text":""},{"location":"software-and-applications/available-software-overview/#virtualenv-bigdatamy_python_env","title":"virtualenv ~/bigdata/my_python_env","text":""},{"location":"software-and-applications/available-software-overview/#example-activate-virtual-environment","title":"Example: Activate virtual environment","text":""},{"location":"software-and-applications/available-software-overview/#source-bigdatamy_python_envbinactivate","title":"source ~/bigdata/my_python_env/bin/activate","text":""},{"location":"software-and-applications/available-software-overview/#example-install-packages-within-active-environment","title":"Example: Install packages within active environment","text":""},{"location":"software-and-applications/available-software-overview/#pip-install-numpy-pandas","title":"pip install numpy pandas","text":""},{"location":"software-and-applications/available-software-overview/#example-deactivate-virtual-environment","title":"Example: Deactivate virtual environment","text":""},{"location":"software-and-applications/available-software-overview/#deactivate","title":"deactivate","text":"<p>``` *   Links: *   Python Website *   Python Virtual Environment Tutorial *   Python Simple Job Tutorial *   Python Array Job Tutorial</p>"},{"location":"software-and-applications/available-software-overview/#qualtrics","title":"Qualtrics","text":"<ul> <li>Description: Qualtrics is a web-based platform for creating, distributing, and analyzing surveys.</li> <li>Availability:<ul> <li>Stern faculty and students (via Stern Qualtrics account)</li> <li>NYU Community (via NYU Survey Service in NYUHome)</li> </ul> </li> <li>Links:<ul> <li>NYU Survey Service (Qualtrics)</li> <li>Qualtrics Commercial Website</li> </ul> </li> </ul>"},{"location":"software-and-applications/available-software-overview/#r","title":"R","text":"<ul> <li>Description: R is a free software environment and language for statistical computing and graphics. It offers a wide variety of statistical and graphical techniques. RStudio provides a popular integrated development environment (IDE) for R.</li> <li>Availability:<ul> <li>SCRC Linux Servers (Multiple versions via <code>module load R/...</code>; RStudio via <code>module load rstudio</code>)</li> <li>Apps@Stern</li> <li>NYU Virtual Computer Lab (VCL)</li> <li>Windows/Mac/Linux (Install locally from CRAN)</li> </ul> </li> <li> <p>Usage on SCRC:</p> <ul> <li>Run R scripts in batch jobs using <code>R CMD BATCH</code>.</li> <li>Run R interactively in the terminal or via RStudio (requires FastX/<code>srun</code>).</li> <li>Install personal packages locally within your home or <code>/bigdata</code> directory. ```bash</li> </ul> </li> </ul>"},{"location":"software-and-applications/available-software-overview/#example-load-r-module","title":"Example: Load R module","text":"<p>module load R/4.3.2</p>"},{"location":"software-and-applications/available-software-overview/#example-run-r-script-in-batch-from-fitsplinesbatch","title":"Example: Run R script in batch (from fitspline.sbatch)","text":""},{"location":"software-and-applications/available-software-overview/#r-cmd-batch-no-save-no-restore-fitspliner-fitsplineslurm_array_task_idrout","title":"R CMD BATCH --no-save --no-restore fitspline.R fitspline.$SLURM_ARRAY_TASK_ID.Rout","text":""},{"location":"software-and-applications/available-software-overview/#example-load-rstudio-module-requires-interactive-session","title":"Example: Load RStudio module (requires interactive session)","text":"<p>module load rstudio</p>"},{"location":"software-and-applications/available-software-overview/#example-start-rstudio-requires-interactive-session","title":"Example: Start RStudio (requires interactive session)","text":"<p>rstudio ``` *   Links: *   The R Project for Statistical Computing *   CRAN (Comprehensive R Archive Network) *   Run RStudio on SLURM *   Install Local R Packages *   R Fitspline Tutorial *   R Monte Carlo Simulation Tutorial</p>"},{"location":"software-and-applications/available-software-overview/#sas-and-enterprise-miner","title":"SAS and Enterprise Miner","text":"<ul> <li>Description: SAS (Statistical Analysis System) is a comprehensive software suite for data access, management, analysis, and presentation. SAS Enterprise Miner provides tools specifically for data mining.</li> <li>Availability:<ul> <li>SCRC Linux Servers (Load via <code>module load sas/...</code>)</li> <li>Windows (via SCRC Virtual Machines or licensed install)</li> <li>NYU Virtual Computer Lab (VCL)</li> <li>Local Installation (Windows): Purchase licenses via nyu.onthehub.com (Faculty, Staff, Grad Students); Stern PhD students contact Doctoral Office.</li> </ul> </li> <li> <p>Usage on SCRC: Run SAS programs in batch jobs.     ```bash     # Example: Load SAS module     module load sas/9.4</p> </li> </ul>"},{"location":"software-and-applications/available-software-overview/#example-run-sas-program-in-batch-from-crosstabsbatch","title":"Example: Run SAS program in batch (from crosstab.sbatch)","text":"<p>sas -nodms crosstab.sas ``` *   Links: *   SAS Commercial Website *   SAS Tutorial *   UCLA SAS Resources</p>"},{"location":"software-and-applications/available-software-overview/#spss","title":"SPSS","text":"<ul> <li>Description: SPSS (Statistical Package for the Social Sciences) is a widely used program for statistical analysis in social science.</li> <li>Availability:<ul> <li>Windows/Mac (Purchase discounted licenses via NYU Computer Store)</li> <li>NYU Data Services Workstations</li> </ul> </li> <li>Links:<ul> <li>SPSS Commercial Website</li> <li>NYU Computer Store</li> </ul> </li> </ul>"},{"location":"software-and-applications/available-software-overview/#stattransfer","title":"Stat/Transfer","text":"<ul> <li>Description: Stat/Transfer is a utility designed to simplify the conversion of data between different statistical package formats (e.g., Stata, SAS, SPSS, R, Excel). Supports over 30 formats.</li> <li>Availability:<ul> <li>Windows (via SCRC installation request or purchase)</li> <li>Stern Faculty/PhD Students: Request local PC installation by emailing research@stern.nyu.edu.</li> <li>Purchase directly from Stat/Transfer.</li> </ul> </li> <li>Links:<ul> <li>Stat/Transfer Website</li> </ul> </li> </ul>"},{"location":"software-and-applications/available-software-overview/#stata","title":"Stata","text":"<ul> <li>Description: Stata is an integrated statistical software package providing tools for data analysis, data management, and graphics. XStata refers to the graphical interface version.</li> <li>Availability:<ul> <li>SCRC Linux Servers (Load via <code>module load stata/...</code> or <code>module load xstata-...</code>; XStata requires interactive session)</li> <li>Apps@Stern</li> <li>NYU Virtual Computer Lab (VCL)</li> <li>Windows/Mac/Linux (Purchase discounted licenses via Stata Prof+ Plan)</li> </ul> </li> <li> <p>Usage on SCRC:</p> <ul> <li>Run Stata <code>.do</code> files in batch jobs.</li> <li>Run Stata interactively in the terminal.</li> <li>Run XStata graphical interface (requires FastX/<code>srun</code>). ```bash</li> </ul> </li> </ul>"},{"location":"software-and-applications/available-software-overview/#example-load-stata-module-command-line","title":"Example: Load Stata module (Command Line)","text":"<p>module load stata-se/17.0</p>"},{"location":"software-and-applications/available-software-overview/#example-load-xstata-module-gui-requires-interactive-session","title":"Example: Load XStata module (GUI - requires interactive session)","text":"<p>module load xstata-se/17.0</p>"},{"location":"software-and-applications/available-software-overview/#example-start-xstata-gui-requires-interactive-session","title":"Example: Start XStata GUI (requires interactive session)","text":"<p>xstata <code>*   **Links:** *   [Stata Commercial Website](https://www.stata.com/) *   [Resources for Learning Stata](https://www.stata.com/links/resources-for-learning-stata/) *   [Run XStata on SLURM](link-to-run-xstata-slurm)</code></p>"},{"location":"software-and-applications/installing-r-packages/","title":"Installing Local R Packages","text":"<pre><code>---\ntitle: \"Installing Local R Packages\"\ncategory: \"software-and-applications\"\ndescription: \"Instructions for installing R packages into your user library from CRAN, Bioconductor, and GitHub.\"\n---\n\n# Installing Local R Packages\n\nThis document provides instructions for installing R packages into your personal library on the SCRC environment. R packages extend the capabilities of R by adding new functions or improving existing ones. Packages can be installed from various repositories.\n\n**Prerequisite:** Before installing packages, ensure you are running an interactive session on a compute node with R loaded (e.g., via RStudio). For instructions, see [Getting Started with Slurm Interactive Jobs](link-to-getting-started).\n\n## Package Repositories\n\nRepositories are central locations where R packages are stored for download and installation. The most common repositories include:\n\n*   **CRAN (The Comprehensive R Archive Network):** The official repository maintained by the R community. Contains a vast collection of stable R packages.\n*   **Bioconductor:** A specialized repository focusing on open-source software for bioinformatics and computational biology.\n*   **GitHub:** A popular platform for hosting open-source projects, including many R packages under active development.\n\n## Installing Packages\n\nPackage installation is typically done from within an R session (e.g., in the RStudio Console).\n\n### Installing from CRAN\n\nPackages available on CRAN can be installed directly using the `install.packages()` function.\n\n*   **Install a single package:**\n    Replace `\"package_name\"` with the name of the package you want to install.\n\n    ```R\n    install.packages(\"package_name\")\n    ```\n\n    For example, to install the `vioplot` package:\n\n    ```R\n    install.packages(\"vioplot\")\n    ```\n\n*   **Install multiple packages:**\n    Provide a character vector `c()` containing the names of the packages.\n\n    ```R\n    install.packages(c(\"vioplot\", \"MASS\"))\n    ```\n\n*   **Specify a CRAN mirror:**\n    You can choose a specific mirror for downloading using the `repo` argument. A list of mirrors is available [here](https://cran.r-project.org/mirrors.html).\n\n    ```R\n    install.packages(\"vioplot\", repo = \"https://lib.ugent.be/CRAN/\")\n    ```\n\n### Installing Bioconductor Packages\n\nBioconductor packages require the `BiocManager` package. Detailed instructions are available on the [Bioconductor installation page](https://www.bioconductor.org/install/).\n\n1.  **Install BiocManager:**\n    If you don't have `BiocManager` installed, install it from CRAN first:\n\n    ```R\n    install.packages(\"BiocManager\")\n    ```\n\n2.  **Find available packages:**\n    You can browse available Bioconductor packages [online](https://www.bioconductor.org/packages/release/BiocViews.html#___Software) or check programmatically within R:\n\n    ```R\n    BiocManager::available()\n    ```\n\n3.  **Install Bioconductor packages:**\n    Use the `BiocManager::install()` function. You can install single or multiple packages.\n\n    ```R\n    BiocManager::install(c(\"GenomicFeatures\", \"AnnotationDbi\"))\n    ```\n\n### Installing GitHub (and other) Packages via `devtools`\n\nThe `devtools` package provides convenient functions to install packages directly from repositories like GitHub, Bitbucket, or even local files.\n\n1.  **Install devtools:**\n    Install `devtools` from CRAN if you haven't already:\n\n    ```R\n    install.packages(\"devtools\")\n    ```\n\n2.  **Use `devtools::install_*` functions:**\n    `devtools` offers various functions for installation (see `devtools` [documentation](https://cran.r-project.org/web/packages/devtools/devtools.pdf) for details):\n    *   `install_bioc()`: from Bioconductor\n    *   `install_bitbucket()`: from Bitbucket\n    *   `install_cran()`: from CRAN\n    *   `install_git()`: from a git repository\n    *   `install_github()`: from GitHub\n    *   `install_local()`: from a local file\n    *   `install_svn()`: from a SVN repository\n    *   `install_url()`: from a URL\n    *   `install_version()`: a specific version from CRAN\n\n3.  **Example: Install from GitHub:**\n    To install the `babynames` package from the `hadley` repository on GitHub:\n\n    ```R\n    devtools::install_github(\"hadley/babynames\")\n    ```\n\n## Removing Packages\n\nTo remove an installed package, use the `remove.packages()` function:\n\n```R\nremove.packages(\"package_name\")\n</code></pre> <p>For example, to remove the <code>vioplot</code> package:</p> <pre><code>remove.packages(\"vioplot\")\n</code></pre>"},{"location":"software-and-applications/installing-r-packages/#updating-packages","title":"Updating Packages","text":"<p>You can check for outdated packages and update them.</p> <ul> <li> <p>Check for outdated packages:     The <code>old.packages()</code> function lists installed packages for which newer versions are available on the repositories.</p> <p><code>R old.packages()</code></p> </li> <li> <p>Update a specific package:     Simply reinstall the package using <code>install.packages()</code>. This will fetch the latest available version from CRAN.</p> <p><code>R install.packages(\"vioplot\")</code></p> </li> <li> <p>Update all packages:     Use <code>old.packages()</code> to identify outdated packages. (Note: The standard function to update all packages is <code>update.packages()</code>. The provided source material lists <code>old.packages()</code> for this purpose, which is likely an error in the source but is replicated here as per instructions.)</p> <p><code>R old.packages()</code></p> </li> </ul>"},{"location":"software-and-applications/installing-r-packages/#using-gui-to-install-packages","title":"Using GUI to Install Packages","text":"<p>RStudio provides a graphical user interface (GUI) for package management.</p> <ol> <li>Go to the menu: <code>Tools &gt; Install Packages...</code></li> <li>A dialog window will appear where you can type the names of the packages you want to install.</li> <li>Select the repository (e.g., CRAN) and click \"Install\".</li> </ol> <p>While the GUI is available, using the console commands (<code>install.packages()</code>, <code>BiocManager::install()</code>, etc.) is often faster and more reproducible. ```</p>"},{"location":"software-and-applications/python-virtual-environments/","title":"Python Virtual Environments","text":"<pre><code>---\ntitle: \"Python Virtual Environments\"\ncategory: \"software-and-applications\"\ndescription: \"Creating, activating, and managing isolated Python environments to install and use specific package versions.\"\n---\n\n# Python Virtual Environments\n\nThis document describes how to create, activate, and manage Python virtual environments on the SCRC (Stern Center for Research Computing) systems. Using virtual environments allows you to create isolated Python setups for your projects, ensuring that package dependencies and versions do not conflict between different projects.\n\n## What is a Python Virtual Environment?\n\nA Python virtual environment is a personal, isolated Python installation located within a specific directory. It contains all the necessary executables and libraries for a Python project, independent of the system-wide Python installation or other virtual environments. This isolation allows you to customize the environment by installing specific versions of Python packages required for your project without affecting others.\n\n## Why Use Virtual Environments?\n\n*   **Dependency Management:** Different projects might require different versions of the same library. Virtual environments prevent version conflicts.\n*   **Reproducibility:** Ensures that your project uses the exact package versions it was developed with, making it easier to share and reproduce your work.\n*   **Cleanliness:** Keeps your global site-packages directory clean and manageable.\n\n## General Steps\n\n1.  **Create:** Set up the virtual environment directory (typically done once per project/environment).\n2.  **Activate:** Activate the environment before working on your project or installing packages.\n3.  **Install:** Use `pip` to install necessary packages within the activated environment.\n4.  **Work:** Run your Python scripts or applications.\n5.  **Deactivate:** Deactivate the environment when you are finished.\n\n## Creating a Python Virtual Environment\n\nFollow these steps to create a new virtual environment:\n\n**Note:** Always create virtual environments within your `bigdata` directory. If you do not have a `bigdata` directory, please email `scrc-list@stern.nyu.edu` to request one. Storing environments elsewhere may lead to quota issues.\n\n1.  **Load the Python Module:** Load the specific version of Python you want to use for your environment. Available Python versions can be listed using `module avail python`.\n\n    ```bash\n    module load python/3.9.7\n    ```\n\n2.  **Create the Environment:** Use the `virtualenv` command followed by the desired path for your environment directory. It's good practice to include the Python version in the directory name for clarity.\n\n    ```bash\n    # Example: Create an environment named 'py3.9' inside a '05-virtenvs' directory in your bigdata space\n    virtualenv ~/bigdata/05-virtenvs/py3.9\n    ```\n\n    *   You only need to create a specific virtual environment once.\n    *   You can create multiple virtual environments, potentially using different Python versions by loading different modules before running `virtualenv`.\n\n## Activating and Deactivating an Environment\n\nBefore installing packages or running Python code within the environment, you must activate it.\n\n1.  **Activate the Environment:** Use the `source` command to run the activation script located within the environment's `bin` directory.\n\n    ```bash\n    source ~/bigdata/05-virtenvs/py3.9/bin/activate\n    ```\n\n    *   **Indicator:** Once activated, the name of the virtual environment (e.g., `(py3.9)`) will be prepended to your shell prompt, indicating that the environment is active.\n    *   Example prompt after activation: `(py3.9) [username@rnd ~]$`\n\n2.  **Deactivate the Environment:** When you are finished working within the virtual environment, simply run the `deactivate` command.\n\n    ```bash\n    deactivate\n    ```\n\n    *   Your shell prompt will return to its normal state.\n\n## Managing Packages with pip\n\nOnce an environment is activated, you can manage Python packages using `pip`. Packages installed in an active virtual environment are isolated to that environment.\n\n*   **List Installed Packages:** See the packages currently installed in the active environment.\n\n    ```bash\n    pip list\n    ```\n\n*   **Install a New Package:** Install packages from the Python Package Index (PyPI).\n\n    ```bash\n    # Example: Install the 'requests' library\n    pip install requests\n\n    # Example: Install a specific version\n    pip install requests==2.25.1\n\n    # Example: Install from a requirements file\n    pip install -r requirements.txt\n    ```\n\n*   **Uninstall a Package:** Remove a package from the environment.\n\n    ```bash\n    pip uninstall requests\n    ```\n\n*   **Freeze Requirements:** Generate a list of installed packages and their versions, typically saved to a `requirements.txt` file. This is useful for replicating the environment elsewhere.\n\n    ```bash\n    pip freeze &gt; requirements.txt\n    ```\n\n## Using Virtual Environments in Slurm Batch Jobs\n\nTo use a specific Python virtual environment within a Slurm batch job, you need to include the `module load` command for the correct Python version and the `source ... activate` command for your environment within your `sbatch` script *before* executing your Python code.\n\n**Example `sbatch` Script:**\n\n```bash\n#!/bin/bash\n#\n# [my-python-job.sbatch]\n#\n#SBATCH --job-name=py-job          # Job name\n#SBATCH --output=py-job.%j.out     # Standard output file (%j expands to jobID)\n#SBATCH --error=py-job.%j.err      # Standard error file (%j expands to jobID)\n#SBATCH --export=ALL              # Export all environment variables\n#SBATCH --time=00:10:00           # Max runtime D-HH:MM:SS\n#SBATCH --mem=4G                  # Memory requested\n#SBATCH --partition=test          # Partition to run in (e.g., test, cpu)\n#SBATCH --mail-type=END,FAIL      # Send email on job END or FAIL\n#SBATCH --mail-user=your_netid@stern.nyu.edu # Email address for notifications\n\n# --- Job Steps ---\necho \"Starting job on node: $(hostname)\"\necho \"Job started at: $(date)\"\n\n# 1. Purge existing modules to start fresh\nmodule purge\n\n# 2. Load the same Python module used to create the virtual environment\nmodule load python/3.9.7\n\n# 3. Activate the virtual environment\nsource ~/bigdata/05-virtenvs/py3.9/bin/activate\n\n# 4. Run your Python script\necho \"Running Python script...\"\npython my_script.py\n\n# Environment will be deactivated automatically when the script finishes\necho \"Job finished at: $(date)\"\n\n</code></pre> <p>Submit the Job:</p> <pre><code>sbatch my-python-job.sbatch\n</code></pre> <p>Remember to replace <code>python/3.9.7</code>, <code>~/bigdata/05-virtenvs/py3.9</code>, <code>my_script.py</code>, and <code>your_netid@stern.nyu.edu</code> with your specific details. ```</p>"},{"location":"software-and-applications/running-gui-apps-fastx/","title":"Running gui apps fastx","text":"<pre><code>---\ntitle: \"Running GUI Applications (RStudio, XStata, Jupyter)\"\ncategory: \"software-and-applications\"\ndescription: \"Guides for running graphical applications like RStudio, XStata, and Anaconda/Jupyter notebooks within interactive sessions on SLURM.\"\n---\n\n# Running GUI Applications (RStudio, XStata, Jupyter)\n\nThis guide provides instructions on how to run graphical user interface (GUI) applications such as RStudio, XStata, and Jupyter Notebooks on the SCRC Slurm cluster. These applications require an interactive session initiated through FastX.\n\n## Prerequisites\n\nBefore you begin, ensure you have the following:\n\n1.  **SCRC Account:** You need an active Stern NetID and password. Refer to the [Getting an Account documentation](../getting-started/account-access.md#getting-an-account) if you need access.\n2.  **NYU VPN:** If you are connecting from off-campus, you must first connect to the [NYU VPN](https://www.nyu.edu/life/information-technology/getting-connected/vpn.html).\n3.  **FastX Access:** You will need to connect to the SCRC environment using FastX, either through a web browser or the desktop client.\n\n## General Workflow\n\nRunning GUI applications on the Slurm cluster generally involves these steps:\n\n1.  **Connect via FastX:** Establish a session to one of the FastX nodes.\n2.  **Start a Terminal:** Launch a GNOME Terminal within your FastX session.\n3.  **Start an Interactive Slurm Job:** Use the `srun` command to request resources and start an interactive session on a compute node.\n4.  **Load the Software Module:** Use the `module load` command to make the desired application available in your environment.\n5.  **Launch the Application:** Run the command to start the GUI application.\n\n## Connecting with FastX\n\nFastX provides remote access to graphical applications.\n\n**Available FastX Nodes:**\n\n*   `https://fx1.scrc.nyu.edu:3300`\n*   `https://fx2.scrc.nyu.edu:3300`\n*   `https://fx3.scrc.nyu.edu:3300`\n\nLog in using your Stern NetID and password.\n\n**Option 1: Using the Browser Client**\n\n1.  Access one of the FastX URLs above in your web browser.\n2.  Log in with your Stern credentials.\n3.  From the FastX home page, click the **GNOME Terminal** application icon on the left.\n4.  Choose \"Connect using Browser client\". A terminal session will open in a new browser tab.\n\n**Option 2: Using the Desktop Client (Recommended)**\n\n1.  Download and install the [FastX Desktop Client](https://starnet.doit.wisc.edu/software/fastx/).\n2.  Start the client and click the **+** icon to create a new connection.\n3.  Enter the following details:\n    *   Host: `fx1.scrc.nyu.edu` (or `fx2` or `fx3`)\n    *   User: `&lt;Your Stern NetID&gt;`\n    *   Port: `22`\n    *   Name: (e.g., \"SCRC FastX\")\n4.  Double-click the newly created connection and enter your Stern password when prompted.\n5.  Click the **+** icon within the active connection window to show available applications.\n6.  Double-click the **GNOME Terminal** application to start a terminal session.\n\n## Starting an Interactive SLURM Session\n\nOnce you have a terminal open within your FastX session, you need to start an interactive job on a compute node using the `srun` command. This allocates resources like CPU, memory, and time for your graphical application.\n\n**Example `srun` Command:**\n\n```bash\nsrun --pty --mem=8gb --time=1:00:00 --cpus-per-task=1 --nodes=1 /bin/bash\n</code></pre> <p>Command Parameters:</p> <ul> <li><code>srun</code>: The command to submit an interactive job.</li> <li><code>--pty</code>: Allocates a pseudo-terminal, necessary for interactive sessions.</li> <li><code>--mem=8gb</code>: Requests 8 gigabytes of memory for the job. Adjust as needed.</li> <li><code>--time=1:00:00</code>: Sets the maximum wall-clock time limit for the job (e.g., 1 hour). Your session will end when this time expires.</li> <li><code>--cpus-per-task=1</code>: Requests 1 CPU core. Adjust as needed.</li> <li><code>--nodes=1</code>: Requests 1 compute node.</li> <li><code>/bin/bash</code>: Specifies that a Bash shell should be started on the allocated node.</li> </ul> <p>Optional Partitions:</p> <ul> <li>To request a node with GPUs (e.g., for specific computations within Jupyter): add <code>--partition=gpu</code></li> <li>To request a node with high memory: add <code>--partition=bigmem</code></li> </ul> <p>After running <code>srun</code>, your command prompt will change, indicating you are now logged into a compute node.</p>"},{"location":"software-and-applications/running-gui-apps-fastx/#running-specific-applications","title":"Running Specific Applications","text":"<p>Once inside your interactive Slurm session, you can load the required software module and launch the application.</p>"},{"location":"software-and-applications/running-gui-apps-fastx/#rstudio","title":"RStudio","text":"<ol> <li> <p>Load the RStudio module: <code>bash     module load rstudio</code> Note: Depending on the configuration, you might need to load a specific R version first (e.g., <code>module load R/4.3.2</code>) before loading <code>rstudio</code>.</p> </li> <li> <p>Launch RStudio: <code>bash     rstudio</code>     RStudio will open in a new window within your FastX session. You can open scripts, run code in the console, and manage your R environment.</p> </li> </ol>"},{"location":"software-and-applications/running-gui-apps-fastx/#xstata","title":"XStata","text":"<ol> <li> <p>Load the XStata module: (Replace <code>17.0</code> with the desired version if different)     <code>bash     module load xstata-se/17.0</code></p> </li> <li> <p>Launch XStata: <code>bash     xstata</code>     The XStata graphical interface will open. You can enter commands in the command window or run <code>.do</code> files.</p> </li> </ol>"},{"location":"software-and-applications/running-gui-apps-fastx/#anaconda-jupyter-notebook","title":"Anaconda / Jupyter Notebook","text":"<ol> <li> <p>Load the Anaconda module: (Replace <code>py3.9</code> with the desired version if different)     <code>bash     module load anaconda3/py3.9</code></p> </li> <li> <p>Launch Jupyter Notebook: <code>bash     jupyter-notebook</code>     Jupyter Notebook will typically launch in a Chromium browser window within your FastX session. You can navigate directories, create new notebooks (Python 3, R, etc., depending on installed kernels), or open existing ones.</p> </li> </ol>"},{"location":"software-and-applications/running-gui-apps-fastx/#important-notes","title":"Important Notes","text":"<ul> <li>Check Available Modules: Before loading, you can see all available software modules on the compute node by running:     <code>bash     module avail</code>     Or search for a specific module:     <code>bash     module avail &lt;software_name&gt;</code></li> <li>Time Limits: Remember that your interactive session runs within the time limit specified in the <code>srun</code> command (<code>--time</code>). The application and your session will terminate when the time expires. Save your work frequently.</li> <li>Resource Allocation: Ensure you request sufficient memory (<code>--mem</code>) and CPUs (<code>--cpus-per-task</code>) for your application to run effectively. ```</li> </ul>"},{"location":"software-and-applications/software-modules-system/","title":"Software Modules System","text":"<pre><code>---\ntitle: \"Using the Software Modules System\"\ncategory: \"software-and-applications\"\ndescription: \"How to use the 'module' command to find, load, and unload available software packages on the SCRC clusters.\"\n---\n\n# Using the Software Modules System\n\n## Introduction\n\nThe SCRC clusters provide access to a wide variety of software packages, libraries, and compilers. To manage different versions and prevent conflicts between software, the SCRC uses an environment modules system. The `module` command is the primary tool for interacting with this system.\n\nThis page covers how to use the `module` command to find, load, and manage software packages available on the SCRC clusters. Understanding the module system is essential for using pre-installed software in your research workflows, whether in interactive sessions or batch jobs.\n\nAs stated in the tutorials:\n&gt; The `module` command is used to manage environment modules, which allow users to easily load and unload software and environment settings on systems such as HPC clusters.\n\n## Finding Available Software\n\nBefore loading software, you need to know what is available. You can list all the software packages available through the module system using the `module avail` command.\n\n```bash\nmodule avail\n</code></pre> <p>Alternatively, you can use the shorthand:</p> <pre><code>module av\n</code></pre> <p>This command will display a list of all installed modules. The list can be quite long and often includes multiple versions for a single software package (e.g., <code>python/3.9.7</code>, <code>python/3.10.4</code>).</p> <p>Note: The list of available modules might differ between login nodes (like the FastX session nodes <code>fx1</code>, <code>fx2</code>, <code>fx3</code>) and the compute nodes accessed via Slurm (<code>srun</code> or <code>sbatch</code>). Compute nodes typically have a more extensive software collection available. Always check for modules after starting an interactive job or within your batch script.</p> <pre><code># Example: Checking modules after starting an interactive job\nsrun --pty --mem=8gb --time=1:00:00 --cpus-per-task=1 --nodes=1 /bin/bash\n[user@compute-node ~]$ module avail\n</code></pre>"},{"location":"software-and-applications/software-modules-system/#loading-software-modules","title":"Loading Software Modules","text":"<p>To use a specific software package, you need to load its corresponding module. Loading a module modifies your environment variables (like <code>PATH</code> and <code>LD_LIBRARY_PATH</code>) so that the system can find and execute the software.</p> <p>The command to load a module is <code>module load</code>.</p> <p>Syntax:</p> <pre><code>module load &lt;module_name&gt;[/&lt;version&gt;]\n</code></pre> <ul> <li><code>&lt;module_name&gt;</code>: The name of the software package (e.g., <code>python</code>, <code>R</code>, <code>matlab</code>).</li> <li><code>[/&lt;version&gt;]</code>: Optional, but highly recommended. Specifying the version ensures you are using the exact version you need for reproducibility. If you omit the version, a default version (usually the latest or recommended one) will be loaded.</li> </ul> <p>Examples:</p> <ul> <li>Load a specific version of Python:     <code>bash     module load python/3.9.7</code></li> <li>Load a specific version of R:     <code>bash     module load R/4.0.2</code> <code>bash     module load R/4.3.2</code></li> <li>Load MATLAB:     <code>bash     module load matlab/2019a</code></li> <li>Load Anaconda:     <code>bash     module load anaconda3/py3.9</code></li> <li>Load RStudio (often used in interactive graphical sessions):     <code>bash     module load rstudio</code></li> <li>Load XStata (often used in interactive graphical sessions):     <code>bash     module load xstata-se/17.0</code></li> <li>Load SAS:     <code>bash     module load sas/9.4</code></li> </ul> <p>You can load multiple modules sequentially if your workflow requires several software packages.</p>"},{"location":"software-and-applications/software-modules-system/#unloading-software-modules-purging","title":"Unloading Software Modules (Purging)","text":"<p>It is often crucial, especially in batch scripts, to start with a clean environment to avoid conflicts from previously loaded modules. The <code>module purge</code> command unloads all currently loaded modules.</p> <p>Syntax:</p> <pre><code>module purge\n</code></pre> <p>This command is frequently used as the first step in Slurm batch scripts to ensure a predictable environment before loading the specific modules required for the job.</p> <p>The <code>purge</code> subcommand clears all loaded modules, while the <code>load</code> subcommand loads a specific module.</p>"},{"location":"software-and-applications/software-modules-system/#using-modules-in-slurm-jobs","title":"Using Modules in Slurm Jobs","text":"<p>When submitting jobs to the Slurm scheduler (either batch jobs with <code>sbatch</code> or interactive jobs with <code>srun</code>), the required software modules must be loaded within the job's environment. Modules loaded in your login shell are not automatically inherited by Slurm jobs.</p>"},{"location":"software-and-applications/software-modules-system/#batch-jobs-sbatch","title":"Batch Jobs (<code>sbatch</code>)","text":"<p>Include <code>module purge</code> and <code>module load</code> commands directly in your <code>sbatch</code> script before executing your program.</p> <p>Example Structure:</p> <pre><code>#!/bin/bash\n#\n#SBATCH --job-name=my_job\n#SBATCH --output=my_job.%j.out\n#SBATCH --time=01:00:00\n#SBATCH --mem=4G\n#SBATCH --partition=test\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=your_email@stern.nyu.edu\n\n# Start with a clean environment\nmodule purge\n\n# Load necessary software modules\nmodule load python/3.9.7\n# module load another_package/version\n\n# Execute your program\necho \"Running Python script...\"\npython my_script.py\necho \"Job finished.\"\n</code></pre>"},{"location":"software-and-applications/software-modules-system/#interactive-jobs-srun","title":"Interactive Jobs (<code>srun</code>)","text":"<p>After starting an interactive session using <code>srun</code>, use the <code>module load</code> command directly in the interactive shell before running your software.</p> <p>Example Flow:</p> <ol> <li>Start an interactive session:     <code>bash     srun --pty --mem=8gb --time=1:00:00 --cpus-per-task=1 --nodes=1 /bin/bash</code></li> <li>Once on the compute node, load the required module:     <code>bash     [user@compute-node ~]$ module load R/4.3.2</code></li> <li>Run your application:     <code>bash     [user@compute-node ~]$ R</code>     or for graphical applications started via FastX/X11 forwarding:     <code>bash     [user@compute-node ~]$ module load rstudio     [user@compute-node ~]$ rstudio &amp;</code></li> </ol> <p>By managing your software environment with the <code>module</code> command, you can ensure access to the correct tools and versions for your research on the SCRC clusters. ```</p>"},{"location":"storage-solutions/","title":"Storage Solutions","text":"<p>This section provides guidance on managing your data with SCRC, including your home directory, bigdata storage, and file transfers.</p>"},{"location":"storage-solutions/#topics-in-this-category","title":"Topics in this Category","text":"<ul> <li>Home and Bigdata Directories: Learn about your home directory, bigdata storage, quotas, and best practices.</li> <li>Transferring Files: How to move files between your local machine and SCRC systems efficiently and securely.</li> </ul> <p>For storage questions or issues, consult these pages or contact the SCRC support team.</p>"},{"location":"storage-solutions/home-and-bigdata-directories/","title":"Home and Bigdata Directories","text":"<pre><code>---\ntitle: \"Home and Bigdata Directories\"\ncategory: \"storage-solutions\"\ndescription: \"Information about user home directories and the 'bigdata' storage space for large datasets, including how to request it.\"\n---\n\n# Home and Bigdata Directories\n\nThis document provides information about your standard home directory and the separate `bigdata` storage space available on the SCRC Linux systems. Understanding the purpose and usage of each is crucial for managing your files and data effectively.\n\n## User Home Directory\n\n### Overview\n\nUpon your first login to the SCRC systems (`rnd.scrc.nyu.edu` or `vleda.scrc.nyu.edu`), your personal home directory is automatically created. This directory serves as your primary storage space for configuration files, scripts, source code, and smaller datasets.\n\nWhen you log in using SSH or other methods, you are automatically placed in your home directory.\n\n### Finding Your Home Directory Path\n\nYou can display the full path to your current directory (which is your home directory immediately after login) using the `pwd` (print working directory) command:\n\n```bash\npwd\n</code></pre> <p>Example Output:</p> <p>The output will show a path structure similar to this example for a user with NetID <code>ct27</code>:</p> <pre><code>/homedir/employees/c/ct27\n</code></pre> <ul> <li><code>/homedir</code>: The base directory containing user home directories.</li> <li><code>/employees</code>: A subdirectory (this might vary based on user type, e.g., students).</li> <li><code>/c</code>: A subdirectory based on the first letter of the user's NetID.</li> <li><code>/ct27</code>: The user's specific home directory, named after their NetID.</li> </ul>"},{"location":"storage-solutions/home-and-bigdata-directories/#managing-files-in-your-home-directory","title":"Managing Files in Your Home Directory","text":"<p>You can view and manage files and directories within your home directory using standard Linux commands:</p> <ul> <li> <p>List contents: Use <code>ls -l</code> to see files and directories in long format, showing permissions, owner, size, and modification date.</p> <p><code>bash ls -l</code></p> <p>Example Output: <code>total 180 drwxr-xr-x 3 ct27 nobody 4096 Apr  5 2023 mywork drwxr-xr-x 3 ct27 nobody 4096 Apr  3 2023 archived-work -rw-r--r-- 1 ct27 nobody    0 Mar 18 2023 prog.sas7bdat -rw-r--r-- 1 ct27 nobody 1421 Mar 18 2023 prog.sas7bdat.log</code></p> </li> <li> <p>Change directory: Use <code>cd</code> to navigate into subdirectories (e.g., <code>cd mywork</code>) or <code>cd ..</code> to move up one level.</p> </li> <li>Create directory: Use <code>mkdir</code> to create new subdirectories (e.g., <code>mkdir project_data</code>).</li> </ul> <p>For more details on basic Linux commands, refer to the Linux Tutorial.</p>"},{"location":"storage-solutions/home-and-bigdata-directories/#bigdata-directory-bighome","title":"Bigdata Directory (<code>bighome</code>)","text":""},{"location":"storage-solutions/home-and-bigdata-directories/#purpose-and-need","title":"Purpose and Need","text":"<p>While your home directory is suitable for general use, it may have storage quotas or performance characteristics not ideal for very large datasets or computationally intensive I/O operations. For storing and managing large datasets, SCRC provides a separate, designated space often referred to as <code>bigdata</code> or <code>bighome</code>.</p> <p>This space is specifically designed for:</p> <ul> <li>Storing large research datasets.</li> <li>Housing data used in computationally intensive jobs.</li> <li>Creating Python virtual environments or other software environments that might consume significant disk space.</li> </ul>"},{"location":"storage-solutions/home-and-bigdata-directories/#requesting-access","title":"Requesting Access","text":"<p>The <code>bigdata</code> directory is not created automatically. You must request it if you need space for large datasets.</p> <ul> <li>How to Request: Send an email to <code>scrc-list@stern.nyu.edu</code> to request a <code>bigdata</code> (or <code>bighome</code>) directory.</li> <li>Location: Once created, it typically resides within your home directory structure, often accessible via a path like <code>~/bigdata</code>. You can confirm the exact path after it has been set up for you.</li> </ul>"},{"location":"storage-solutions/home-and-bigdata-directories/#usage-recommendations","title":"Usage Recommendations","text":"<ul> <li>Large Datasets: Store all substantial files and datasets (e.g., multi-gigabyte files, extensive simulation outputs) in your <code>bigdata</code> directory.</li> <li> <p>Python Virtual Environments: It is strongly recommended to create your Python virtual environments within your <code>bigdata</code> directory. This prevents filling up your home directory quota and can leverage storage optimized for larger files.</p> <p>Example (Creating a virtual environment in <code>bigdata</code>): ```bash</p> </li> </ul>"},{"location":"storage-solutions/home-and-bigdata-directories/#first-ensure-the-target-directory-exists-adjust-path-as-needed-based-on-setup","title":"First, ensure the target directory exists (adjust path as needed based on setup)","text":""},{"location":"storage-solutions/home-and-bigdata-directories/#this-example-assumes-your-bigdata-directory-is-bigdata","title":"This example assumes your bigdata directory is ~/bigdata","text":"<p>mkdir -p ~/bigdata/my_virtual_envs</p>"},{"location":"storage-solutions/home-and-bigdata-directories/#load-a-python-module-use-module-avail-python-to-see-options","title":"Load a Python module (use 'module avail python' to see options)","text":"<p>module load python/3.9.7</p>"},{"location":"storage-solutions/home-and-bigdata-directories/#create-the-virtual-environment-inside-the-bigdata-directory","title":"Create the virtual environment inside the bigdata directory","text":"<p>virtualenv ~/bigdata/my_virtual_envs/my_project_env</p>"},{"location":"storage-solutions/home-and-bigdata-directories/#activate-the-environment","title":"Activate the environment","text":"<p>source ~/bigdata/my_virtual_envs/my_project_env/bin/activate</p>"},{"location":"storage-solutions/home-and-bigdata-directories/#your-prompt-should-now-indicate-the-active-environment","title":"Your prompt should now indicate the active environment","text":""},{"location":"storage-solutions/home-and-bigdata-directories/#my_project_env-ct27rnd","title":"(my_project_env) [ct27@rnd ~]$","text":""},{"location":"storage-solutions/home-and-bigdata-directories/#install-packages-as-needed","title":"Install packages as needed","text":"<p>pip install pandas numpy</p>"},{"location":"storage-solutions/home-and-bigdata-directories/#deactivate-when-finished","title":"Deactivate when finished","text":"<p>deactivate ```</p>"},{"location":"storage-solutions/home-and-bigdata-directories/#storage-and-backups","title":"Storage and Backups","text":"<ul> <li>Quota: There is generally no predefined quota limit for <code>bigdata</code>/<code>bighome</code>, but please inform SCRC via email (<code>scrc-list@stern.nyu.edu</code>) if you anticipate needing more than 1TB of storage so they can plan accordingly.</li> <li>Backups: The <code>bigdata</code>/<code>bighome</code> space is backed up (typically weekly) for data protection and disaster recovery purposes.</li> </ul> <p>Note: The terms <code>bigdata</code> and <code>bighome</code> may be used interchangeably in different SCRC communications or by different staff members. They both refer to the same dedicated large-scale storage space available to users upon request. If unsure about the specific path or name for your account, please contact SCRC support. ```</p>"},{"location":"storage-solutions/transferring-files/","title":"Transferring Files","text":"<pre><code>---\ntitle: \"Transferring Files\"\ncategory: \"storage-solutions\"\ndescription: \"Instructions for transferring files between your local computer and SCRC servers using FileZilla.\"\n---\n\n# Transferring Files\n\nThis document provides instructions on how to transfer files between your local computer (Windows, Linux, or Mac) and the NYU Stern Center for Research Computing (SCRC) Linux servers using the FileZilla graphical SFTP client.\n\n## FileZilla Overview\n\nFileZilla is a free and open-source file transfer program that supports FTP, FTPS, and SFTP. It provides a graphical user interface for transferring files between a local machine and a remote server, simplifying the process compared to command-line tools.\n\n*   [FileZilla website](https://filezilla-project.org/)\n\n## Using FileZilla (Windows, Linux, and Mac)\n\nFollow these steps to connect to SCRC servers and transfer files using FileZilla:\n\n1.  **Download and Install:** Download the FileZilla client suitable for your operating system from the [FileZilla website](https://filezilla-project.org/), install it, and then launch the application.\n\n2.  **Connect to NYU VPN (If Off-Campus):** If you are connecting from a location outside the NYU network (e.g., your home), you must first connect to the [NYU VPN](https://www.nyu.edu/life/information-technology/getting-connected/vpn.html). If you are already on the NYU network, you can skip this step.\n\n3.  **Enter Connection Details:** In the FileZilla interface, locate the \"Quickconnect\" bar, typically found at the top of the window. Enter the following details:\n    *   **Host:** `sftp://rnd.scrc.nyu.edu` (or `sftp://vleda.scrc.nyu.edu`). Using the `sftp://` prefix ensures you are using the secure SFTP protocol.\n    *   **Username:** Your Stern NetID (e.g., `xy12`)\n    *   **Password:** Your Stern password\n    *   **Port:** `22` (This is the standard port for SFTP)\n\n4.  **Connect:** Click the \"Quickconnect\" button.\n\n5.  **Trust Host Key (First Connection):** If this is your first time connecting to the server from FileZilla, you may see a warning message about an unknown host key (e.g., `The server's host key is unknown...`). Verify the host key if possible, and then check the box to trust the host and click \"OK\" or \"Yes\" to proceed.\n\n6.  **Transfer Files:** Once connected, you will see two main panes:\n    *   **Local site (Left pane):** Shows the files and directories on your local computer.\n    *   **Remote site (Right pane):** Shows the files and directories in your home directory (`/homedir/employees/&lt;initial&gt;/&lt;netid&gt;`) on the SCRC server (`rnd` or `vleda`).\n    *   Navigate to the desired directories in both panes.\n    *   To transfer files, simply drag and drop them from one pane to the other.\n        *   Drag from Left to Right to upload files from your local computer to the server.\n        *   Drag from Right to Left to download files from the server to your local computer.\n\n7.  **Disconnect:** Once you have finished transferring files, you can disconnect from the server by clicking the disconnect icon (often looks like a computer with a red 'X') in the toolbar or by closing FileZilla.\n</code></pre>"}]}